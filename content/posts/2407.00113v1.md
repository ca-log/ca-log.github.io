---
author: 'TechScribe'
title: '个性化联邦持续学习：通过多粒度提示实现知识的高效融合与个性化'
date: '2024-06-27'
Lastmod: '2024-07-05'
description: 'Personalized Federated Continual Learning via Multi-granularity Prompt'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Personalized Federated Continual Learning via Multi-granularity Prompt](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00113v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00113v1)

## 摘要

本文介绍了一种名为“个性化联邦持续学习通过多粒度提示”（PFCL）的新型实用场景，该场景在共享和个性化知识方面提出了更大的挑战。PFCL不仅依赖于全局时空视角的知识融合，还需要根据每个客户端的本地需求改进模型。现有的个性化联邦学习（PFL）或联邦持续学习（FCL）方法忽视了知识的多粒度表示，这种表示可以用来克服时空灾难性遗忘（STCF）并采用粗到细的人类认知机制来适应普遍知识。为此，我们提出了一种称为多粒度提示的新概念，即通过共同模型学习过程获得的粗粒度全局提示，以及用于个性化普遍表示的细粒度本地提示。前者专注于高效地转移共享的全局知识而不会空间遗忘，后者强调特定学习个性化本地知识以克服时间遗忘。此外，我们设计了一种选择性提示融合机制，用于从不同客户端提炼的全局提示中聚合知识。通过粗粒度知识的独占融合，我们实现了客户端之间的共同知识的传输和细化，进一步增强了个性化性能。大量实验证明了所提方法在解决STCF以及提高个性化性能方面的有效性。<!--more-->

## 原理

本文提出的Federated Multi-Granularity Prompt（FedMGP）方法通过引入多粒度知识空间到PFCL中，以更好地满足个性化需求和解决时空遗忘问题。具体来说，客户端设计了两种粒度级别的提示用于知识表示，即粗粒度全局提示和细粒度本地提示。全局提示代表粗粒度的共同知识，而本地提示，建立在全局提示之上，代表类别的细粒度知识。仅融合粗粒度的共同知识有助于形成普遍知识并避免因聚合细粒度知识而导致的时空遗忘。基于全局提示的本地提示旨在个性化来自服务器的普遍知识，同时防止由于类别增加而导致的时间遗忘。在服务器端，我们设计了一种新的全局提示融合方法，称为选择性提示融合，不会导致时空遗忘。仅聚合粗粒度知识不仅提高了聚合速度，还提供了进一步的隐私保护改进。

## 流程

FedMGP的工作流程如下：首先，客户端使用预训练的ViT模型作为基础认知系统，通过冻结ViT的参数，客户端学习在输入级别操作的全局提示。这些全局提示代表通过共同模型学习过程获得的粗粒度知识。接下来，基于冻结的全局提示，客户端进一步开发细粒度类别特定的本地提示。这些本地提示直接影响模型的多头部自注意力（MSA）层，有助于提取本地细粒度知识。在服务器端，通过知识蒸馏从不同客户端的提示池中聚合全局提示，增强了它们的普遍性。整个过程通过迭代训练和聚合，不断优化全局和本地提示，以适应不断变化的任务需求和客户端特性。

## 应用

PFCL通过多粒度提示的应用，为处理动态和异构数据环境中的持续学习问题提供了有效的解决方案。这种方法不仅适用于传统的联邦学习场景，还可以扩展到垂直联邦学习和多目标联邦学习等更广泛的联邦学习场景。通过优化隐私、效用和效率，PFCL有望在未来的联邦学习应用中发挥重要作用，特别是在需要高度个性化和持续知识更新的领域，如金融科技、智能城市和个性化医疗等。