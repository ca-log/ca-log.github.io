---
author: 'TechScribe'
title: '揭秘Transformer：Contextual Decomposition提升模型解释性'
date: '2024-07-01'
Lastmod: '2024-07-05'
description: 'Mechanistic Interpretation through Contextual Decomposition in Transformers'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Mechanistic Interpretation through Contextual Decomposition in Transformers](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00886v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00886v1)

## 摘要

本文介绍了一种名为Contextual Decomposition for Transformers (CD-T)的新型解释方法，旨在提高Transformer模型的机制解释性。Transformer模型因其复杂的非线性特征关系而被视为“黑箱”，CD-T方法通过计算高效的方式，揭示了输入特征或内部组件（如注意力头和前馈网络）对最终预测或任意内部组件输出的贡献。该方法在真实世界的病理报告分类任务中展示了其优越性，不仅提高了计算效率（速度提升2倍），还增强了模型的可信度和用户对模型输出的信任。<!--more-->

## 原理

CD-T方法通过将输入分解为相关和无关部分，并将其通过Transformer模型的节点传播，从而计算目标激活的分解。具体来说，CD-T定义了一套规则来确定模块输出的分解，例如对于元素级的ReLU激活函数，输出分解定义为相关和无关部分的组合。这种方法的关键贡献在于处理自注意力模块，这是Transformer模型中唯一未被先前工作解决的部分。通过这种方式，CD-T能够捕捉到特征交互，并提供对Transformer模型内部工作机制的深入理解。

## 流程

CD-T的工作流程包括以下步骤：
1. 输入分解：将输入向量分解为相关部分（β）和无关部分（γ）。
2. 模块传播：通过定义的规则，将分解后的输入通过Transformer模型的各个模块传播。
3. 输出分解：计算模块输出的分解，特别是自注意力模块的输出分解。
4. 电路构建：使用CD-T方法构建电路，通过迭代识别网络各层中的关键内部组件，从而形成一个解释模型行为的子图。

## 应用

CD-T方法的应用前景广泛，特别是在需要高度信任和透明度的领域，如医学图像分析和药物发现。此外，CD-T还能用于本地解释，帮助用户理解模型在特定预测中的决策过程。随着深度学习模型在各个行业的应用越来越广泛，CD-T提供了一种强大的工具，用于增强模型的可解释性和可靠性。