---
author: 'TechScribe'
title: '揭秘Transformer：CD-T方法开启深度学习模型的机械性解释新篇章'
date: '2024-07-01'
Lastmod: '2024-07-05'
description: 'Mechanistic Interpretation through Contextual Decomposition in Transformers'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Mechanistic Interpretation through Contextual Decomposition in Transformers](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00886v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00886v1)

## 摘要

本文介绍了一种名为“Contextual Decomposition for Transformers (CD-T)”的新型解释方法，旨在提高Transformer模型的机械性解释能力。Transformer模型因其复杂的非线性特征关系而被视为“黑箱”，而CD-T方法通过计算高效的方式，能够捕捉输入特征或内部组件（如注意力头和前馈网络）对最终预测或任意目标内部组件输出的贡献。该方法在真实世界的病理报告分类任务中展示了其优越性，不仅提高了计算效率（速度提升2倍），还增强了局部解释的能力。此外，通过人体实验，CD-T被证明能够帮助用户更好地理解和信任模型的输出。<!--more-->

## 原理

CD-T方法通过将输入分解为相关和不相关的部分，并将其通过Transformer模型的各个模块传播，从而实现对模型内部工作机制的解释。具体来说，CD-T定义了一套规则来确定模块输出的分解，这些规则考虑了输入分解对输出分解的影响。例如，对于元素级的ReLU激活函数，输出分解定义为相关和不相关部分的组合。这种方法的关键在于能够分解Transformer中的自注意力模块，这是其他模块（如线性变换和非线性激活函数）所不具备的。通过这种方式，CD-T能够揭示模型内部组件如何相互作用，从而影响最终的预测结果。

## 流程

CD-T的工作流程包括以下几个步骤：
1. **输入分解**：将输入向量分解为相关和不相关的部分。
2. **模块传播**：通过Transformer模型的各个模块传播这些分解部分。
3. **输出分解**：根据定义的规则，计算模块输出的分解。
4. **电路发现**：使用CD-T方法从模型的输出逻辑开始，逆向追踪重要的内部组件，构建一个解释模型行为的电路。

例如，在病理报告分类任务中，CD-T首先将报告文本分解为相关和不相关的词汇，然后通过BERT模型传播这些分解，最终识别出对分类决策有重要贡献的词汇和短语。

## 应用

CD-T方法的应用前景广泛，特别是在需要高度信任和透明度的领域，如医疗和网络安全。通过提供对Transformer模型内部工作机制的深入理解，CD-T有助于诊断和修正模型错误，确保模型在关键应用中的安全性和可靠性。此外，CD-T的局部解释能力也使其在自然语言处理任务中具有广泛的应用潜力，如情感分析和新闻主题分类。