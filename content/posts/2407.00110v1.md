---
author: 'TechScribe'
title: 'Chat AI：引领HPC与AI融合的新时代，打造无缝、安全、高效的智能服务体验'
date: '2024-06-27'
Lastmod: '2024-07-05'
description: 'Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Chat AI: A Seamless Slurm-Native Solution for HPC-Based Services](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00110v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00110v1)

## 摘要

本文介绍了一种名为Chat AI的创新解决方案，旨在为基于高性能计算（HPC）的服务提供一个高效、安全和私密的原生Slurm集成平台。随着大型语言模型（LLMs）的广泛采用，研究人员需要一个能够运行开源或自定义微调LLMs的基础设施，同时确保用户数据隐私和安全。Chat AI通过利用本地大学和研究中心的可信环境，提供了一个替代商业LLM服务的私有和安全选项。该解决方案与Slurm无缝集成，能够在HPC集群上与常规Slurm工作负载并行运行，同时利用Slurm创建的时间间隙。为了确保HPC系统的安全性，使用了SSH ForceCommand指令构建了一个强大的断路器，防止面向Web的服务器受到攻击。Chat AI已成功部署为生产服务，并提供了源代码。<!--more-->

## 原理

Chat AI的核心工作原理是通过一个云虚拟机（VM）上的Web服务与HPC系统上的可扩展后端相结合，实现对多种AI模型的运行。该系统原生集成Slurm，使得在HPC集群上的部署变得无缝。关键的安全措施包括使用SSH ForceCommand指令限制SSH密钥的权限，防止其被用于执行通用shell命令。此外，通过在HPC代理中维护一个SSH连接，确保了从Web服务器到HPC登录节点的通信安全。Chat AI的架构还包括一个调度器，该调度器定期检查并确保服务实例的可用性，根据需求动态调整服务实例的数量。这种设计不仅提高了资源利用率，还增强了系统的安全性和可靠性。

## 流程

Chat AI的工作流程从用户通过Web界面或API提交请求开始。请求首先通过Kong API网关进行路由和认证，然后通过HPC代理转发到HPC集群上的服务节点。调度器负责监控请求量并根据需要启动或终止服务实例。每个服务实例运行一个LLM模型，如vLLM，通过OpenAI兼容的API处理请求。整个流程中，用户的输入和模型的输出都不在服务器端存储，确保了数据隐私。例如，用户可以通过Web界面与LLM进行实时对话，所有对话内容仅存储在用户的本地设备上。

## 应用

Chat AI的应用前景广泛，特别适用于需要高性能和数据隐私保护的场景，如学术研究、企业内部服务和政府机构。随着LLMs在各个领域的应用越来越广泛，Chat AI提供了一个可靠的平台，支持实时对话和模型推理，同时确保数据安全和用户隐私。未来，Chat AI还可以扩展支持更多类型的AI服务，如视觉语言模型（VLMs）和音频处理服务，进一步扩大其应用范围。