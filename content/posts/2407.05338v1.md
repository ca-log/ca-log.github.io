---
author: 'TechScribe'
title: '生成式AI的审计蓝图：确保技术伦理与社会责任的创新方法'
date: '2024-07-07'
Lastmod: '2024-07-10'
description: 'A Blueprint for Auditing Generative AI'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![A Blueprint for Auditing Generative AI](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.05338v1.pdf_0.jpg)](https://arxiv.org/abs/2407.05338v1)

## 摘要

本文由Jakob Mökander, Justin Curl, 和 Mihir Kshirsagar 等人撰写，针对生成式人工智能（Generative AI）系统广泛应用带来的伦理和社会挑战，提出了一种新颖的审计蓝图。文章指出，现有的审计程序未能充分解决生成式AI系统所展现的涌现能力和广泛适应性带来的治理挑战。为此，作者提出了一种三层审计方法，包括治理审计（针对设计和传播生成式AI系统的技术提供者）、模型审计（在预训练后但在发布前对生成式AI系统进行）和应用审计（针对基于生成式AI系统的应用），这些审计相互补充并相互通知。文章还讨论了这种审计方法的局限性，并强调了审计在确保生成式AI系统合法、伦理和技术稳健性方面的重要性。<!--more-->

## 原理

文章提出的三层审计方法的工作原理如下：
1. **治理审计**：评估开发生成式AI系统的技术提供者的组织程序、问责结构和质量管理体系。这包括审查组织治理结构的充分性，创建模型开发过程的审计跟踪，以及映射组织内角色和责任。
2. **模型审计**：在模型预训练后但在其适应和部署到特定应用之前，评估其技术特性。这包括评估模型的性能、信息安全和鲁棒性。模型审计还涉及审查训练数据集，以确保模型的设计和训练过程中没有偏见。
3. **应用审计**：评估基于生成式AI模型的产品和服务的合法性和对用户及社会的影响。这包括功能审计（评估应用的预期和操作目标）和影响审计（评估应用对不同用户、群体和自然环境的影响）。

这些审计层级通过结构化和协调的方式进行，确保从技术提供者到最终应用的每个阶段都能识别和管理生成式AI系统带来的伦理和社会风险。

## 流程

文章提出的审计工作流程包括以下步骤：
1. **治理审计**：技术提供者首先进行自我评估，然后由独立第三方进行审查，确保组织内部有适当的治理结构和风险管理机制。
2. **模型审计**：在模型预训练完成后，由技术提供者和独立审计师共同进行，评估模型的性能、安全性和鲁棒性，并生成模型卡等文档，记录模型的特性和限制。
3. **应用审计**：在应用开发和部署过程中，由应用开发者和独立审计师进行，确保应用符合法律规定和伦理标准，并持续监控应用的实际影响。

这些步骤通过输出和输入的相互关联，形成一个闭环的审计流程，确保生成式AI系统的每个阶段都能得到适当的监督和评估。

## 应用

文章提出的三层审计方法适用于各种生成式AI系统的监管和治理，特别是在医疗、金融和网络安全等高风险行业。通过这种审计方法，可以有效识别和管理生成式AI系统带来的伦理和社会风险，促进技术的负责任使用和可持续发展。