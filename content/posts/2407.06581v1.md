---
author: 'TechScribe'
title: '震惊！这些人工智能模型竟然是“睁眼瞎”？'
date: '2024-07-09'
Lastmod: '2024-07-10'
description: 'Vision language models are blind'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Vision language models are blind](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.06581v1.pdf_0.jpg)](https://arxiv.org/abs/2407.06581v1)

## 摘要

这篇论文探讨了具有视觉能力的大型语言模型（VLMs）在低级别视觉任务上的表现。作者通过一系列实验，测试了四种最先进的 VLMs 在识别几何图形、计数、路径跟踪等方面的能力。结果发现，这些模型在一些简单任务上的表现远不如人类，例如无法准确判断两个圆是否重叠、两个直线是否相交、字母是否被圆圈包围等。这表明 VLMs 的视觉能力存在局限性，需要进一步改进。<!--more-->

## 原理

论文的关键内容是通过设计一系列简单的视觉任务，测试 VLMs 的视觉能力。这些任务包括计数线的交点、判断两个圆是否重叠、识别被圆圈包围的字母、计数重叠的形状、计数嵌套的正方形、计数网格的行数和列数以及跟踪单颜色路径等。作者使用了多种方法来评估模型的性能，包括准确率、召回率和 F1 值等。

## 流程

1. 选择四种最先进的 VLMs：GPT-4o、Gemini-1.5 Pro、Claude-3 Sonnet 和 Claude-3.5 Sonnet。
2. 设计一系列简单的视觉任务，包括计数线的交点、判断两个圆是否重叠、识别被圆圈包围的字母、计数重叠的形状、计数嵌套的正方形、计数网格的行数和列数以及跟踪单颜色路径等。
3. 使用多种方法来评估模型的性能，包括准确率、召回率和 F1 值等。
4. 分析实验结果，发现 VLMs 在一些简单任务上的表现远不如人类。

## 应用

这项研究对于理解 VLMs 的视觉能力和局限性具有重要意义。它可以帮助研究人员更好地设计和改进 VLMs，提高它们在视觉任务上的性能。此外，这项研究还可以为其他领域的研究提供参考，例如计算机视觉、自然语言处理和人工智能等。