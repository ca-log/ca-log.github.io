---
author: 'TechScribe'
title: 'MetaLLM：动态智能路由LLMs，实现高性能与成本效益的平衡'
date: '2024-07-15'
Lastmod: '2024-07-16'
description: 'MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.10834v1.pdf_0.jpg)](https://arxiv.org/abs/2407.10834v1)

## 摘要

本文介绍了一种名为MetaLLM的高性能和成本效益动态框架，用于包装大型语言模型（LLMs）。随着机器学习的快速发展，出现了许多在各种任务和领域中表现出色的LLMs。这些模型在计算或定价方面具有不同的能力和成本。由于每个查询的需求可能因查询的领域或复杂性而异，因此在应用程序中默认使用一个LLM（无论是最大、最昂贵或平均测试性能最好的）通常不是最佳选择。因此，为应用程序选择既准确又成本效益高的LLM仍然是一个挑战。MetaLLM框架通过动态智能地将每个查询路由到最优的LLM（在多个可用的LLMs中），实现了显著提高的准确性和成本效益。通过将选择问题框架化为多臂老虎机问题，MetaLLM在不确定性下平衡了预测准确性和成本效率。实验结果显示，MetaLLM在实际场景中的有效性，为未来超越分类任务的扩展奠定了基础。<!--more-->

## 原理

MetaLLM的核心工作原理是将选择最佳LLM的问题框架化为一个多臂老虎机问题。在这个框架中，每个输入查询被视为一个“臂”，模型的目标是选择能够以最低成本提供正确答案的LLM。具体来说，MetaLLM使用一个基于多臂老虎机的算法来动态选择LLM，该算法不需要查询任何LLMs即可做出路由决策。该算法通过计算每个“臂”（即LLM）的预期奖励，选择预期奖励最高的LLM来处理查询。预期奖励是根据LLM的性能和成本计算的，从而在保证准确性的同时优化成本。

## 流程

MetaLLM的工作流程包括训练和推理两个阶段。在训练阶段，MetaLLM使用Sentence-BERT模型提取查询的嵌入向量，并通过线性模型将这些向量映射到奖励期望。模型通过最小化真实奖励和预测奖励之间的均方损失来优化。在推理阶段，MetaLLM计算每个LLM对查询的预期奖励，并将查询路由到预期奖励最高的LLM。例如，对于一个情感分析查询，MetaLLM会评估每个LLM的性能和成本，选择最合适的LLM来处理该查询。

## 应用

MetaLLM框架的应用前景广泛，特别是在需要高准确性和成本效益的自然语言处理任务中，如文本分类、情感分析等。由于其能够动态选择最优的LLM，MetaLLM可以显著降低成本并提高性能，适用于各种商业和研究场景。此外，该框架的设计允许未来扩展到其他语言任务，只需调整奖励函数以包含适当的评估响应质量的指标。