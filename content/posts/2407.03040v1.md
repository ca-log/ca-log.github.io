---
author: 'Xia Hou,Qifeng Li,Jian Yang,Tongliang Li,Linzheng Chai,Xianjie Wu,Hangyuan Ji,Zhoujun Li,Jixuan Nie,Jingbo Dun,Wenfeng Song'
title: '"R2S框架：利用对话逻辑链提升大型语言模型的多轮对话能力"'
date: '2024-07-03'
Lastmod: '2024-07-05'
description: 'Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction Tuning for Large Language Model](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.03040v1.pdf_0.jpg)](https://arxiv.org/abs/2407.03040v1)

## 摘要

本文介绍了一种名为R2S的创新框架，该框架利用对话逻辑链（CoD）指导大型语言模型（LLMs）生成知识密集型的多轮对话，用于指令调优。通过整合开源数据集和领域特定的网络爬取文档到一个名为K-BENCH的基准中，覆盖了包括英语维基百科、中文科学和中文文物等多个领域。R2S方法首先确定当前对话的逻辑流程，然后提示LLMs生成关键短语以搜索相关响应内容。这种方法使得创建GINSTRUCT指令数据集成为可能，该数据集在对话式交互中保留了原始文档知识。利用这一数据集，我们微调了GLLM模型，该模型旨在将原始文档转换为结构化的多轮对话，从而将全面的领域知识注入到SFT模型中，以增强指令调优。这项工作标志着在提高LLMs处理和生成更准确、上下文相关响应的适应性和有效性方面迈出了重要一步。<!--more-->

## 原理

R2S框架的核心在于利用对话逻辑链（CoD）来指导LLMs生成合理且自然的知识密集型多轮对话。CoD通过预定义的对话类型（如意见交换、信息性问答、任务导向问答等）来引导对话的每一轮，确保对话的连贯性和上下文相关性。此外，CoD还包括搜索-发现组件，该组件能够引入事实知识到对话中，增强回答的准确性。与传统的输入-输出对提示构造方法不同，CoD引入了中间推理步骤，指导模型向最终输出的方向发展。

## 流程

R2S框架的工作流程包括三个主要步骤：首先，从原始文档中提取信息并确定对话类型；其次，使用LLMs生成关键短语以搜索相关信息，并生成相应的对话内容；最后，通过综合评估确保生成的多轮对话在信息性、理解性、有用性等方面达到高标准。例如，在处理关于Zigui County的旅游信息时，用户询问当地的旅游景点，系统通过搜索相关文档内容，生成包括Qu Yuan's hometown、QuYuan Temple等景点的回答。

## 应用

R2S框架的应用前景广泛，特别是在需要复杂对话管理和知识密集型交互的领域，如客户服务、教育辅导、专业咨询等。通过有效地将原始文档知识整合到多轮对话中，R2S能够提升LLMs在处理特定领域任务时的性能和用户体验。