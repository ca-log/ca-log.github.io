---
author: 'TechScribe'
title: '安全强化学习的新篇章：基于反馈的成本函数推断与新颖性采样机制'
date: '2024-06-28'
Lastmod: '2024-07-05'
description: 'Safety through feedback in Constrained RL'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Safety through feedback in Constrained RL](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.19626v1.pdf_0.jpg)](https://arxiv.org/abs/2406.19626v1)

## 摘要

本文探讨了在安全关键的强化学习（RL）环境中，通过引入额外的成本函数来确保代理的安全行为，而不是修改奖励函数。设计或评估这样的成本函数在实际应用中非常复杂且昂贵，例如在自动驾驶领域。为了解决这一问题，本文提出了一种方法，该方法可以在训练轮之间从离线收集的反馈中学习成本函数。反馈可以由系统生成或由观察训练过程的人类提供。本文引入了一种能够扩展到更复杂领域并接受超越状态级别反馈的方法，从而减轻评估者的负担。为了解决在基于轨迹级别反馈推断成本函数时分配信用给个别状态的挑战，本文提出了一种代理目标，将问题转化为带有噪声标签的状态级别监督分类任务，可以高效解决。此外，由于不可能收集代理生成的每条轨迹的反馈，本文引入了一种基于新颖性的采样机制，仅在代理遇到新颖轨迹时才涉及评估者，并在轨迹不再新颖时停止查询。通过在多个基准安全环境和高保真自动驾驶场景中进行实验，本文的方法在许多领域中仅从轨迹级别的反馈就实现了接近最优的性能，突出了其有效性和可扩展性。<!--more-->

## 原理

本文提出的方法主要通过以下几个关键步骤实现安全强化学习：
1. **反馈收集**：在每个训练阶段，代理根据当前策略执行一系列轨迹，并从中选择一部分轨迹供评估者（人类或系统）提供反馈。
2. **成本函数推断**：利用收集到的反馈数据，通过一个代理损失函数将轨迹级别的成本推断问题转化为状态级别的二元分类问题。这个代理损失函数通过最大化似然来推断每个状态的安全概率。
3. **新颖性采样**：为了减少对评估者的负担，引入了一种基于新颖性的采样机制，仅在代理遇到新颖轨迹时才请求反馈，并在轨迹不再新颖时停止查询。
4. **政策优化**：利用推断出的成本函数和收集到的轨迹，通过任何约束强化学习算法（如PPO-Lagrangian）来更新策略，确保策略在满足安全约束的同时优化性能。

## 流程

1. **数据收集阶段**：代理根据当前策略执行一系列轨迹，并从中选择一部分轨迹供评估者提供反馈。
2. **反馈收集阶段**：评估者对选定的轨迹进行评估，并提供关于轨迹是否安全的二元反馈。
3. **成本推断阶段**：利用收集到的反馈数据，通过代理损失函数推断每个状态的安全概率，并构建成本函数。
4. **政策优化阶段**：利用推断出的成本函数和收集到的轨迹，通过约束强化学习算法更新策略，确保策略在满足安全约束的同时优化性能。
5. **重复迭代**：重复上述步骤，直到策略收敛。

## 应用

本文提出的方法在自动驾驶、机器人操作和其他需要高度安全性的复杂任务中具有广泛的应用前景。通过从反馈中学习成本函数，可以在不明确知道成本函数的情况下实现安全强化学习，这对于实际应用中的复杂和动态环境尤为重要。此外，该方法的可扩展性和高效性使其能够适应不同规模和复杂度的任务，为未来的安全强化学习研究提供了新的方向。