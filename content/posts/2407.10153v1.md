---
author: 'TechScribe'
title: '探索LLMs幻觉之谜：因果视角下的自注意力机制干预'
date: '2024-07-14'
Lastmod: '2024-07-16'
description: 'Look Within, Why LLMs Hallucinate: A Causal Perspective'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Look Within, Why LLMs Hallucinate: A Causal Perspective](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.10153v1.pdf_0.jpg)](https://arxiv.org/abs/2407.10153v1)

## 摘要

本文探讨了大型语言模型（LLMs）中的幻觉问题，这是一个在文本理解和生成任务中普遍存在的现象。尽管LLMs在多个下游任务中取得了显著成功，但其幻觉问题严重影响了实际应用。传统的研究主要集中在数据质量上，而本文从因果关系的角度出发，研究了LLMs中的自注意力机制与幻觉之间的关系。文章提出了一种干预自注意力层的方法，通过禁用特定的自注意力层来减轻幻觉问题，并在多个开源LLMs上进行了实验验证。<!--more-->

## 原理

本文通过建立一个结构因果模型（SCM）来描述LLMs生成包含幻觉内容的过程。利用前门准则，将包含幻觉内容的自注意力层定义为中介变量，并通过前门调整来消除幻觉。具体方法是在LLMs的前向过程中，将特定自注意力层的所有自注意力头的输出张量修改为零张量，从而在不影响模型结构和大小的情况下干预自注意力层。

## 流程

1. 建立一个结构因果模型（SCM）来描述LLMs生成包含幻觉内容的过程。
2. 根据前门准则，定义包含幻觉内容的自注意力层为中介变量。
3. 开发一种方法，通过在前向过程中禁用特定自注意力层的所有自注意力头的输出，来干预自注意力层。
4. 在多个开源LLMs上进行实验，评估干预前后模型在幻觉检测基准上的性能。
5. 结果显示，禁用某些特定的自注意力层（尤其是前端或尾部的层）可以显著减轻幻觉问题。

## 应用

本文的方法为理解和减轻LLMs的幻觉问题开辟了新的途径。未来，这种方法可以应用于更广泛的LLMs模型，提高其在实际应用中的可靠性和准确性，特别是在医疗、法律和教育等对信息准确性要求极高的领域。