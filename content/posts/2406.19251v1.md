---
author: 'TechScribe'
title: 'AutoRAG-HP：革新RAG系统超参数优化的在线多臂老虎机方法'
date: '2024-06-27'
Lastmod: '2024-07-05'
description: 'AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.19251v1.pdf_0.jpg)](https://arxiv.org/abs/2406.19251v1)

## 摘要

本文介绍了一种名为AutoRAG-HP的框架，旨在解决Retrieval-Augmented Generation (RAG)系统中超参数优化和在线适应的挑战。该框架将超参数调整问题形式化为一个在线多臂老虎机（MAB）问题，并引入了一种新颖的两级层次MAB（Hier-MAB）方法，以高效探索大型搜索空间。通过在ALCE-ASQA和Natural Questions数据集上进行广泛实验，证明了基于MAB的在线学习方法在搜索空间具有显著梯度的场景中可以达到Recall@5 ≈ 0.8，仅使用Grid Search方法所需LLM API调用的约20%。此外，提出的Hier-MAB方法在更具挑战性的优化场景中表现优于其他基线。<!--more-->

## 原理

AutoRAG-HP框架的核心在于将RAG系统中的超参数选择问题转化为一个多臂老虎机（MAB）问题。MAB问题是一种在线学习方法，通过在多个选项（即“臂”）中进行选择，目标是最大化累积奖励。在RAG系统中，每个“臂”代表一组特定的超参数组合。通过不断尝试不同的超参数组合并观察其性能（即“奖励”），系统可以逐步学习到哪些超参数组合能产生最佳结果。为了更高效地探索大型搜索空间，论文提出了一种两级层次MAB（Hier-MAB）方法。在这种方法中，一个高级别的MAB负责选择要调整的超参数，而几个低级别的MAB则在每个超参数的搜索空间内寻找最佳设置。这种层次结构确保了每个MAB都有合理的臂数可供选择，同时所有MAB组合可以覆盖大型搜索空间。

## 流程

AutoRAG-HP的工作流程包括以下步骤：
1. **问题形式化**：将超参数调整问题形式化为一个MAB问题。
2. **高级别MAB选择**：高级别MAB选择要调整的超参数。
3. **低级别MAB搜索**：低级别MAB在选定的超参数搜索空间内进行详细搜索。
4. **评估与更新**：根据观察到的性能（奖励）更新对每个臂的奖励估计。
5. **迭代优化**：重复上述步骤，直到达到预定的迭代次数或满足特定的停止条件。

例如，在一个RAG系统中，高级别MAB可能首先选择调整“top-k retrieved documents”超参数，然后低级别MAB会在可能的k值（如k=1, 3, 5等）中进行选择，并根据性能更新奖励估计。

## 应用

AutoRAG-HP框架的应用前景广泛，特别是在需要高效且自适应优化的大型语言模型（LLM）和RAG系统中。该框架可以应用于各种超参数优化任务，包括但不限于检索模块、提示压缩模块和其他基于LLM的解决方案（如代理框架）。随着LLM和RAG系统的不断发展，AutoRAG-HP有望在提高系统性能和降低计算成本方面发挥重要作用。