---
author: 'TechScribe'
title: 'XEUS：突破性的多语言语音编码器，引领自监督学习新纪元'
date: '2024-06-30'
Lastmod: '2024-07-05'
description: 'Towards Robust Speech Representation Learning for Thousands of Languages'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Towards Robust Speech Representation Learning for Thousands of Languages](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00837v2.pdf_0.jpg)](https://arxiv.org/abs/2407.00837v2)

## 摘要

本文介绍了一种名为XEUS的新型跨语言通用语音编码器，该编码器通过自监督学习（SSL）在超过100万小时的4057种语言数据上进行预训练。XEUS通过结合现有的公开可用语料库和新的7400+小时的多语言语料库，显著扩展了SSL模型的语言覆盖范围。为了增强模型对多语言语音数据多样性的处理能力，XEUS引入了一种新的去混响目标，提高了模型的鲁棒性。实验结果显示，XEUS在多个基准测试中持续优于或达到与最先进（SOTA）SSL模型相当的性能，特别是在ML-SUPERB基准测试中，XEUS在参数较少或预训练数据较少的情况下，分别比MMS 1B和w2v-BERT 2.0 v2高出0.8%和4.4%。<!--more-->

## 原理

XEUS基于HuBERT架构，通过引入E-Branchformer（一种卷积增强模型）和交叉熵损失函数进行了改进。模型通过掩蔽预测和去噪目标进行预训练，其中去混响目标要求模型从模拟的混响音频中预测干净的离散音素伪标签。这种结合了去混响和去噪的预训练方法，使得XEUS能够处理各种语言和录音条件下的语音数据，从而提高了模型的鲁棒性和泛化能力。

## 流程

XEUS的预训练流程包括数据准备、特征提取、掩蔽预测、去噪和去混响等多个步骤。首先，从37个公开可用的语料库中筛选和准备数据，确保语音和录音条件的多样性。然后，使用预训练的WavLabLM模型提取特征，并通过k-means聚类生成伪标签。在训练过程中，模型会随机应用噪声或混响增强，同时保持目标标签不变，使模型学习从受损音频中恢复原始信号。最终，模型通过两遍训练集的迭代完成预训练。

## 应用

XEUS的应用前景广泛，包括但不限于多语言自动语音识别（ASR）、语音翻译（ST）、语音合成和情感识别等。其强大的语言覆盖能力和鲁棒性使其特别适合于低资源语言和多样录音条件下的语音处理任务。此外，XEUS的开放性和可复现性也为未来的多语言语音研究提供了坚实的基础。