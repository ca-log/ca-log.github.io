---
author: 'TechScribe'
title: '"Re-Tuning：突破大型语言模型的组合性限制"'
date: '2024-07-05'
Lastmod: '2024-07-10'
description: 'Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Re-Tuning: Overcoming the Compositionality Limits of Large Language Models with Recursive Tuning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.04787v1.pdf_0.jpg)](https://arxiv.org/abs/2407.04787v1)

## 摘要

本文介绍了一种名为Re-Tuning的新方法，旨在帮助大型语言模型（LLM）解决组合任务。尽管LLM在传统的语言理解任务中表现出色，但它们在解决组合任务时遇到困难，这些任务的解决方案依赖于解决同一问题的较小实例。Re-Tuning方法通过递归地调整模型，将问题分解为子问题，解决这些子问题，并将结果结合起来，从而显著提高了模型在整数加法、动态规划和奇偶性等代表性组合任务上的性能。与保持解决问题的中间步骤的现有最先进方法相比，Re-Tuning不仅实现了更高的准确性，而且在GPU内存效率方面表现更佳。<!--more-->

## 原理

Re-Tuning方法的核心在于递归地调整大型语言模型（LLM）以解决组合任务。具体来说，该方法包括三个关键步骤：首先，模型递归地调用自身，将问题分解为越来越小的子问题；其次，解决基本情况，即可以直接在同一上下文中解决的简单问题；最后，将子问题的解决方案沿着递归调用栈向上传递，逐步解决越来越复杂的子问题，直至解决原始问题。这种方法的先进性在于其能够有效地处理组合任务的递归性质，通过将问题分解为更小的部分，模型可以更专注于相关上下文，从而提高解决每个子问题的准确性。

## 流程

Re-Tuning的工作流程包括以下几个步骤：首先，模型接收一个组合任务，例如整数加法“1234 + 4567”，然后递归地调用自身，将问题分解为更小的子问题，如“234 + 567”。接着，模型在新上下文中解决这些子问题，并将解决方案返回给原始上下文。这个过程不断重复，直到解决最小的子问题，例如“4 + 7”。最后，所有子问题的解决方案被组合起来，形成原始问题的最终答案。整个过程通过递归调用栈管理，确保每个子问题的解决方案都能正确地传递和组合。

## 应用

Re-Tuning方法的应用前景广泛，特别是在需要递归计算的任务中，如数学问题解决、编程问题求解和复杂逻辑推理等。由于其能够有效地处理组合任务的递归性质，Re-Tuning有望在教育、软件开发和人工智能研究等领域发挥重要作用。此外，该方法在提高模型性能的同时，还保持了较高的GPU内存效率，使其在资源受限的环境中尤为适用。