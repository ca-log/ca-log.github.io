---
author: 'TechScribe'
title: 'ShiftAddAug：通过混合计算增强无乘法小型神经网络，实现高效能与低能耗'
date: '2024-07-03 07:56:51+00:00'
Lastmod: '2024-07-04 01:53:02.752236'
description: 'ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid Computation'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![ShiftAddAug: Augment Multiplication-Free Tiny Neural Network with Hybrid Computation](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02881v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02881v1)

## 摘要

本文介绍了一种名为ShiftAddAug的创新方法，旨在通过混合计算增强无乘法的小型神经网络，从而在不增加推理开销的情况下显著提高其准确性。该方法通过将无乘法的小型神经网络嵌入到一个更大的乘法模型中进行训练，利用乘法部分作为增强，推动目标无乘法模型达到更好的状态。此外，为了解决混合运算符之间的权重差异问题，提出了一种新的异构权重共享策略。ShiftAddAug在图像分类和语义分割任务中进行了实验验证，结果显示其能够显著提升无乘法神经网络的性能。<!--more-->

## 原理

ShiftAddAug的核心原理是通过将无乘法的小型神经网络（如ShiftConv或AddConv）嵌入到一个更大的乘法神经网络中进行训练。在训练过程中，乘法部分作为增强，帮助无乘法部分学习到更多的信息。为了解决不同运算符之间权重分布不一致的问题，提出了一种异构权重共享策略，通过映射函数将权重从一种分布转换到另一种分布，确保权重在不同运算符间的有效共享。此外，ShiftAddAug还采用了一种两阶段的神经架构搜索策略，以进一步优化无乘法小型神经网络的结构。

## 流程

ShiftAddAug的工作流程包括以下几个关键步骤：
1. **嵌入与增强**：将无乘法的小型神经网络嵌入到一个更大的乘法神经网络中，乘法部分作为增强，帮助无乘法部分学习。
2. **异构权重共享**：通过映射函数将权重从一种分布转换到另一种分布，确保权重在不同运算符间的有效共享。
3. **两阶段神经架构搜索**：首先从搜索空间中提取一个增强的大型网络，然后在增强的网络中搜索可部署的小型网络。
4. **训练与部署**：在训练过程中，更新权重并重新排序重要权重，最终只部署无乘法部分。

## 应用

ShiftAddAug方法在资源受限的平台上具有广泛的应用前景，特别是在边缘设备上的图像分类和语义分割任务。由于其能够在不增加推理开销的情况下提升模型性能，因此非常适合于需要高效能和低能耗的场景，如物联网设备和移动设备。