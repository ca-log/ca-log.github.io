---
author: 'TechScribe'
title: '多智能体训练新突破：课程学习与自对弈策略在Pommerman中的应用'
date: '2024-06-30'
Lastmod: '2024-07-05'
description: 'Multi-Agent Training for Pommerman: Curriculum Learning and Population-based Self-Play Approach'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Multi-Agent Training for Pommerman: Curriculum Learning and Population-based Self-Play Approach](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00662v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00662v1)

## 摘要

本文介绍了一种用于训练多智能体系统在Pommerman环境中进行游戏的方法，结合了课程学习和基于种群的自对弈策略。Pommerman是一个多智能体环境，具有延迟动作效果、稀疏奖励和假阳性奖励等挑战。本文提出的系统通过课程学习阶段帮助智能体逐步掌握游戏技能，随后在自对弈阶段通过种群内的竞争自然进化策略。此外，本文还解决了稀疏奖励和有效的匹配机制问题，通过引入基于智能体性能的自适应退火因子调整密集探索奖励，并利用Elo评分系统进行智能体匹配。实验结果显示，训练出的智能体在没有团队通信的情况下，能够超越顶级学习智能体。<!--more-->

## 原理

本文的核心在于两个阶段的训练系统：课程学习和基于种群的自对弈。课程学习阶段通过逐步增加难度的三个阶段，帮助智能体掌握基本技能，如地图探索、物品拾取和防御策略。自对弈阶段则通过种群内的智能体相互竞争，促进策略的自然进化。关键的创新点包括自适应退火因子，它根据智能体的性能动态调整探索奖励，以及基于Elo评分的匹配机制，确保智能体在训练中有效配对，促进渐进式学习。

## 流程

1. **课程学习阶段**：智能体首先与静态对手对战，学习基本的地图探索和物品拾取技能。随后，智能体与移动对手对战，学习更复杂的策略。最后，智能体与能够放置炸弹的对手对战，学习防御和攻击策略。
2. **自对弈阶段**：智能体在一个包含八个智能体的种群中进行对战，通过Elo评分系统进行匹配。智能体的Elo评分根据对战结果动态更新，确保智能体与适当难度的对手对战。
3. **奖励调整**：在课程学习阶段，通过自适应退火因子动态调整探索奖励，帮助智能体从密集探索奖励过渡到稀疏游戏奖励。在自对弈阶段，智能体仅基于游戏奖励进行训练。

## 应用

本文提出的方法不仅适用于Pommerman游戏，还可以推广到其他多智能体环境，如机器人协作、自动驾驶和复杂策略游戏等。通过有效的训练系统和智能体匹配机制，可以显著提高多智能体系统的性能和适应性，具有广泛的应用前景。