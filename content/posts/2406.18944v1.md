---
author: 'TechScribe'
title: '"突破个性化扩散模型的安全防线：对抗性扰动与净化策略的深度探究"'
date: '2024-06-27'
Lastmod: '2024-07-05'
description: 'Investigating and Defending Shortcut Learning in Personalized Diffusion Models'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Investigating and Defending Shortcut Learning in Personalized Diffusion Models](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.18944v1.pdf_0.jpg)](https://arxiv.org/abs/2406.18944v1)

## 摘要

本文探讨了个性化扩散模型在适应预训练文本到图像模型以生成特定主题图像时面临的挑战，特别是在面对微小的对抗性扰动时模型的脆弱性。研究指出，这些模型在损坏的数据集上的微调性能大幅下降，并可能被利用来保护敏感图像免受未经授权的生成。为了应对这一问题，研究者提出了基于扩散的净化方法，以移除这些扰动并保持生成性能。然而，现有工作缺乏对个性化扩散模型基本捷径学习漏洞的详细分析，并且倾向于过度净化图像，导致信息丢失。本文通过深入分析个性化扩散模型的微调过程，提出了一个假设，解释了现有扰动方法的底层操纵机制。具体来说，研究者发现扰动图像在其基于CLIP的潜在空间中与其原始配对提示存在显著偏移，导致模型在训练时学习错误的概念映射，从而引发严重的性能下降。基于这一观察，本文提出了一种系统方法，通过净化重新对齐潜在图像及其语义意义，并引入带有负标记的对比学习，以解耦所需清洁身份和不需要的噪声模式的学习，显示出对进一步自适应扰动的强大潜力。本研究有助于更好地理解个性化扩散模型中的捷径学习漏洞，并为未来的保护性扰动研究提供了坚实的评估框架。<!--more-->

## 原理

本文的核心在于揭示和解决个性化扩散模型在面对对抗性扰动时的脆弱性。通过分析模型在潜在空间中的表现，研究者发现扰动图像与其原始提示之间的不匹配是导致模型性能下降的关键因素。具体来说，扰动图像在CLIP潜在空间中与其原始提示的偏移，导致模型在训练时学习到错误的概念映射。为了解决这一问题，本文提出了一种系统的方法，包括输入图像-文本对的净化、对比学习与负标记的解耦学习以及质量增强采样。这些方法共同作用，通过重新对齐图像与提示的潜在表示，以及通过对比学习强化模型对正确概念的学习，从而提高模型对抗扰动的能力。

## 流程

本文提出的方法包括三个主要步骤：输入净化、解耦学习和引导采样。首先，通过使用现成的图像恢复模型，将低质量、噪声图像转换为高质量、净化的图像，以去除对抗性扰动。其次，在提示设计中加入与噪声模式相关的额外标签，如“带有XX噪声模式”，旨在解耦概念学习与不希望的噪声结构。同时，利用先验数据集帮助模型区分噪声模式，通过在文本提示中添加“没有XX噪声模式”的后缀。最后，在采样过程中，使用无分类器引导（CFG）与负提示“噪声、抽象、模式、低质量”，引导模型生成与学习概念相关的高质量图像。

## 应用

本文提出的方法不仅在面部数据集上进行了评估，而且其框架设计具有通用性，可以适应各种图像领域，如对象和艺术品。这表明该方法可能被用于进一步侵犯那些已经通过先前扰动“保护”的数据，这可能产生一些负面社会影响，但也可能推动该领域的进步。此外，本文还发现CodeFormer在面对自适应攻击时可能更脆弱，但在净化方面表现更好，因此仍有空间通过确定SR和CodeFormer模块的最佳层组合来最大化鲁棒性与有效性之间的权衡。