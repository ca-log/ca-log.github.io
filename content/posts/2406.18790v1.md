---
author: 'TechScribe'
title: 'MUMU：引领多模态图像生成的新纪元'
date: '2024-06-26'
Lastmod: '2024-07-05'
description: 'MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![MUMU: Bootstrapping Multimodal Image Generation from Text-to-Image Data](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.18790v1.pdf_0.jpg)](https://arxiv.org/abs/2406.18790v1)

## 摘要

本文介绍了一种名为MUMU的新型多模态图像生成模型，该模型能够从交错的文本和图像提示中生成图像，例如“一个<男人的照片>男人和他的<狗的照片>狗在一个<卡通风格的照片>动画风格中”。MUMU通过从现有的文本-图像数据中提取语义上有意义的图像裁剪来启动多模态数据集。该模型由一个视觉-语言模型编码器和一个扩散解码器组成，并在单个8xH100 GPU节点上进行训练。尽管仅在同一图像的裁剪上进行训练，MUMU能够将来自不同图像的输入组合成连贯的输出，例如，将一个现实人物和一个卡通图像输入，输出为该人物的卡通风格图像。这使得模型能够泛化到风格转换和角色一致性等任务。研究结果显示了使用多模态模型作为图像生成的通用控制器的潜力。<!--more-->

## 原理

MUMU模型的核心在于其独特的多模态处理能力。该模型通过替换扩散模型中的文本编码器CLIP，使用了一个视觉-语言模型（VLM）Idefics2，从而实现了对多模态提示的理解和处理。Idefics2由一个视觉变换器（ViT）、一个感知器变换器和一个大型视觉-语言模型变换器组成。在MUMU中，去除了感知器变换器，以使用更多的图像令牌，这有助于提高图像质量。此外，模型还增加了一个小的非因果“适配器”变换器在Idefics2的隐藏状态之上，进一步增强了模型的性能。这种架构设计使得MUMU能够直接将条件图像放置到生成的图像中，并且能够将来自不同输入的条件图像协调成连贯的输出。

## 流程

MUMU的工作流程包括两个主要阶段。首先，模型从文本-图像数据中提取图像裁剪，并将这些裁剪放置在文本提示中相应单词之前，形成多模态提示。其次，模型使用这些多模态提示进行训练，通过迭代应用去噪过程生成图像。在训练过程中，模型使用SDXL的预训练卷积UNet，并通过交叉注意力机制处理CLIP隐藏状态。MUMU通过替换SDXL的主要CLIP文本编码器，使用Idefics2的隐藏状态，进一步优化了模型的性能。例如，当输入一个现实人物和一个卡通图像时，模型能够输出该人物的卡通风格图像，展示了其强大的图像生成和风格转换能力。

## 应用

MUMU模型的应用前景广泛，特别是在需要高度定制化和交互式图像生成的领域。例如，在游戏开发、电影制作、虚拟现实和增强现实等领域，MUMU可以用于快速生成和修改角色、场景和物品的图像。此外，MUMU还可以用于艺术创作、广告设计和个性化内容生成，为用户提供更加丰富和个性化的视觉体验。随着技术的进一步发展和优化，MUMU有望成为图像生成领域的通用控制器，推动相关行业的创新和进步。