---
author: 'TechScribe'
title: 'HuatuoGPT-Vision：引领医学多模态智能的新纪元'
date: '2024-06-27'
Lastmod: '2024-07-05'
description: 'HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.19280v1.pdf_0.jpg)](https://arxiv.org/abs/2406.19280v1)

## 摘要

本文介绍了一种名为HuatuoGPT-Vision的新型多模态大型语言模型（MLLM），旨在通过大规模注入医学视觉知识来提升模型的医学多模态能力。该研究通过从PubMed中精心筛选高质量的医学图像-文本对，并利用GPT-4V模型进行数据去噪和重构，创建了一个包含130万医学视觉问答（VQA）样本的数据集——PubMedVision。实验证明，PubMedVision能显著增强现有MLLM的医学多模态能力，并在多个基准测试中显示出显著改进。此外，使用PubMedVision训练的34B参数医学MLLM HuatuoGPT-Vision在开放源代码的MLLM中表现出卓越的性能。<!--more-->

## 原理

HuatuoGPT-Vision的核心在于其能够利用“非盲”的MLLM（如GPT-4V）来重构和去噪PubMed中的医学图像-文本对，从而构建出高质量的医学VQA数据集。这种方法通过结合图像的视觉信息和上下文文本，生成更为准确和相关的描述，进而提升模型的医学图像理解和问答能力。通过这种方式，模型不仅能够理解图像中的医学特征，还能根据上下文提供更为精确的解释和答案。

## 流程

1. 数据收集：从PubMed中筛选出高质量的医学图像和相应的文本描述。
2. 数据重构：使用GPT-4V模型对筛选出的图像-文本对进行去噪和重构，生成高质量的医学VQA数据。
3. 数据集构建：将重构后的数据集命名为PubMedVision，包含130万医学VQA样本。
4. 模型训练：使用PubMedVision数据集训练HuatuoGPT-Vision模型，优化其在医学多模态场景中的性能。
5. 性能验证：通过在多个医学VQA基准测试中进行验证，展示HuatuoGPT-Vision的优越性能。

## 应用

HuatuoGPT-Vision模型在医学图像分析、疾病诊断辅助、医学教育等多个领域具有广泛的应用前景。其能够帮助医生更准确地解读医学影像，辅助临床决策，同时也可用于医学教育和培训，提高医学专业人员的技能和效率。