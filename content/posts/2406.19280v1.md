---
author: 'TechScribe'
title: 'HuatuoGPT-Vision：引领医学多模态大型语言模型的新纪元'
date: '2024-06-27'
Lastmod: '2024-07-05'
description: 'HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.19280v1.pdf_0.jpg)](https://arxiv.org/abs/2406.19280v1)

## 摘要

本文介绍了一种名为HuatuoGPT-Vision的新型多模态大型语言模型（MLLM），旨在通过大规模注入医学视觉知识来提升模型的医学多模态能力。该研究通过精心筛选和处理PubMed中的医学图像-文本对，利用GPT-4V模型进行数据去噪和格式化，构建了一个包含130万医学视觉问答（VQA）样本的高质量数据集PubMedVision。实验验证表明，PubMedVision显著提升了现有MLLM的医学多模态能力，并在多个基准测试中显示出优越性能。此外，使用PubMedVision训练的34B参数医学MLLM HuatuoGPT-Vision在开放源代码的MLLM中表现出最佳性能。<!--more-->

## 原理

HuatuoGPT-Vision的核心工作原理是通过“非盲”方式利用GPT-4V模型对PubMed中的医学图像-文本对进行去噪和格式化。传统的“盲”方式仅依赖文本信息，而“非盲”方式则结合了图像和文本信息，从而更准确地理解和描述医学图像内容。这种双模态处理方式有效减少了数据中的噪声，提高了数据质量，进而提升了模型的医学多模态理解和问答能力。

## 流程

1. 数据收集与筛选：从PubMed中收集大量医学图像和相关文本，通过文本和图像过滤器筛选出高质量的图像-文本对。
2. 数据去噪与格式化：使用GPT-4V模型对筛选后的数据进行去噪和格式化，生成高质量的医学VQA数据。
3. 数据集构建：将处理后的数据构建为PubMedVision数据集，包含130万医学VQA样本。
4. 模型训练：使用PubMedVision数据集训练HuatuoGPT-Vision模型，优化其在医学多模态场景中的性能。
5. 性能评估：在多个医学VQA基准测试中评估HuatuoGPT-Vision的性能，验证其优越性。

## 应用

HuatuoGPT-Vision及其构建的PubMedVision数据集在医学图像分析、疾病诊断、治疗规划等领域具有广泛的应用前景。通过提升模型的医学多模态能力，可以更准确地理解和解释医学图像，辅助医生进行临床决策，提高医疗服务的质量和效率。