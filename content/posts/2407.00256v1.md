---
author: 'TechScribe'
title: '“Mixture-of-Prompts：革新大型语言模型的提示优化方法”'
date: '2024-06-28'
Lastmod: '2024-07-05'
description: 'One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00256v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00256v1)

## 摘要

本文介绍了一种名为“Mixture-of-Prompts (MoP)”的自动化提示构建方法，旨在优化大型语言模型（LLMs）的提示设计。传统的提示优化方法通常局限于单一的演示无提示指令，这种方法在处理复杂任务时可能不足以覆盖整个问题空间。MoP通过采用混合专家（MoE）范式，将问题空间划分为多个子区域，并为每个子区域配备一个专门的专家（提示），每个专家包含一个指令和一组演示。文章详细介绍了MoP的两阶段构建过程：演示分配和指令分配，并通过实验证明了MoP在多个主要基准测试中平均胜率达到81%，显著优于现有方法。<!--more-->

## 原理

MoP的核心思想是将问题空间划分为多个同质区域，每个区域由一个专门的专家（提示）管理。这些专家不仅包含一个指令，还包含一组演示，这些演示和指令共同优化以适应每个专家区域的问题空间。具体来说，MoP通过以下步骤工作：
1. **演示分配**：基于上下文学习与核回归的理论联系，将演示根据其语义相似性分组到不同的专家中。
2. **指令分配**：为每个专家区域进行基于区域的联合搜索，找到最佳的指令来补充分配给该专家的演示，从而产生协同效应。
通过这种分而治之的方法，MoP能够显著扩大问题空间的覆盖范围，并提高LLMs的问题解决能力。

## 流程

MoP的工作流程包括两个主要阶段：
1. **演示分配阶段**：使用聚类算法根据演示的语义相似性将演示分配给不同的专家。
2. **指令分配阶段**：为每个专家区域进行基于区域的联合搜索，找到最佳的指令来补充分配给该专家的演示。
具体步骤如下：
- 在演示分配阶段，首先将所有演示映射到嵌入空间，然后使用K-means聚类算法将它们分组。
- 在指令分配阶段，为每个专家生成候选指令，并评估这些指令在区域验证集上的表现，选择最佳指令。
在推理时，每个新查询将被路由到嵌入空间中最接近的专家，并由该专家使用其提示（指令 + 演示）进行最终预测。

## 应用

MoP方法在多个领域具有广泛的应用前景，特别是在需要复杂问题解决和广泛知识应用的场景中。例如，在编程、数学、常识推理和知识检索等任务中，MoP能够提供更精确和高效的提示优化，从而提高LLMs的性能。此外，MoP的灵活性和可扩展性使其能够适应不断变化的任务需求和技术进步，为未来的AI应用提供强大的支持。