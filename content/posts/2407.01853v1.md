---
author: 'TechScribe'
title: '革新多语言处理：通过自然多样数据集提升大型语言模型性能'
date: '2024-07-01 23:47:09+00:00'
Lastmod: '2024-07-04 01:17:44.656262'
description: 'Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01853v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01853v1)

## 摘要

本文提出了一种创新方法，通过利用英语为中心的大型语言模型（LLM）、单语语料库和评分函数，来收集多语言指令微调（IFT）数据集，以保持语言的自然性和确保指令的多样性。实验结果表明，使用这些IFT数据集微调的LLM在生成性和判别性任务上均显示出显著的性能提升，特别是在多语言摘要任务上，相比基于翻译和模板的数据集，分别实现了17.57%和15.23%的改进。<!--more-->

## 原理

该方法的核心在于利用英语为中心的LLM生成高质量、多样化的指令，并通过单语语料库捕捉每种语言的独特语言和文化 nuances。具体步骤包括：选择响应、将响应翻译成英语、使用英语响应和任务特定提示生成英语指令、对生成的英语指令进行评分，以及将英语指令翻译回原始语言。评分函数确保了生成的指令与响应对的质量，从而提高了数据集的整体质量。

## 流程

1. **选择响应**：从单语语料库中提取文本片段，应用各种启发式方法过滤低质量片段。
2. **翻译响应**：将选定的响应翻译成英语。
3. **生成指令**：利用英语LLM和翻译后的响应生成英语指令。
4. **评分**：使用LLM作为评判，对生成的指令和响应对进行评分。
5. **翻译指令**：将英语指令翻译回原始语言，形成训练对。

## 应用

该方法生成的多语言IFT数据集可以广泛应用于各种语言处理任务，如机器翻译、文本摘要和情感分析等。这些数据集不仅提高了LLM在非英语环境下的性能，还为多语言应用提供了强大的支持，特别是在低资源语言的处理上具有重要意义。