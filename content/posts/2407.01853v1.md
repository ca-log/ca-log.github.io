---
author: 'TechScribe'
title: '"多语言指令微调的新纪元：提升大型语言模型的全球适应性"'
date: '2024-07-01'
Lastmod: '2024-07-05'
description: 'Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01853v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01853v1)

## 摘要

本文提出了一种创新方法，通过利用英语为中心的大型语言模型（LLMs）和单语语料库，以及一个评分函数，来收集多语言指令微调（IFT）数据集，以保持语言的自然性和确保指令的多样性。实验结果表明，使用这些IFT数据集微调的LLMs在生成和判别任务中均显示出显著的性能提升，特别是在多语言摘要任务中，相较于基于翻译和模板的数据集，取得了17.57%和15.23%的改进。<!--more-->

## 原理

该方法的核心在于利用英语为中心的LLMs生成高质量、多样化的指令，并通过单语语料库捕捉每种语言的独特语言和文化 nuances。评分函数用于控制生成IFT示例的质量，确保知识和技术从英语为中心的LLMs适当地适应和优化到非英语语言中。这种方法不仅避免了翻译错误，还通过多样化的指令提高了模型的泛化能力。

## 流程

1. **选择响应**：从单语语料库中提取文本片段，应用各种启发式方法过滤低质量片段。
2. **翻译响应**：将选定的响应翻译成英语。
3. **生成指令**：利用英语LLM和翻译后的响应生成英语指令。
4. **评分**：使用LLM作为评判，评估生成的指令和响应对的质量，筛选出高质量的示例。
5. **翻译指令**：将英语指令翻译回原始语言，形成训练对。

## 应用

该方法生成的多语言IFT数据集可以广泛应用于各种自然语言处理任务，如机器翻译、文本摘要和情感分析等。通过提高LLMs在非英语环境下的性能，这种方法有助于推动全球范围内的语言技术发展和应用，特别是在低资源语言的处理上具有重要意义。