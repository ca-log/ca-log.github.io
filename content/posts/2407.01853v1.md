---
author: 'TechScribe'
title: '创新方法提升多语言大型语言模型的性能'
date: '2024-07-01'
Lastmod: '2024-07-05'
description: 'Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01853v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01853v1)

## 摘要

本文提出了一种创新方法，通过利用英语为中心的大型语言模型（LLMs）和单语语料库，以及一个评分函数，来收集多语言指令微调（IFT）数据集。这种方法旨在保持语言的自然性和确保指令的多样性，从而提高LLMs在非英语环境中的语言理解和生成能力。实验结果表明，使用这种方法微调的LLMs在多语言摘要任务中表现显著优于基于翻译和模板的数据集微调的模型。<!--more-->

## 原理

该方法的核心在于利用英语为中心的LLMs生成高质量、多样化的多语言IFT数据集。具体步骤包括：首先从单语语料库中选择响应，然后将这些响应翻译成英语，利用LLM生成英语指令，并通过评分函数筛选出高质量的指令-响应对，最后将这些英语指令翻译回原始语言。这种方法不仅避免了翻译过程中的错误引入，还通过评分函数确保了数据集的质量和多样性。

## 流程

1. **选择响应**：从单语语料库中提取文本片段，应用各种启发式方法过滤低质量片段。
2. **翻译响应**：将选定的响应翻译成英语。
3. **生成指令**：利用LLM和翻译后的响应生成英语指令。
4. **评分**：使用LLM作为评判，对生成的指令-响应对进行评分，筛选出高质量的例子。
5. **翻译指令**：将英语指令翻译回原始语言，形成训练对。

## 应用

该方法生成的多语言IFT数据集可以广泛应用于各种自然语言处理任务，如机器翻译、文本摘要和情感分析等。通过提高LLMs在非英语环境中的性能，这种方法有助于推动全球范围内的语言技术发展和应用。