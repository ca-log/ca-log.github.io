---
author: 'TechScribe'
title: '探索AI红队测试的人类因素：社会与协作计算的新视角'
date: '2024-07-10'
Lastmod: '2024-07-11'
description: 'The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.07786v1.pdf_0.jpg)](https://arxiv.org/abs/2407.07786v1)

## 摘要

本文探讨了人工智能（AI）红队测试中的人类因素，这是一个源自军事和网络安全应用的对抗性测试实践。随着AI技术的快速发展，红队测试在AI领域的重要性日益增加，但同时也带来了一系列关于红队成员选择、测试过程中的偏见和盲点，以及有害内容对红队成员心理影响的问题。本文通过社会和协作计算的视角，分析了红队测试的概念、劳动实践、以及红队成员的福祉和潜在伤害，旨在建立一个跨学科的研究和实践网络，以促进AI红队测试的创新和负责任的发展。<!--more-->

## 原理

AI红队测试的核心在于通过模拟对抗性场景来测试AI系统的安全性和鲁棒性。红队成员，包括领域专家和国际众包工作者，扮演伪对抗角色，故意生成有害输出以测试AI系统的反应。这种方法借鉴了军事和计算机安全领域的红队测试历史，但其定义和方法随着AI技术的发展而不断演变。例如，Anthropic公司利用众包工作者测试AI系统，试图使其生成有害内容。这种测试不仅涉及技术挑战，还包括对红队成员心理健康的影响，因为他们在测试过程中会反复暴露于有害内容。

## 流程

本文提出了一种混合式工作坊，旨在通过一系列活动深入探讨AI红队测试的各个方面。工作坊包括介绍、红队测试练习、小组讨论、工具开发和分享环节。在红队测试练习中，参与者将被分成小组，通过模拟提示注入攻击来测试流行的LLM（如ChatGPT），并生成测试报告。通过这些活动，参与者将共同开发初步的研究议程和工具包，以支持AI红队测试的实践和研究。

## 应用

AI红队测试的应用前景广泛，涉及AI系统的安全性评估、伦理审查和劳动实践优化。通过建立跨学科的研究网络，本文旨在促进AI红队测试的负责任发展，确保AI系统的安全性和社会责任感。此外，关注红队成员的福祉和潜在伤害，将有助于制定有效的保护措施和干预策略，从而推动AI技术的健康发展。