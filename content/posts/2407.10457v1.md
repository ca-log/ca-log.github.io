---
author: 'TechScribe'
title: '探索LLMs的非确定性：评估与未来方向'
date: '2024-07-15'
Lastmod: '2024-07-16'
description: 'The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.10457v1.pdf_0.jpg)](https://arxiv.org/abs/2407.10457v1)

## 摘要

本文探讨了大型语言模型（LLMs）评估中常被忽视的非确定性问题。传统评估方法通常基于每个示例的单一输出，忽略了LLMs在不同解码配置下可能产生的多样性输出。研究通过对比贪婪解码和采样方法，分析了不同LLM在多个基准测试中的表现差异，揭示了非确定性对LLM性能评估的重要性，并提出了未来LLM开发和评估的新方向。<!--more-->

## 原理

本文通过对比贪婪解码和采样方法，分析了LLMs在不同解码配置下的输出多样性。贪婪解码选择每个时间步最高概率的下一个词，而采样方法（如Top-k和Top-p采样）则基于概率分布随机选择下一个词，增加了输出的多样性。研究还探讨了温度参数、重复惩罚等解码参数对生成过程的影响，以及模型大小和校准方法对非确定性的影响。

## 流程

研究选择了多个基准测试，包括AlpacaEval、Arena-Hard、WildBench v2、MixEval、MMLU-Redux、GSM8K和HumanEval，涵盖了指令遵循、知识、数学推理和代码生成等能力。实验中，模型在不同解码配置下生成多个输出，然后通过官方评估脚本进行评分。例如，在GSM8K基准测试中，贪婪解码的准确率显著高于采样方法。通过这种方式，研究揭示了不同解码配置对模型性能的影响。

## 应用

本文的研究结果表明，考虑非确定性对LLM评估至关重要。未来的应用前景包括：1）改进LLM的解码策略，以提高输出质量和多样性；2）开发新的基准测试，更全面地评估LLM的性能；3）探索模型校准和集成学习方法，以提升LLM在复杂任务上的表现。这些发现将为LLM的进一步研究和应用提供重要指导。