---
author: 'TechScribe'
title: '揭秘GPT的性别偏见：AI如何无意中强化刻板印象？'
date: '2024-07-08'
Lastmod: '2024-07-10'
description: 'Surprising gender biases in GPT'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Surprising gender biases in GPT](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.06003v1.pdf_0.jpg)](https://arxiv.org/abs/2407.06003v1)

## 摘要

本研究由Raluca Alexandra Fulgu和Valerio Capraro进行，旨在探讨大型语言模型（如GPT）中的性别偏见。通过七项实验，研究揭示了GPT在生成文本和处理道德困境时表现出明显的性别不对称性。例如，GPT更倾向于将典型的男性特征归因于女性，而在道德决策中，GPT认为对男性施加暴力以防止灾难比对女性施加暴力更为合适。这些发现强调了在管理包容性努力时需要谨慎，以避免无意中的歧视。<!--more-->

## 原理

研究通过设计特定的实验场景，测试了GPT在不同情境下的反应。在第一系列实验中，GPT被要求根据包含性别刻板印象的短语来推测作者的性别。结果显示，GPT更频繁地将男性特征归因于女性，这可能是由于在训练过程中过度强调了女性在传统男性领域的参与，而忽视了男性在传统女性领域的参与。在第二系列实验中，GPT在处理涉及暴力的高风险道德困境时，表现出对女性的保护倾向，认为对男性施加暴力比对女性更为合理，这种偏见在混合性别场景中更为明显。

## 流程

实验分为两个主要系列。第一系列实验中，GPT被提供了20对包含性别刻板印象的短语，每对短语分别包含典型的女性和男性特征。GPT的任务是推测这些短语的潜在作者的性别。第二系列实验中，GPT面临一系列道德困境，涉及在防止核灾难的情况下对不同性别个体施加暴力的合理性。每个问题GPT需要在一个7点Likert量表上给出回应，从“强烈不同意”到“强烈同意”。

## 应用

本研究的结果对于理解和改进AI模型中的性别偏见具有重要意义。在AI决策支持系统、内容生成和其他应用中，这些发现提示我们需要更细致地调整和监控模型，以确保它们不会无意中强化或创造新的性别偏见。未来的研究应进一步探索这些偏见的普遍性和边界条件，以及如何在模型设计和训练中更有效地消除这些偏见。