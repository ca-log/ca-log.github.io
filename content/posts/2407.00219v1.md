---
author: 'TechScribe'
title: '探索大型语言模型的理性提取：对齐与忠实度的深入研究'
date: '2024-06-28'
Lastmod: '2024-07-05'
description: 'Evaluating Human Alignment and Model Faithfulness of LLM Rationale'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Evaluating Human Alignment and Model Faithfulness of LLM Rationale](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00219v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00219v1)

## 摘要

本文探讨了大型语言模型（LLMs）如何通过提取的理性（rationales）解释其生成内容，这些理性是从输入文本中提取的令牌，反映了LLMs的决策过程。研究通过两种方法提取理性：基于归因的方法（使用注意力或梯度定位重要令牌）和基于提示的方法（通过提示引导LLMs提取理性）。实验表明，基于提示的理性与人工标注的理性对齐更好，即使在模型性能较差的情况下也能合理地与人类推理对齐。此外，研究发现基于提示的方法的忠实度限制可能与其预测崩溃有关。通过在相应数据集上微调这些模型，基于提示和归因的方法都显示出忠实度的提高。本研究为LLM理性的更严格和公平评估提供了见解，特别是对于基于提示的方法。<!--more-->

## 原理

本文的核心在于评估大型语言模型（LLMs）生成的理性（rationales）与人类标注的理性之间的对齐程度以及模型的忠实度。基于归因的方法通过模型内部的注意力权重或梯度来定位输入文本中的关键令牌，而基于提示的方法则通过向模型提供明确的提示来引导其提取理性。实验通过比较这两种方法在多个LLM架构和数据集上的表现，发现基于提示的方法在人类对齐方面表现更优，即使模型性能不佳时也能提供与人类推理相符的解释。然而，基于提示的方法在忠实度方面存在局限，这可能与其预测崩溃有关。通过微调模型，两种方法的忠实度都得到了提升，表明微调可以改善模型对输入文本的理解和解释能力。

## 流程

研究首先定义了理性的概念，并介绍了两种提取理性的方法：基于归因的方法和基于提示的方法。然后，研究团队在多个数据集上进行了广泛的实验，以评估这些方法的性能。实验包括对模型输出的理性与人工标注的理性进行比较，以及通过遮蔽关键令牌来测试模型的预测变化，从而评估理性的忠实度。例如，在e-SNLI数据集上，模型被要求识别出与给定标签最相关的关键令牌。通过这些实验，研究团队能够量化不同方法在人类对齐和模型忠实度方面的表现，并发现微调模型可以显著提高这些指标。

## 应用

本研究的结果表明，基于提示的理性提取方法在人类对齐方面表现优异，这使得它们在需要高度可解释性的应用场景中具有潜在价值，如法律、医疗和教育领域。此外，通过微调提高模型的忠实度，这些方法在需要高度可靠性的任务中也具有应用前景。未来，这些方法可以进一步优化，以提高其在更广泛应用场景中的性能和适用性。