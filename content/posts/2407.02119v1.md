---
author: 'TechScribe'
title: '"创新RLHF策略：成本效益高的代理奖励模型构建"'
date: '2024-07-02 10:09:19+00:00'
Lastmod: '2024-07-04 01:17:34.195803'
description: 'Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02119v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02119v1)

## 摘要

本文探讨了在大型语言模型管道中广泛采用的基于人类反馈的强化学习（RLHF）中，如何通过成本效益高的代理奖励模型构建策略来有效利用有限的标记数据和专家查询预算。传统的RLHF方法受限于人类偏好数据集的大小，而本文提出的方法通过在线设置，利用少量标记种子数据和大量未标记提示，迭代构建新的偏好数据。文章介绍了两种关键创新：（1）在线策略查询以避免种子数据中的分布外和失衡问题，（2）主动学习选择最具信息量的数据进行偏好查询。这些方法使得我们能够用最少的专家标记数据训练评估模型，进而有效地标记九倍多的偏好对，用于进一步的RLHF训练。例如，使用直接偏好优化（DPO）的模型在AlpacaEval2、MMLU5shot和MMLU-0shot上平均提高了1%，仅使用了1.7K的查询成本。本文的方法与其他基于直接专家查询的策略正交，因此可能与它们集成以进一步降低查询成本。<!--more-->

## 原理

本文的核心创新在于提出了两种策略来构建成本效益高的代理奖励模型：在线策略查询和主动学习。在线策略查询确保了数据集的分布与目标模型一致，避免了分布外（OOD）问题和数据失衡。主动学习则通过选择最具信息量的数据点进行查询，从而在有限的查询预算内最大化模型的性能提升。这两种策略的结合使得模型能够用极少的专家标记数据训练出一个有效的评估模型，该模型能够标记大量的偏好对，用于进一步的RLHF训练。

## 流程

本文的工作流程包括以下几个步骤：
1. 使用预训练模型M0和初始种子数据IFTseed进行SFT（监督微调）以获得M1。
2. 对于一组未标记的提示X，使用M1生成一组响应，形成响应池˜Y1和生成的样本IFT1。
3. 使用主动查询策略从IFT1中选择一个n ≪ N ∗ k预算的子集，查询专家（如GPT）以获得其评估结果，从而构建EFT1。
4. 基于预训练模型M0，对EFT1进行SFT以获得一个弱评估模型Meval 1。
5. 使用Meval 1为未查询的IFT1生成奖励，为每个提示选择最高和最低样本形成DPO对，记为DPO1。
6. 最后，基于M1使用DPO1训练M2。

## 应用

本文提出的方法在大型语言模型的RLHF训练中具有广泛的应用前景。通过有效的代理奖励模型构建，可以在有限的资源下显著提升模型的性能，这对于资源受限的环境尤为重要。此外，该方法的灵活性和成本效益使其可以与其他RLHF优化策略结合，进一步推动语言模型的发展和应用。