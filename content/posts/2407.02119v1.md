---
author: 'TechScribe'
title: '"创新性RLHF方法：通过在线策略和主动学习降低大型语言模型训练成本"'
date: '2024-07-02'
Lastmod: '2024-07-05'
description: 'Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Cost-Effective Proxy Reward Model Construction with On-Policy and Active Learning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02119v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02119v1)

## 摘要

本文探讨了在大型语言模型（LLM）中使用人类反馈的强化学习（RLHF）时，如何通过成本效益高的代理奖励模型来有效构建偏好数据集。传统的RLHF方法受限于人类偏好数据的大小，而本文提出的方法通过在线策略查询和主动学习，利用有限的标记种子数据和专家查询预算，训练出一个评估模型，该模型能够有效地标记比其训练数据多九倍的偏好对，用于进一步的RLHF训练。例如，使用直接偏好优化（DPO）方法，该模型在AlpacaEval2、MMLU5shot和MMLU-0shot等指标上平均提升了1%以上，而查询成本仅为1.7K。<!--more-->

## 原理

本文提出的方法通过两个关键创新来构建成本效益高的代理奖励模型：（1）在线策略查询，以避免种子数据中的分布外（OOD）和不平衡问题；（2）主动学习，选择最具信息量的数据进行偏好查询。在线策略查询确保了生成数据与目标模型的训练数据分布一致，而主动学习则通过选择最具代表性的数据点来最大化查询效率。这两种方法的结合使得即使在小规模数据集上也能训练出有效的评估模型，从而大幅减少专家查询的需求。

## 流程

本文的工作流程包括以下步骤：（1）使用预训练模型M0和初始种子数据IFTseed进行SFT（监督微调）以获得M1；（2）对一组未标记的提示X，使用M1生成一组响应；（3）通过主动查询策略选择一小部分数据点，查询专家（如GPT）以获得其评估结果，从而构建EFT1；（4）基于预训练模型M0，对EFT1进行SFT以获得弱评估模型Meval 1；（5）使用Meval 1生成未查询的IFT1的奖励；（6）最终基于M1使用DPO1训练M2。这个流程的关键在于第三步，通过在线策略和主动学习策略选择最具信息量的数据点进行查询。

## 应用

本文提出的方法在大型语言模型的RLHF训练中具有广泛的应用前景。通过减少对专家查询的依赖，该方法可以显著降低训练成本，同时保持或提升模型性能。这种方法不仅适用于现有的LLM训练框架，还可以与其他优化策略结合，进一步提高效率和效果。随着LLM在各种应用中的普及，这种成本效益高的代理奖励模型构建方法将变得越来越重要。