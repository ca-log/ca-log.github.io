---
author: 'TechScribe'
title: '探索叙事摘要的忠实度：STORYSUMM数据集的挑战与前景'
date: '2024-07-09'
Lastmod: '2024-07-10'
description: 'STORYSUMM: Evaluating Faithfulness in Story Summarization'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![STORYSUMM: Evaluating Faithfulness in Story Summarization](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.06501v1.pdf_0.jpg)](https://arxiv.org/abs/2407.06501v1)

## 摘要

本文介绍了一种名为STORYSUMM的新数据集，用于评估故事摘要中的忠实度。该数据集包含由大型语言模型（LLM）生成的短篇故事摘要，以及局部忠实度标签和错误解释。文章指出，人类评估一直是检查摘要忠实度的黄金标准，但在叙事领域，多个注释者可能同意摘要忠实，而忽略了只有在指出后才明显的细节错误。因此，STORYSUMM数据集旨在通过评估方法来检测具有挑战性的不一致性，特别是对于叙事文本的摘要。文章还发现，目前的自动评估指标在该任务上的表现不佳，表明这是一个具有挑战性的基准，需要未来的工作来改进忠实度评估。<!--more-->

## 原理

STORYSUMM数据集的工作原理基于以下几个关键点：首先，它包含由LLM生成的短篇故事摘要，这些摘要带有局部忠实度标签和错误解释，使得评估方法可以检测和理解摘要中的不一致性。其次，数据集的设计考虑到了注释者的可读性和故事的原创性，确保了故事的多样性和挑战性。最后，通过比较不同的人类注释协议和手动检查结果，文章展示了不同协议能够捕捉到独特的但合法的不一致性，并提出了使用多种注释者和协议来建立忠实度基准的重要性。

## 流程

STORYSUMM数据集的工作流程包括以下步骤：首先，收集来自Reddit的短篇故事，并使用不同的LLM模型生成摘要。然后，通过不同的注释协议（如Amazon Mechanical Turk和Upwork）对摘要进行忠实度评估，标记出忠实和非忠实的摘要，并提供错误解释。接着，通过手动检查和合并不同注释协议的标签集，形成最终的忠实度标签集。最后，使用这些标签集来评估和比较不同的自动评估指标，以确定它们在检测摘要忠实度方面的表现。

## 应用

STORYSUMM数据集的应用前景广泛，主要体现在以下几个方面：首先，它为研究者提供了一个新的基准，用于开发和测试新的忠实度评估方法，特别是在叙事文本的摘要领域。其次，通过分析和比较不同注释协议和自动评估指标的表现，可以进一步优化和改进现有的评估工具。最后，该数据集还可以用于教育和培训，帮助学生和专业人士更好地理解和应用忠实度评估的概念和方法。