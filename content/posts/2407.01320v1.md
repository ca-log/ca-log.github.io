---
author: 'TechScribe'
title: 'CAPABOOST：无需额外参数提升模型容量的简单策略'
date: '2024-07-01'
Lastmod: '2024-07-05'
description: 'Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01320v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01320v1)

## 摘要

本文介绍了一种名为CAPABOOST的新策略，用于在参数有效微调（PEFT）方法中无需增加额外参数即可提升模型容量。CAPABOOST通过在目标层的并行权重模块中利用低秩更新和静态随机掩码，构建了多样化的权重矩阵，从而有效提高了增量权重的秩，而无需增加参数。该方法可以无缝集成到现有的多种PEFT方法中，并通过在自然语言理解、问答和图像分类等多样化下游任务上的实验验证了其有效性。CAPABOOST不仅显著提升了基线性能，而且没有产生额外的计算或存储成本。<!--more-->

## 原理

CAPABOOST的核心原理是通过在共享权重矩阵上应用静态随机掩码，构建一组多样化的权重矩阵。这些掩码在训练过程中保持不变，且不参与训练，从而在不增加参数的情况下增加了模型的容量。具体来说，CAPABOOST通过以下步骤实现：
1. 在目标层的并行权重模块中应用静态随机掩码。
2. 这些掩码构建了一组多样化的权重矩阵，增加了增量权重的秩。
3. 通过这种方式，CAPABOOST能够在不增加参数的情况下提升模型的性能。

## 流程

CAPABOOST的工作流程如下：
1. 在目标层的并行权重模块中应用静态随机掩码。
2. 这些掩码构建了一组多样化的权重矩阵。
3. 通过这些多样化的权重矩阵，CAPABOOST增加了增量权重的秩。
4. 将CAPABOOST集成到现有的PEFT方法中，如Prefix-Tuning、Adapters和LoRA等。
5. 在多个下游任务上进行实验验证，如自然语言理解、问答和图像分类等。
6. 结果显示，CAPABOOST显著提升了基线性能，而没有产生额外的计算或存储成本。

## 应用

CAPABOOST的应用前景广泛，特别适用于需要参数有效微调的场景，如大规模预训练模型的微调。由于其能够在不增加参数的情况下提升模型性能，CAPABOOST可以应用于资源受限的环境，如移动设备或边缘计算。此外，CAPABOOST还可以集成到现有的多种PEFT方法中，为各种自然语言处理和计算机视觉任务提供性能提升。