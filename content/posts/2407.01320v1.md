---
author: 'TechScribe'
title: 'CAPABOOST：无需增加参数的高效模型微调策略'
date: '2024-07-01'
Lastmod: '2024-07-05'
description: 'Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01320v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01320v1)

## 摘要

本文介绍了一种名为CAPABOOST的新策略，旨在通过利用目标层中并行权重模块的低秩更新来增强模型容量，而无需增加参数。CAPABOOST通过应用静态随机掩码到共享权重矩阵，构建了一系列多样化的权重矩阵，有效提高了增量权重的秩，而无需增加参数。该方法可以无缝集成到现有的各种参数高效微调方法中，并通过在自然语言理解、问答和图像分类等多种下游任务上的实验验证了其有效性。结果显示，CAPABOOST在不影响计算或存储成本的情况下，显著优于基线方法。<!--more-->

## 原理

CAPABOOST的核心思想是通过在目标层中引入并行权重模块，利用低秩更新来增强模型的容量。具体来说，通过在共享权重矩阵上应用静态随机掩码，CAPABOOST构建了一系列多样化的权重矩阵，这些矩阵在训练过程中保持不变。这种方法有效地提高了增量权重的秩，从而在不增加参数的情况下增强了模型的表达能力。CAPABOOST的先进性在于其简单而有效的设计，能够无缝集成到现有的参数高效微调方法中，如Prefix-Tuning、Adapters和LoRA等，从而在不增加额外计算负担的情况下提升模型性能。

## 流程

CAPABOOST的工作流程包括以下几个关键步骤：
1. **初始化**：在目标层中引入并行权重模块，并初始化共享权重矩阵。
2. **应用掩码**：对共享权重矩阵应用静态随机掩码，生成多样化的权重矩阵。
3. **训练**：在训练过程中，使用这些多样化的权重矩阵进行前向和反向传播，更新模型参数。
4. **评估**：在多种下游任务上评估模型的性能，验证CAPABOOST的有效性。

例如，在自然语言理解任务中，CAPABOOST通过在RoBERTa模型中应用并行权重模块和随机掩码，显著提高了模型在GLUE基准测试中的性能，同时保持了参数数量不变。

## 应用

CAPABOOST的应用前景广泛，特别适用于需要参数高效微调的场景，如大规模预训练模型的下游任务适应。由于其能够在不增加参数和计算成本的情况下提升模型性能，CAPABOOST有望在自然语言处理、计算机视觉和问答系统等多个领域得到广泛应用。此外，CAPABOOST的灵活性和集成性使其能够与现有的多种微调方法结合，进一步扩展其应用范围。