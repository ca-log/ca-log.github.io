---
author: 'TechScribe'
title: '"专家专业化微调：稀疏架构大型语言模型的参数高效定制"'
date: '2024-07-02'
Lastmod: '2024-07-10'
description: 'Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01906v2.pdf_0.jpg)](https://arxiv.org/abs/2407.01906v2)

## 摘要

本文探讨了在资源受限的情况下，如何通过参数高效的微调（PEFT）方法定制大型语言模型（LLMs），特别是在稀疏架构的LLMs中。尽管已有多种针对密集架构LLMs的PEFT方法，但稀疏架构LLMs的PEFT研究仍不足。本文主要研究了具有混合专家（MoE）架构的LLMs的PEFT方法，并提出了专家专业化微调（ESFT），该方法仅微调与下游任务最相关的专家，同时冻结其他专家和模块的参数。实验结果表明，ESFT不仅提高了微调效率，而且在性能上与全参数微调相当甚至更优。此外，本文还分析了MoE架构对专家专业化微调的影响，发现具有更细粒度专家的MoE模型在选择与下游任务最相关的专家组合时更具优势，从而提高了训练效率和有效性。<!--more-->

## 原理

ESFT方法的核心在于选择并微调与特定任务最相关的专家，同时保持其他专家和模块的参数不变。这种方法的工作原理基于以下观察：在MoE架构中，不同任务由不同的激活专家处理，且任务特定的专家激活分布高度集中。ESFT通过计算专家与任务的相关性得分（如平均门控得分和令牌选择比率），选择得分最高的专家进行微调。这种选择机制确保了模型在保持专家专业化的同时，能够高效地适应新任务。此外，ESFT还利用了MoE模型中细粒度专家的优势，这些专家能够更精确地处理特定任务，从而在保持计算效率的同时提高模型性能。

## 流程

ESFT的工作流程包括以下几个步骤：
1. **数据采样**：从训练数据中随机采样一个子集，用于专家选择。
2. **专家相关性得分计算**：计算每个专家与任务的相关性得分，使用的方法包括平均门控得分（ESFT-Gate）和令牌选择比率（ESFT-Token）。
3. **专家选择和微调**：根据相关性得分选择每个MoE层中的一部分专家进行微调，而其他专家和模块的参数保持不变。
4. **训练和推理**：在训练和推理过程中，令牌可以分配给任何专家，但只有选定的专家参数会被更新。

通过这种方式，ESFT能够在保持模型整体性能的同时，高效地适应特定任务。

## 应用

ESFT方法在多个领域具有广泛的应用前景，特别是在需要定制大型语言模型以适应特定任务的场景中。例如，在法律、医疗、金融等行业中，ESFT可以帮助模型更精确地理解和处理专业领域的文本。此外，ESFT的高效性和性能优势也使其成为资源受限环境下部署大型语言模型的理想选择。随着MoE架构和稀疏模型的进一步发展，ESFT有望在更多应用场景中发挥重要作用。