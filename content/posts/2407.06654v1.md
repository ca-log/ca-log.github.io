---
author: 'TechScribe'
title: 'SoftDedup：革新大型语言模型预训练的高效数据重加权方法'
date: '2024-07-09'
Lastmod: '2024-07-10'
description: 'SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.06654v1.pdf_0.jpg)](https://arxiv.org/abs/2407.06654v1)

## 摘要

本文介绍了一种名为SoftDedup的高效数据重加权方法，旨在加速大型语言模型（LLMs）的预训练过程。该方法通过引入“数据共同性”这一概念，量化数据样本的重复程度，从而在保持数据集完整性的同时，减少高重复度数据的采样权重。实验结果显示，该方法显著提高了训练效率，减少了至少26%的训练步骤，同时提升了下游任务的准确性。此外，该方法在已经严格去重的数据集上也能持续提升性能，表明其具有补充现有去重方法并成为LLMs预训练标准流程的潜力。<!--more-->

## 原理

SoftDedup方法的核心在于通过n-gram模型计算每个数据样本的“数据共同性”，即样本在数据集中的出现概率。具体来说，该方法首先使用n-gram模型（n=4）对原始数据集进行训练，通过最大似然估计计算每个n-gram的概率，并采用Kneser-Ney平滑技术缓解数据稀疏问题。然后，利用训练好的n-gram模型计算每个样本的共同性，即样本中所有n-gram概率的几何平均值。最后，根据样本的共同性调整其采样权重，共同性高的样本权重降低，共同性低的样本权重增加。这种方法避免了传统硬去重方法中删除数据的风险，同时更细致地处理了数据重复问题。

## 流程

SoftDedup方法的工作流程包括以下几个步骤：
1. **数据集准备**：使用大型原始数据集进行预处理。
2. **n-gram模型训练**：训练一个n-gram模型（n=4）来计算每个样本的共同性。
3. **数据重加权**：根据样本的共同性对其进行重加权，共同性高的样本降低采样权重，共同性低的样本增加采样权重。
4. **语言模型预训练**：使用重加权后的数据集进行语言模型的预训练。

例如，在一个包含大量重复样本的数据集中，SoftDedup方法会识别出高共同性的样本（如重复的句子），并降低这些样本的采样权重，从而在训练过程中减少其对模型的影响，提高训练效率和模型性能。

## 应用

SoftDedup方法的应用前景广泛，特别适用于需要高效处理大量重复数据的大型语言模型预训练场景。该方法不仅能够加速训练过程，还能提升模型在下游任务中的表现，尤其是在数据集已经经过初步去重处理的情况下，仍能进一步优化模型性能。因此，SoftDedup有望成为未来大型语言模型预训练的标准流程之一，推动语言模型技术的发展。