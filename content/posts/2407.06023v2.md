---
author: 'TechScribe'
title: '"提炼智慧：将复杂推理技术融入大型语言模型的新方法"'
date: '2024-07-08'
Lastmod: '2024-07-10'
description: 'Distilling System 2 into System 1'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Distilling System 2 into System 1](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.06023v2.pdf_0.jpg)](https://arxiv.org/abs/2407.06023v2)

## 摘要

本文探讨了将大型语言模型（LLMs）中的“系统2”推理技术提炼回“系统1”生成过程的方法。通过自监督学习，研究者展示了如何在不生成中间推理标记序列的情况下，将“系统2”技术的高质量输出提炼回LLMs中。这种方法不仅提高了原始“系统1”性能，而且在推理成本上低于“系统2”。文章指出，这种提炼技术将成为未来持续学习AI系统的重要特征，使它们能够专注于尚未能很好处理的推理任务。<!--more-->

## 原理

本文提出的“系统2提炼”方法通过自监督学习实现，具体步骤包括：首先，使用“系统2”模型在未标记数据上生成响应，并测量预测质量；其次，对于一致性足够的示例，假设这些结果应被提炼，并将其添加到提炼池中；最后，对“系统1”模型进行微调，使其匹配“系统2”模型在收集的示例池上的预测，但不生成中间步骤。这种方法通过将“系统2”的推理过程提炼到“系统1”中，实现了在不增加推理成本的情况下提高模型性能。

## 流程

1. **生成响应**：使用“系统2”模型在未标记数据上生成响应。
2. **质量评估**：通过自一致性等方法评估生成的响应质量。
3. **提炼池构建**：将高质量的响应添加到提炼池中。
4. **模型微调**：对“系统1”模型进行微调，使其匹配“系统2”模型的预测。
5. **性能验证**：在多个任务上验证提炼后的“系统1”模型的性能。

## 应用

本文提出的“系统2提炼”技术在处理偏见信息、澄清任务指令和改进LLM-as-a-Judge评估等方面显示出潜力。未来，这种提炼方法有望在持续学习的AI系统中得到广泛应用，特别是在需要复杂推理的任务中，能够提高效率和性能。