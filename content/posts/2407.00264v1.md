---
author: 'TechScribe'
title: '探索未知：强化学习中的兴趣驱动行为塑造与外部模型适应'
date: '2024-06-28'
Lastmod: '2024-07-05'
description: 'External Model Motivated Agents: Reinforcement Learning for Enhanced Environment Sampling'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![External Model Motivated Agents: Reinforcement Learning for Enhanced Environment Sampling](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00264v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00264v1)

## 摘要

本文介绍了一种名为“External Model Motivated Agents”的强化学习框架，旨在提高代理在变化环境中的外部模型适应效率。该框架通过引入“兴趣场”和“兴趣影响”两个模块，使代理能够在不改变奖励机制的情况下，更有效地收集对环境模型有价值的数据。研究结果表明，该方法在环境变化时，能够显著提升外部模型的适应效率和性能。<!--more-->

## 原理

该框架的核心在于通过两个模块——兴趣场（Interest Field）和兴趣影响（POI Influence）——来驱动代理的行为。兴趣场是一个定义在观察空间上的标量场，用于评估每个观察对策略的“兴趣”程度。兴趣影响模块则利用兴趣场来塑造代理的行为，使其在执行任务的同时，更多地收集“兴趣”数据。这种设计使得代理能够在不改变任务奖励的情况下，更有效地适应环境变化。

## 流程

在每个 episode 开始时，代理通过 Monte Carlo dropout 不确定性兴趣场算法计算兴趣场。然后，使用兴趣值偏置的离散技能采样算法（POI Influence using an Interest Biased Discrete Skill Prior）来选择技能，这些技能将引导代理在 episode 中执行任务。在收集了一定数量的数据后，外部模型和策略会根据新数据进行更新。例如，在一个地下采矿机器人的案例中，代理会被引导到那些对安全模型有高兴趣的区域，以提高模型的准确性和效率。

## 应用

该框架不仅适用于地下采矿机器人的安全模型，还可以扩展到各种外部模型，如世界模型、策略模型和安全预测模型等。这些模型在强化学习中广泛应用，特别是在环境变化频繁的场景中。研究结果表明，兴趣驱动的行为塑造能够显著提高外部模型的适应性和效率，为未来在复杂环境中的代理行为研究开辟了新的途径。