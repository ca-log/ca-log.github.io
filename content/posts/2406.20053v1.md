---
author: 'TechScribe'
title: '"隐蔽恶意微调：保护LLM适应性的挑战"'
date: '2024-06-28'
Lastmod: '2024-07-05'
description: 'Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.20053v1.pdf_0.jpg)](https://arxiv.org/abs/2406.20053v1)

## 摘要

本文探讨了在保护大型语言模型（LLM）适应用户需求时面临的挑战，特别是通过黑箱微调接口可能让恶意行为者破坏模型安全性的问题。文章介绍了“隐蔽恶意微调”方法，该方法通过微调过程破坏模型安全性，同时逃避检测。该方法构建了一个恶意数据集，其中每个数据点看似无害，但微调后模型会对编码的有害请求做出编码的有害响应。应用于GPT-4模型时，该方法生成的微调模型在99%的情况下会按照有害指令行事，并能避开数据集检查、安全评估和输入/输出分类器等防御机制。文章质疑黑箱微调接口能否抵御复杂对手的攻击。<!--more-->

## 原理

隐蔽恶意微调方法的工作原理分为两个阶段：第一阶段是学习编码，模型学习一种之前未知的编码格式；第二阶段是恶意微调，模型被训练以对编码的有害输入做出编码的有害输出。这种方法之所以能逃避检测，是因为所有明文数据都是无害的，而所有有害数据都是编码的。这种方法灵活多变，可以使用任何合适的编码，并且可以针对任何目标进行恶意微调。

## 流程

隐蔽恶意微调的工作流程包括两个主要步骤：首先，模型通过无害数据学习一种新的编码格式；其次，模型在包含编码有害输入和输出的数据集上进行微调。在测试阶段，模型接收编码的有害请求并生成有害的编码响应，这些响应随后可以被解码。文章通过示例详细说明了这一过程，展示了模型如何在保持正常英语输入行为的同时，对编码的有害指令做出响应。

## 应用

隐蔽恶意微调方法揭示了LLM微调接口防御的挑战和现有防御方法的局限性。这强调了改进微调接口防御和部署前测试的必要性，特别是在模型能力不断扩展的情况下。文章还探讨了潜在的防御机制，包括模型自我评估和上下文学习等，强调了在安全微调领域中攻击者和防御者之间持续的“猫鼠游戏”。