---
author: 'TechScribe'
title: 'SBoRA：区域权重更新引领大型语言模型高效微调新纪元'
date: '2024-07-07'
Lastmod: '2024-07-11'
description: 'SBoRA: Low-Rank Adaptation with Regional Weight Updates'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![SBoRA: Low-Rank Adaptation with Regional Weight Updates](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.05413v2.pdf_0.jpg)](https://arxiv.org/abs/2407.05413v2)

## 摘要

本文介绍了一种名为Standard Basis LoRA (SBoRA)的新型参数高效微调方法，专为大型语言模型（LLMs）设计。SBoRA在Low-Rank Adaptation (LoRA)和Orthogonal Adaptation的基础上进一步减少了计算和内存需求，同时提高了学习性能。通过利用正交标准基向量初始化低秩矩阵之一，SBoRA实现了区域权重更新和内存高效微调。该方法的两个变体SBoRA-FA和SBoRA-FB，通过稀疏更新矩阵∆W，使得大部分微调模型的权重（W0 + ∆W）保持不变，从而提高了知识保留和适应新任务的效率。实验结果显示，SBoRA在多种微调任务中优于LoRA，特别是在常识推理和算术推理任务中表现突出。此外，SBoRA在量化LLaMA模型上的有效性评估，进一步强调了其适应新任务的潜力。<!--more-->

## 原理

SBoRA的工作原理基于选择性地更新预训练权重矩阵W0的特定行或列，从而减少计算和内存需求。通过使用正交标准基向量（one-hot向量）来构建投影矩阵，SBoRA实现了区域权重更新。具体来说，SBoRA-FA和SBoRA-FB分别固定一个矩阵（Asb或Bsb），只更新另一个矩阵（B或A），从而生成一个稀疏的更新矩阵∆W，其中大部分行或列是零。这种局部学习过程类似于人脑的模块化组织，其中特定的认知功能位于不同的脑区。SBoRA通过这种方式，不仅减少了内存使用，还通过消除一个矩阵乘法操作，降低了计算复杂度。

## 流程

SBoRA的工作流程包括以下步骤：
1. 使用正交标准基向量初始化投影矩阵之一（Asb或Bsb）。
2. 在微调过程中，固定初始化的矩阵，只更新另一个矩阵（B或A）。
3. 通过稀疏的更新矩阵∆W，实现对预训练权重矩阵W0的局部更新。
4. 在推理阶段，结合预训练权重和更新矩阵，生成最终的模型权重。
例如，在SBoRA-FA中，固定Asb并更新B，而在SBoRA-FB中，固定Bsb并更新A。这种流程确保了模型在保持大部分预训练权重不变的同时，能够有效地适应新任务。

## 应用

SBoRA的关键内容具有广泛的应用前景，特别是在需要高效微调大型语言模型的场景中。由于其参数高效和内存节省的特性，SBoRA可以应用于各种下游任务，如常识推理、算术推理和多任务学习。此外，SBoRA在量化模型上的有效性表明，它也适用于资源受限的环境，如移动设备或边缘计算。随着进一步的研究和开发，SBoRA有望成为微调大型语言模型的标准方法之一，特别是在需要快速适应新任务和数据集的动态环境中。