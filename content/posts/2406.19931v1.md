---
author: 'TechScribe'
title: '"FedDecomp: 创新个性化联邦学习方法，高效解耦通用与个性化知识"'
date: '2024-06-28'
Lastmod: '2024-07-05'
description: 'Decoupling General and Personalized Knowledge in Federated Learning via Additive and Low-Rank Decomposition'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Decoupling General and Personalized Knowledge in Federated Learning via Additive and Low-Rank Decomposition](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.19931v1.pdf_0.jpg)](https://arxiv.org/abs/2406.19931v1)

## 摘要

本文提出了一种名为FedDecomp的新型个性化联邦学习（PFL）方法，旨在解决数据异质性问题。FedDecomp通过参数加性分解，将每个模型参数分解为共享参数和个性化参数，从而更彻底地解耦共享知识和个性化知识。此外，FedDecomp通过交替训练策略，优先训练低秩矩阵以吸收非独立同分布（non-IID）数据的影响，进一步提高了性能。实验结果表明，FedDecomp在多个数据集和不同程度的非IID数据分布下，性能优于现有方法。<!--more-->

## 原理

FedDecomp的核心创新在于参数的加性分解，每个模型参数被分解为共享参数和个性化参数。共享参数用于提取跨客户端的通用知识，而个性化参数则用于学习特定客户端的知识。此外，FedDecomp引入了低秩约束，使得个性化参数矩阵保持低秩，这有助于减少对本地知识的过拟合，同时保留从其他客户端获得的大量通用知识。通过这种分解和约束，FedDecomp能够更有效地解耦和提取通用与个性化知识，从而提高模型性能。

## 流程

FedDecomp的工作流程包括以下步骤：
1. 每个客户端将其个性化模型的每一层分解为全秩矩阵和低秩矩阵。
2. 客户端首先冻结全秩矩阵，更新低秩矩阵，然后冻结低秩矩阵，更新全秩矩阵。
3. 客户端将全秩矩阵上传到服务器，服务器聚合这些矩阵生成全局模型，并将全局模型发送回所有客户端。
4. 客户端使用全局模型初始化其全秩矩阵，继续下一轮训练。
具体示例中，客户端在每一轮通信中交替训练低秩和全秩矩阵，以优化模型性能。

## 应用

FedDecomp在处理非IID数据分布的个性化联邦学习场景中展现出优越性能，适用于多样的数据集和复杂的应用环境。其解耦通用与个性化知识的能力，以及通过低秩约束和交替训练策略的优化，使得FedDecomp在隐私保护和模型性能方面具有广泛的应用前景，特别是在需要高度个性化和数据隐私保护的领域，如医疗、金融和个性化推荐系统等。