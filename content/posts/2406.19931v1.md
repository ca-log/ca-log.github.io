---
author: 'TechScribe'
title: 'FedDecomp：通过加性和低秩分解实现联邦学习中的通用与个性化知识解耦'
date: '2024-06-28'
Lastmod: '2024-07-05'
description: 'Decoupling General and Personalized Knowledge in Federated Learning via Additive and Low-Rank Decomposition'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Decoupling General and Personalized Knowledge in Federated Learning via Additive and Low-Rank Decomposition](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.19931v1.pdf_0.jpg)](https://arxiv.org/abs/2406.19931v1)

## 摘要

本文介绍了一种名为FedDecomp的新型个性化联邦学习（PFL）方法，旨在解决数据异质性问题。FedDecomp通过参数加性分解，将模型参数分解为共享参数和个性化参数，从而更有效地分离通用知识和特定客户端知识。此外，FedDecomp引入了低秩矩阵来表示个性化参数，并通过交替训练策略进一步提高性能。实验结果表明，FedDecomp在多个数据集和不同程度的数据异质性下，性能优于现有最先进的方法。<!--more-->

## 原理

FedDecomp的核心在于通过参数加性分解，将每个模型参数分解为共享参数和个性化参数。共享参数用于提取通用知识，而个性化参数则用于学习特定客户端的知识。通过这种方式，FedDecomp能够更彻底地分离通用知识和个性化知识。此外，FedDecomp利用低秩矩阵来表示个性化参数，这有助于减少模型容量，同时保持对本地知识的有效学习。交替训练策略则通过先训练个性化参数再训练共享参数的方式，减少了非独立同分布（non-IID）数据的影响，从而更好地提取通用知识。

## 流程

FedDecomp的工作流程包括以下步骤：
1. 每个客户端将其个性化模型的每一层分解为全秩矩阵和低秩矩阵。
2. 客户端首先冻结全秩矩阵，更新低秩矩阵，然后冻结低秩矩阵，更新全秩矩阵。
3. 客户端将全秩矩阵上传到服务器，服务器聚合这些矩阵生成全局模型，并将全局模型发送回所有客户端。
4. 客户端使用全局模型初始化其全秩矩阵，继续进行下一轮的训练。
具体示例和详细训练过程可以在算法1中找到。

## 应用

FedDecomp在处理数据异质性问题方面表现出色，适用于各种需要个性化模型的场景，如多媒体人工智能系统、视觉识别和自然语言处理等。随着联邦学习在各个领域的广泛应用，FedDecomp有望成为个性化联邦学习的重要工具。