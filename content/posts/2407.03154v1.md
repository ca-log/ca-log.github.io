---
author: 'Jithendaraa Subramanian,Shivakanth Sujit,Niloy Irtisam,Umong Sain,Derek Nowrouzezahrai,Samira Ebrahimi Kahou,Riashat Islam'
title: '强化学习与蛋白质语言模型：革新蛋白质序列设计的新途径'
date: '2024-07-03'
Lastmod: '2024-07-05'
description: 'Reinforcement Learning for Sequence Design Leveraging Protein Language Models'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Reinforcement Learning for Sequence Design Leveraging Protein Language Models](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.03154v1.pdf_0.jpg)](https://arxiv.org/abs/2407.03154v1)

## 摘要

本文探讨了利用蛋白质语言模型（PLMs）和强化学习（RL）进行蛋白质序列设计的有效性。蛋白质序列设计在药物发现中至关重要，传统方法如进化策略或蒙特卡洛方法未能充分利用组合搜索空间的结构，难以泛化到未见序列。本文提出了一种新的方法，使用PLMs作为奖励函数来生成新的蛋白质序列，同时解决了PLMs计算成本高的问题，通过引入一个周期性微调的小型代理模型来优化得分，从而减少计算负担。实验结果表明，RL方法在生物学合理性和序列多样性方面表现出色，证明了其在生物序列设计中的潜力。此外，本文还提供了一个模块化的开源实现，支持替换奖励模型，以促进该领域的进一步研究。<!--more-->

## 原理

本文的核心在于利用预训练的蛋白质语言模型（PLMs）作为强化学习（RL）的奖励函数，以生成新的蛋白质序列。PLMs能够根据生物学合理性（如TM-score）对蛋白质进行评分。然而，直接查询PLMs的计算成本很高。为了解决这一问题，本文提出了一种代理-微调（Proxy-Finetuning, PF）方法，即在RL学习过程中，使用一个较小且周期性微调的代理模型来近似PLMs的评分。代理模型在训练过程中不断更新，以更好地模拟PLMs的输出，从而在保持较高生物学合理性的同时，显著降低计算成本。

## 流程

1. **初始化序列**：随机初始化蛋白质序列。
2. **策略学习**：使用RL算法（如DQN、PPO等）学习一个策略，该策略能够在每一步选择一个突变动作，即选择一个位置和一个氨基酸替换原有的氨基酸。
3. **奖励评估**：使用PLMs（如ESMFold）或其代理模型对突变后的序列进行评分，评估其生物学合理性。
4. **代理模型微调**：每隔一定步数，使用真实PLMs的评分对代理模型进行微调，以提高其预测准确性。
5. **序列生成**：通过策略生成新的蛋白质序列，并评估其生物学合理性和多样性。
6. **迭代优化**：重复上述步骤，直到生成满足特定标准的蛋白质序列。

## 应用

本文提出的方法在蛋白质工程和药物发现领域具有广泛的应用前景。通过高效生成具有特定功能和结构特征的蛋白质序列，可以加速新药的研发过程。此外，该方法的模块化实现支持替换不同的PLMs和RL算法，为未来的研究和应用提供了灵活性和扩展性。