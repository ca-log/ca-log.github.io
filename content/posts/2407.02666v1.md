---
author: 'TechScribe'
title: '"智能四足机器人的未来：利用视觉-语言模型实现自主适应与导航"'
date: '2024-07-02 21:00:30+00:00'
Lastmod: '2024-07-04 01:53:10.177038'
description: 'Commonsense Reasoning for Legged Robot Adaptation with Vision-Language Models'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Commonsense Reasoning for Legged Robot Adaptation with Vision-Language Models](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02666v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02666v1)

## 摘要

本文探讨了如何利用视觉-语言模型（VLMs）的常识推理能力来帮助四足机器人适应复杂、模糊的环境。四足机器人在执行搜索和救援等任务时，需要能够智能地应对各种障碍，如爬过碎片、穿过缝隙和导航出死胡同。然而，现有的学习方法在处理这些意外和非典型情况时往往需要大量的人工监督。为了解决这一问题，研究者提出了一个名为VLM-Predictive Control（VLM-PC）的系统，该系统结合了两个关键组件：基于先前机器人交互的上下文适应和未来技能的多步规划与重新规划。实验表明，通过推理交互历史和未来计划，VLMs能够使机器人自主感知、导航和行动在广泛的复杂场景中，这些场景原本需要特定环境的工程设计或人工指导。<!--more-->

## 原理

VLM-PC系统的工作原理基于两个关键洞察：首先，通过考虑机器人的交互历史，利用链式思维推理，可以显著提高VLMs在新型情况下的鲁棒性；其次，提示模型进行多步技能规划并在每个时间步重新规划，对于预见潜在失败至关重要。该系统可以看作是技能空间中视觉模型预测控制的历史条件高级模拟。通过输入机器人的视图图像和交互历史，VLM被提示生成多步技能计划。模型随后通过推理机器人的当前状态和先前计划的进展来选择遵循哪个计划，并在需要时重新规划。

## 流程

VLM-PC的工作流程如下：首先，机器人接收当前视图和交互历史作为输入。然后，VLMs被提示基于这些信息生成一个多步技能计划。在执行计划中的第一个技能后，VLM再次被查询，以根据新的视图和交互历史重新评估和调整计划。这一过程持续进行，直到任务完成。例如，在一个实验中，机器人被任务找到一个红色咀嚼玩具，它首先爬过沙发，然后当发现是死路时退出，转向绕过沙发，爬过一个大垫子，最终找到玩具。

## 应用

VLM-PC系统的应用前景广泛，特别是在需要机器人自主处理复杂、不可预测环境的情况下，如搜索和救援、灾难响应和远程探索等。通过利用VLMs的常识推理能力，机器人能够更有效地适应和导航各种现实世界场景，减少对人工监督和特定环境工程的依赖。