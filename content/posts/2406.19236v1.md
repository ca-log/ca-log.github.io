---
author: 'TechScribe'
title: '"HA-VLN: 融合动态人类活动的视觉语言导航新前沿"'
date: '2024-06-27'
Lastmod: '2024-07-05'
description: 'Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.19236v1.pdf_0.jpg)](https://arxiv.org/abs/2406.19236v1)

## 摘要

本文介绍了一种名为Human-Aware Vision-and-Language Navigation (HA-VLN)的新型导航任务，旨在通过模拟动态人类活动和放松传统假设来缩小模拟与现实之间的差距。文章提出了Human-Aware 3D (HA3D)模拟器和Human-Aware Room-to-Room (HA-R2R)数据集，以支持HA-VLN的研究。此外，文章还提出了两种多模态代理：Expert-Supervised Cross-Modal (VLN-CM)和Non-Expert-Supervised Decision Transformer (VLN-DT)，利用跨模态融合和多样化的训练策略在动态人类环境中进行有效导航。全面的评估结果强调了进一步研究以增强HA-VLN代理在现实世界中的鲁棒性和适应性的必要性。最终，这项工作为未来在具身AI和Sim2Real转移方面的研究提供了基准和见解，为在人类环境中更真实和适用的VLN系统铺平了道路。<!--more-->

## 原理

HA-VLN任务通过引入动态人类活动和放松关键假设来扩展传统的Vision-and-Language Navigation (VLN)。HA3D模拟器结合了Matterport3D数据集和自收集的人类活动与姿态模拟(HAPS)数据集，提供了动态人类活动的真实环境。HA-R2R数据集扩展了R2R数据集，包含了人类活动描述。为了解决HA-VLN的挑战，提出了VLN-CM和VLN-DT代理，它们利用跨模态融合模块动态加权语言和视觉信息，增强了对不同模态的理解和利用。VLN-CM通过模仿专家演示学习，而VLN-DT展示了仅从随机轨迹学习而不需要专家监督的潜力。这些代理通过设计丰富的奖励函数来激励有效导航，并在考虑人类活动的全新指标下进行了全面评估。

## 流程

HA-VLN的工作流程涉及一个具身代理在一个动态环境中根据自然语言指令I = ⟨w1, w2, ..., wL⟩从初始位置导航到目标位置。代理在每个时间步t评估其状态st = � pt, ϕt, λt, Θ60 t �，并执行一系列动作AT = ⟨a0, a1, ..., aT⟩，导致状态和观察ST = ⟨s0, s1, ..., sT⟩。每个动作at ∈ A = ⟨aforward, aleft, aright, aup, adown, astop⟩导致新的状态st+1。HA-VLN通过放松三个关键假设来解决Sim2Real的差距：采用有限的60◦前视动作空间，整合基于SMPL模型的动态环境，并从次优专家演示中学习导航动态环境。代理必须实时感知和响应这些活动，同时保持安全距离dsafe，反映现实世界的导航挑战。

## 应用

HA-VLN的研究不仅限于模拟环境，还扩展到真实世界的机器人应用。通过在Unitree GO1-EDU四足机器人上部署训练好的代理，展示了在办公室环境中导航的能力，包括在有人类活动的情况下。这些实验验证了从模拟到现实世界转移学习策略的有效性，强调了进一步改进现实世界鲁棒性和适应性的必要性。HA-VLN的应用前景广泛，包括但不限于家庭服务机器人、导览机器人和紧急响应机器人，这些应用需要在动态和人类密集的环境中进行精确和安全的导航。