---
author: 'TechScribe'
title: '无需重新训练，个性化你的大型语言模型'
date: '2024-07-04'
Lastmod: '2024-07-10'
description: 'Orchestrating LLMs with Different Personalizations'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Orchestrating LLMs with Different Personalizations](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.04181v1.pdf_0.jpg)](https://arxiv.org/abs/2407.04181v1)

## 摘要

本文介绍了一种新颖的方法，用于将大型语言模型（LLMs）与个人偏好对齐，这种方法被称为个性化人类反馈强化学习（RLPHF）。文章提出了一种无需重新训练LLM的方法，通过从多个特定偏好的专家LLM中合并输出，动态生成符合用户偏好的文本。实验表明，这种方法在效率和可扩展性上优于现有的偏好合并技术，为个性化LLM的微调提供了一种有效替代方案。<!--more-->

## 原理

本文提出的方法基于一个轻量级的偏好控制模型（PCM），该模型能够根据用户的偏好描述和当前上下文，动态地为每个令牌预测权重。通过在令牌级别上合并专家模型的输出，生成符合用户多维度偏好的文本。这种方法的关键先进性在于其黑箱操作，不依赖于专家模型的参数，仅需要访问每个专家模型的顶部输出逻辑（或概率），这使得该方法适用于通过API暴露的专有模型。

## 流程

在推理阶段，用户提供指令和偏好，PCM根据当前生成的部分响应和用户偏好输出权重向量。这些权重用于合并专家模型的下一个令牌概率分布，从而采样生成下一个令牌。这个过程持续进行，直到生成结束令牌（EOS）。训练阶段，使用在线强化学习算法（如REBEL）训练PCM，以最大化用户偏好中指定的维度的平均奖励。

## 应用

该方法的应用前景广泛，特别是在需要个性化内容生成的领域，如个人助手、内容推荐系统等。由于其无需重新训练模型的特性，该方法在处理新用户或变化的需求时具有显著的优势，能够快速适应并提供定制化的服务。