---
author: 'TechScribe'
title: '"突破性能极限：FPGA上的高效能神经网络加速器AMU"'
date: '2024-07-02'
Lastmod: '2024-07-05'
description: 'Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02362v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02362v1)

## 摘要

本文介绍了一种名为“Approximate Multiplication Unit (AMU)”的高吞吐量、可扩展且能效高的非元素级矩阵乘法单元，该单元专门设计用于FPGA上的量化神经网络（QNN）加速器。AMU通过优化基于查找表（LUT）的近似矩阵乘法算法MADDNESS，实现了显著的性能提升，与现有技术相比，其吞吐量提高了9倍，能效提高了112倍。本文还探讨了AMU在不同优化策略下的性能，并展示了其在复杂神经网络推理任务中的应用前景。<!--more-->

## 原理

AMU的核心创新在于通过消除元素级算术操作，采用编码、检索和求和的方式进行矩阵乘法。具体来说，AMU通过三个优化策略（I/O数据修剪、特征图重组和参数压缩）对MADDNESS算法进行改进，从而有效地将乘法计算开销与输入特征图的分辨率解耦。这种设计使得AMU能够在不增加延迟的情况下，灵活调整资源、精度和能效，显著提升FPGA上神经网络加速器的性能。

## 流程

AMU的工作流程包括三个主要组件：分配器（Allocator）、编码器（Encoder）和聚合器（Aggregator）。分配器负责将输入特征图打包并分配给多个并行编码器。编码器通过查找表（LUT）查找部分点积结果，并进行编码。聚合器则负责将所有编码结果进行求和，并通过连续阈值处理（Successive Thresholding）生成输出特征图。整个流程通过流水线操作实现高效的数据处理，确保了高吞吐量和低延迟。

## 应用

AMU的设计不仅适用于传统的全连接层，还能通过img2col函数转换应用于卷积层，显示出在图像分类等复杂神经网络任务中的广泛应用潜力。随着深度学习模型的不断发展，AMU的高效能和可扩展性使其成为未来神经网络加速器设计的重要方向。