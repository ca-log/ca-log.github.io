# "AMU：FPGA上的高效能神经网络加速器"

[![Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02362v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02362v1)

## 摘要

本文介绍了一种名为“Approximate Multiplication Unit (AMU)”的高吞吐量、可扩展且能效高的非元素级矩阵乘法单元，该单元基于FPGA平台，旨在作为神经网络（NN）的基本组件。AMU通过优化MADDNESS算法中的层间和层内冗余，设计了一种快速、高效的近似矩阵乘法模块。该模块通过专用内存管理和访问设计进一步优化了基于LUT的矩阵乘法，显著提升了FPGA上NN加速器的效率。实验结果显示，使用AMU的FPGA基数量化神经网络（QNN）加速器在吞吐量和能效上分别比现有技术高出9倍和112倍。

## 原理

AMU的核心在于消除矩阵乘法中的元素级算术操作，通过引入三种优化策略（I/O数据修剪、特征图重组和参数压缩）来解耦乘法计算开销与输入特征图的分辨率。这些策略使得AMU能够在不增加延迟的情况下，显著提高NN推理加速器的吞吐量和能效。AMU架构包括分配器、编码器和聚合器，每个组件都针对提出的优化技术进行了优化，以实现高效的块分配、原型查找、部分结果获取和乘法结果打包功能。

## 流程

AMU的工作流程涉及三个主要阶段：分配、编码和聚合。在分配阶段，AMU接收修剪后的输入特征图，并将其分配给多个并行编码器。编码阶段中，每个编码器负责编码特定代码本（codebook）的ID，用于在聚合器中查找权重子矩阵的点积结果。聚合阶段则等待所有ID准备好后，从LUT中获取所有选定的单元（标记为灰色单元），并进行求和和偏置加法，然后通过连续阈值处理（successive thresholding）操作得到输出特征图，并将其传递给下一个AMU。

## 应用

AMU的设计不仅限于FPGA平台，其优化策略和硬件设计空间探索为ASIC加速器提供了潜在的性能提升。AMU在处理复杂NN模型，尤其是包含卷积层的模型时，展现了其高吞吐量和能效的优势。随着AI应用场景的不断扩展，AMU有望在IoT设备、健康监测和其他需要高性能NN推理的应用中发挥重要作用。

