---
author: 'TechScribe'
title: '探索大型语言模型在科学综合评估中的应用与挑战'
date: '2024-07-03 10:21:27+00:00'
Lastmod: '2024-07-04 01:52:56.890040'
description: 'Large Language Models as Evaluators for Scientific Synthesis'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Large Language Models as Evaluators for Scientific Synthesis](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02977v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02977v1)

## 摘要

本文探讨了最先进的大型语言模型（LLMs），如GPT-4和Mistral，如何评估科学摘要的质量，特别是科学综合评估，并将它们的评估与人类注释者的评估进行比较。研究使用了100个研究问题及其由GPT-4从五个相关论文的摘要中生成的综合，与人类质量评级进行对比。初步结果显示，LLMs能够提供逻辑解释，与质量评级有一定匹配，但深入的统计分析显示LLM和人类评级之间的相关性较弱，这表明LLMs在科学综合评估中存在潜在的局限性。<!--more-->

## 原理

LLMs通过处理和分析大量的文本数据，学习语言的结构和模式，从而能够生成和评估文本内容。在科学综合评估任务中，LLMs被训练来识别和评估摘要中的关键信息，如全面性、可信度和实用性。这些模型通过比较生成的摘要与原始摘要的内容，评估其是否准确、全面地反映了原始研究的关键点。尽管LLMs能够提供逻辑上一致的评估和解释，但它们与人类评估者之间的相关性较弱，表明在理解和评估复杂科学内容方面仍存在局限。

## 流程

研究使用了GPT-4 Turbo和Mistral模型来评估CORE-GPT数据集中的100个研究问题的综合质量。每个问题都附有五个相关工作的标题和摘要，以及由GPT-4生成的答案。人类注释者对每个综合的三个维度（全面性、可信度和实用性）进行0到10的评分。LLMs被要求根据相同的三个方面评估综合，并提供每个评分的理由。例如，GPT-4 Turbo在评估一个关于低收入社区对社会网络和福祉影响的综合时，给出了7分的全面性评分，理由是答案提供了广泛的概述，但未详细阐述每个摘要的具体机制。Mistral则在另一个关于社交媒体对美国年轻人关系形成影响的评估中，给出了9分的全面性评分，理由是答案提供了具体例子和直接引用研究。

## 应用

LLMs在科学综合评估中的应用前景广泛，特别是在自动化和加速科学研究的评估过程中。这些模型可以帮助研究人员快速筛选和评估大量文献，提高研究效率。然而，LLMs的评估结果需要与人类专家的评估相结合，以确保评估的准确性和可靠性。未来，随着模型的进一步发展和优化，LLMs在科学综合评估中的应用将更加广泛和深入。