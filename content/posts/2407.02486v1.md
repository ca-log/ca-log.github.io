# Neurocache：革新长文档处理的高效向量检索方法

[![Neurocache: Efficient Vector Retrieval for Long-range Language Modeling](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02486v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02486v1)

## 摘要

本文介绍了一种名为Neurocache的新方法，旨在通过使用外部向量缓存来扩展大型语言模型（LLMs）的有效上下文大小。Neurocache利用高效的k-最近邻（kNN）算法从缓存中检索相关过去状态，并将其整合到注意力过程中。该方法通过存储压缩状态、每个令牌执行单一检索操作以及扩展检索窗口到相邻状态，提高了推理速度和下游任务的准确性。实验表明，无论是从头开始训练的模型还是如Llama27B和Mistral-7B这样的预训练模型，Neurocache都能有效提升其性能。此外，Neurocache在与传统文本检索方法的比较中，在单文档问答和少样本学习任务中显示出改进。

## 原理

Neurocache的核心在于通过k-最近邻（kNN）策略从外部向量缓存中高效检索压缩的过去状态。这种方法通过以下几个关键步骤实现其先进性：
1. **状态压缩**：文本段通过Transformer解码器堆栈处理，并在解码器的r层获得隐藏状态Hr，随后通过学习投影矩阵Wp将其投影为压缩形式C。这一步骤提高了后续kNN检索的效率。
2. **状态检索**：对于C中的每个压缩状态c，从缓存Ccache中识别出最相似的top-k状态Cret。这一选择基于C和缓存中各状态之间的L2距离。
3. **缓存更新**：缓存Ccache通过添加压缩状态C进行更新，同时保持固定的m个条目大小。这一更新策略确保只检索相关过去状态。
4. **缓存增强层**：从(r+1)层开始，缓存增强层Lj通过专门的注意力机制整合Cret中的状态。每个层使用独特的投影矩阵W j k, W j v, 和 W j q生成从Cret得到的键Kj ret和值V j ret，以及从隐藏状态Hj得到的查询Qj。

## 流程

Neurocache的工作流程包括以下几个关键步骤：
1. **文档分段**：将长文本序列分割成包含n个令牌的小段，每个段适合模型的注意力窗口大小。
2. **状态压缩**：通过Transformer解码器堆栈顺序处理文本段，并在r层获得隐藏状态Hr，随后通过学习投影矩阵Wp将其投影为压缩形式C。
3. **状态检索**：对于C中的每个压缩状态c，从缓存Ccache中识别出最相似的top-k状态Cret。
4. **缓存更新**：缓存Ccache通过添加压缩状态C进行更新，同时保持固定的m个条目大小。
5. **缓存增强层**：从(r+1)层开始，缓存增强层Lj通过专门的注意力机制整合Cret中的状态。

## 应用

Neurocache的应用前景广泛，特别是在需要处理长文档的任务中，如文档摘要和学术文献回顾。其高效的检索机制和扩展的上下文长度使其在单文档问答和少样本学习任务中表现出色。此外，Neurocache的适应性和性能提升预示着其在自然语言处理领域的革命性潜力。

