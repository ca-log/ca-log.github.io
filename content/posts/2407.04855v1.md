---
author: 'TechScribe'
title: '提升抽取式摘要连贯性：新数据集与LLMs微调方法'
date: '2024-07-05'
Lastmod: '2024-07-10'
description: 'Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.04855v1.pdf_0.jpg)](https://arxiv.org/abs/2407.04855v1)

## 摘要

本文介绍了一种旨在提高抽取式摘要连贯性的新方法，通过创建一个人工标注的数据集，结合自然语言用户反馈，对大型语言模型（LLMs）进行监督微调，以增强其生成的摘要的连贯性。初步实验表明，这种方法在生成连贯摘要方面取得了显著的性能提升（约10%的Rouge-L分数），并且通过用户反馈对指令调优模型进行基准测试，得出了一些有趣的发现。<!--more-->

## 原理

本文的核心在于通过人工标注的数据集和自然语言用户反馈，对LLMs进行监督微调，以提高抽取式摘要的连贯性。工作原理主要包括以下几个步骤：
1. 创建一个人工标注的数据集，包含五个公开数据集的连贯摘要和自然语言用户反馈。
2. 利用这些数据对LLMs进行监督微调，使其生成的摘要更加连贯。
3. 通过Rouge-L评估模型性能，同时结合人工评估来验证摘要的连贯性。
这种方法的先进性在于它首次将用户意图融入到连贯性评估中，通过用户反馈来指导模型的微调，从而生成更符合用户需求的连贯摘要。

## 流程

1. 数据收集：从五个不同类别的公开数据集中随机选择源文本进行标注。
2. 摘要生成：使用大型语言模型生成连贯摘要。
3. 标注过程：雇佣专家标注者对生成的摘要进行审查，并提供自然语言反馈以改进连贯性。
4. 微调与推理：利用标注数据对LLMs进行监督微调，生成更连贯的摘要。
例如，对于一个新闻文档，模型首先生成一个初始摘要，然后标注者提供反馈，指出哪些句子需要改进，最终生成一个连贯的摘要。

## 应用

这种方法的应用前景广泛，特别是在需要高度连贯性和忠实于原文的摘要生成任务中，如视频缩短、法律文件摘要等。通过提高摘要的连贯性，可以显著提升用户体验，增强摘要的实用性和可读性。未来，这种方法还可以扩展到多语言场景，进一步提高抽取式摘要的质量。