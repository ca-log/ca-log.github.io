# 探索RLHF模型中的世界模型与代理模型权衡：预测与行动的交织

[![Predicting vs. Acting: A Trade-off Between World Modeling & Agent Modeling](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02446v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02446v1)

## 摘要

本文探讨了在强化学习从人类反馈（RLHF）对齐的语言模型（LMs）中，世界模型与代理模型之间的权衡问题。RLHF对齐的LMs在基准测试和长文本生成方面表现出色，但在下一个令牌预测这一基础任务上却遇到困难。本文通过实证分析，提出了一种解释：为了进行连贯的长文本生成，RLHF模型通过隐式蓝图限制了随机性，这虽然有助于生成结构化的长文本，但也限制了模型生成不包含这些锚定跨度的文档的能力。文章还探讨了这种权衡在当前最有效的代理模型中的表现，以及为何即使在改进对齐技术的情况下，这种权衡仍然存在。

## 原理

RLHF模型通过集中概率在特定的锚定跨度上，这些跨度在多个生成过程中共同出现，形成了一个文本支架。这种集中概率的做法使得模型能够在生成长文本时保持连贯性，但同时也限制了模型对任意文本的预测能力。这种权衡是由于RLHF模型在训练过程中被引导向特定的目标和任务，从而牺牲了其作为世界模型的原始能力，即准确预测任意前缀后的文本分布。

## 流程

论文通过收集和对比基础模型（Base LMs）和RLHF模型的生成结果，展示了RLHF模型在生成相同提示时的相似性。例如，对于“Python和JavaScript编程语言的主要区别是什么？”这一提示，RLHF模型生成的文本在多个长跨度上高度一致，而基础模型则显示出较低的一致性。这种一致性是通过核采样（nucleus sampling）和序列对齐方法（如MAFFT）来量化的，显示出RLHF模型在生成过程中倾向于重复使用相同的n-gram，从而形成了一个隐式的生成蓝图。

## 应用

本文的研究揭示了RLHF模型在生成连贯长文本方面的优势，这使得它们在需要高度一致性和可预测性的应用场景中非常有用，如自动文档生成、对话系统等。然而，这种集中概率的策略也限制了模型在更广泛文本分布上的灵活性和创造性。未来的研究可能会探索如何在保持代理模型行动能力的同时，恢复或增强其世界模型的预测能力。

