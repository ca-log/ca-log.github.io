---
author: 'Yuan Sun,Navid Salami Pargoo,Taqiya Ehsan,Zhao Zhang Jorge Ortiz'
title: 'VCHAR：方差驱动的复杂人类活动识别框架及其生成表示的先进性探索'
date: '2024-07-03'
Lastmod: '2024-07-05'
description: 'VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.03291v1.pdf_0.jpg)](https://arxiv.org/abs/2407.03291v1)

## 摘要

本文介绍了一种名为VCHAR的新型框架，用于解决复杂人类活动识别（CHAR）中的关键挑战。传统的CHAR方法通常需要对原子活动进行细致的标记，这在实际应用中既耗时又容易出错。VCHAR框架通过利用生成方法和方差驱动的策略，能够在不需要精确时间或序列标记的情况下，提高复杂活动识别的准确性。此外，VCHAR通过视频解释的方式，使得没有机器学习专业知识的用户也能理解复杂活动的分类依据。实验结果显示，VCHAR在多个公开数据集上的表现优于传统方法，并通过用户研究证实了其解释的可理解性。<!--more-->

## 原理

VCHAR框架的核心在于其方差驱动的复杂人类活动识别方法。该框架通过Kullback-Leibler（KL）散度来近似原子活动输出的分布，从而在特定时间间隔内识别出关键的原子活动，而无需移除瞬态状态或其他无关数据。这种方法使得VCHAR能够在没有详细标记原子活动的情况下，提高复杂活动的检测率。此外，VCHAR引入了一种生成解码器框架，将传感器模型的输出转换为集成视觉域表示，包括复杂和原子活动的详细可视化，以及模型中的传感器相关信息。利用语言模型（LM）代理，该框架组织了多样化的数据源，并采用视觉-语言模型（VLM）生成全面的视觉输出。

## 流程

VCHAR框架的工作流程从传感器数据编码开始，通过多任务学习同时识别原子和复杂活动。编码后的数据被送入生成解码器，该解码器利用预训练的传感器基础模型和一次性调优策略，快速适应特定智能空间场景。解码器将传感器数据转换为视觉表示，并通过语言模型代理生成易于理解的解释。例如，在检测到复杂活动如“制作三明治”时，VCHAR不仅能识别相关的原子活动如“打开门”，还能提供传感器激活值的详细信息，帮助用户理解活动识别的依据。

## 应用

VCHAR框架的应用前景广泛，特别是在智能家居、健康监测和紧急响应系统等领域。由于其能够在没有精确标记的情况下提高活动识别的准确性，VCHAR特别适合于那些标记数据稀缺或不准确的实际应用场景。此外，通过提供直观的视觉解释，VCHAR有助于增强用户对智能系统决策的信任和理解，推动其在日常生活中的广泛采用。