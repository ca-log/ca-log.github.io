---
author: 'TechScribe'
title: 'PromptIntern：通过内部化提示知识优化大型语言模型推理效率'
date: '2024-07-02'
Lastmod: '2024-07-05'
description: 'PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02211v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02211v1)

## 摘要

本文介绍了一种名为PromptIntern的新方法，旨在通过内部化重复的提示知识到模型参数中，以减少大型语言模型（LLMs）在推理过程中的计算负担。在实际应用中，对于重复查询，相似的提示组件会导致显著的计算负担。现有的提示压缩和直接微调方法往往难以在成本效率和性能效果之间找到最佳平衡，尤其是在复杂的任务如NL2Code中。PromptIntern方法通过逐步微调，使LLMs能够模拟人类学习新任务的过程，其中详细的模板和示例在提示中逐渐被内部化并逐步淘汰，从而使模型适应任务。实验表明，该方法减少了超过90%的推理令牌，加速了4.2倍的推理速度，并节省了88.3%的成本。<!--more-->

## 原理

PromptIntern的核心原理是通过逐步微调，将提示中的知识（如任务约束、输出格式/风格）内部化到LLMs的参数中。这种方法受到人类学习过程的启发，即在入职时，人类实习生需要详细的指导、示例和文档来学习新任务，随着他们对材料的内部化和熟悉度的提高，他们掌握了必要的技能，不再需要额外的指导。类似地，当特定的提示信息在微调过程中反复暴露给LLM时，模型可以逐渐将这些知识内部化到其更新的参数中。因此，这种重复的信息可以逐步从提示输入中消除，因为它在LLM的推理中变得不必要。

## 流程

PromptIntern的工作流程包括几个关键步骤：首先，将输入提示分类为模板、示例和查询三个组件。然后，设定一个计划，在线性减少模板压缩率和少量示例数量的训练阶段中实施模板压缩和示例吸收。接着，引入一个全面的管道，使LLMs在微调过程中逐步将模板和示例组件内部化到模型参数中，并使用仅包含查询的提示进行高效推理。该方法在挑战性的NL2Code任务中进行了评估，实验结果表明，在相同的微调设置下，PromptIntern不仅超越了提示压缩方法，而且在准确性上与直接微调相当，同时加速了推理过程并减少了令牌使用量。

## 应用

PromptIntern方法的应用前景广泛，特别适用于那些对计算资源有限制且成本敏感的场景。由于其能够在保持高性能的同时显著减少推理成本，该方法可以优化LLMs在各种任务中的表现，包括自然语言生成、推理和代码生成等。此外，随着LLMs在更多领域的应用，PromptIntern有望成为提高模型效率和降低运营成本的关键技术。