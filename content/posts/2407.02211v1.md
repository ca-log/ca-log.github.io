---
author: 'TechScribe'
title: 'PromptIntern：通过内部化提示知识优化大型语言模型微调，实现高效推理'
date: '2024-07-02 12:21:14+00:00'
Lastmod: '2024-07-04 01:17:31.236269'
description: 'PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![PromptIntern: Saving Inference Costs by Internalizing Recurrent Prompt during Large Language Model Fine-tuning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02211v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02211v1)

## 摘要

本文介绍了一种名为PromptIntern的新方法，旨在通过内部化重复提示知识来优化大型语言模型（LLMs）的微调过程，从而显著降低推理成本。在实际应用中，针对重复查询的相似提示组件会导致显著的计算负担。现有的提示压缩和直接微调方法虽然旨在解决这些挑战，但往往难以在成本效率和性能效果之间找到最佳平衡，尤其是在复杂的任务如NL2Code中。PromptIntern方法通过逐步将提示知识内部化到模型参数中，模拟人类学习新任务的过程，从而在保持高性能的同时，减少推理令牌使用超过90%，加速推理4.2倍，并节省88.3%的成本。<!--more-->

## 原理

PromptIntern的核心原理是通过渐进式微调将提示知识内部化到模型参数中。该方法首先将输入提示分为模板、示例和查询三个组件。在训练阶段，通过线性减少模板压缩率和示例数量，逐步将这些组件的知识吸收到模型参数中。具体来说，模板压缩系统通过预定的压缩率τtmp逐步减少模板信息，而示例吸收则通过相关性评分函数选择最相关的示例，并逐步减少这些示例的数量。通过这种方式，模型在微调过程中逐渐习惯于任务，不再需要额外的指导，从而在推理时仅使用查询部分即可高效执行。

## 流程

PromptIntern的工作流程包括三个主要阶段：预处理、渐进式微调和推理。在预处理阶段，输入提示根据预定的模板压缩率和示例吸收率进行处理。在渐进式微调阶段，模型参数根据处理后的提示进行更新。最后，在推理阶段，模型使用仅包含查询的提示进行预测。例如，在NL2F任务中，初始提示包含完整的模板和10个示例，随着训练的进行，模板和示例数量逐渐减少，最终在推理时仅使用查询部分。

## 应用

PromptIntern方法在NL2Code等复杂任务中显示出显著的成本效率和性能效果，适用于需要在资源受限环境下优化LLM性能的场景。该方法不仅在保持高准确性的同时大幅减少令牌使用和推理时间，还为未来在更广泛的任务和领域中优化LLM提供了新的思路和方法。