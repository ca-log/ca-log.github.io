---
author: 'TechScribe'
title: '深度学习模型的成员推理攻击：代码投毒的新威胁'
date: '2024-07-02'
Lastmod: '2024-07-05'
description: 'A Method to Facilitate Membership Inference Attacks in Deep Learning Models'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![A Method to Facilitate Membership Inference Attacks in Deep Learning Models](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01919v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01919v1)

## 摘要

本文介绍了一种针对深度学习模型的新型成员推理攻击方法，通过代码投毒攻击来放大成员隐私泄露。在现代机器学习生态系统中，即使非专家用户也可以使用现成的代码库在其敏感数据上构建高性能的机器学习模型。本文考虑了一种恶意机器学习提供者，他们提供模型训练代码给数据持有者，但不访问训练过程，仅通过黑盒查询访问生成的模型。在这种设置下，本文展示了一种新形式的成员推理攻击，其效果严格优于现有技术。该攻击使攻击者能够可靠地识别所有训练样本（平均攻击TPR@0.1% FPR >99%），并且被破坏的模型仍然保持与其未被破坏的对应模型相当的性能（平均准确度下降<1%）。最后，本文还展示了被投毒的模型在常见的成员隐私审计下可以有效地伪装其放大的成员泄露，这只能通过攻击者已知的一组秘密样本揭示。总体而言，本文不仅指出了最坏情况下的成员隐私泄露，还揭示了现有隐私审计方法的一个常见陷阱，因此呼吁未来重新思考机器学习模型中成员隐私审计的当前实践。<!--more-->

## 原理

本文提出的成员推理攻击基于代码投毒，通过修改模型训练代码中的损失值计算函数和模型结构来实现。攻击者通过在训练代码中注入恶意代码，使得模型在训练过程中不仅学习训练样本的标签信息，还会记忆一组额外的秘密（合成）样本，这些样本的输出可以用来编码训练样本的成员信息。攻击的核心原理是将训练样本的成员信息秘密转移到另一组秘密样本的成员信息上，从而使训练样本的成员信息可以通过对应的秘密样本被推断出来。这些秘密样本被特别设计为模型可以记忆的样本，从而使攻击者能够进行准确的成员推理攻击。

## 流程

攻击的工作流程如下：
1. 攻击者修改训练代码中的损失值计算函数，使其在计算损失时不仅考虑训练样本，还考虑一组由恶意代码生成的秘密样本。
2. 在训练过程中，模型被优化以最小化包含训练样本和秘密样本的损失值。
3. 在推理阶段，攻击者使用目标样本生成对应的秘密样本，并通过查询模型对秘密样本的输出来推断目标样本的成员信息。
具体示例：在CIFAR10数据集上，攻击者通过投毒的模型在标准成员推理评估下表现出与未被破坏的模型相似的准确度和成员推理风险，同时允许黑盒攻击者秘密地识别所有训练样本。

## 应用

本文提出的成员推理攻击方法揭示了机器学习模型在隐私保护方面的严重漏洞，特别是在敏感数据处理领域，如医疗记录分析。该攻击方法的应用前景包括但不限于：
1. 提高对机器学习模型隐私保护措施的认识和需求。
2. 促进开发更强大的隐私保护技术和审计方法。
3. 在实际应用中，如医疗、金融等领域，加强对第三方代码库的安全审查和使用限制。