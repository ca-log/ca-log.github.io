---
author: 'TechScribe'
title: '深度学习模型中的成员隐私泄露：一种新型的代码投毒攻击方法'
date: '2024-07-02 03:33:42+00:00'
Lastmod: '2024-07-04 01:17:41.364750'
description: 'A Method to Facilitate Membership Inference Attacks in Deep Learning Models'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![A Method to Facilitate Membership Inference Attacks in Deep Learning Models](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01919v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01919v1)

## 摘要

本文探讨了一种针对深度学习模型的新型成员推理攻击（Membership Inference Attack, MIA），即通过代码投毒攻击（code poisoning attack）来放大模型的成员隐私泄露。在现代机器学习生态系统中，即使非专家用户也可以使用现成的代码库在其敏感数据上构建高性能的机器学习模型。本文考虑了一个恶意机器学习提供者，该提供者向数据持有者提供模型训练代码，但无法访问训练过程，仅能通过黑盒查询访问最终模型。在这种设置下，本文展示了一种新形式的成员推理攻击，其攻击力严格超过现有技术。该攻击使攻击者能够可靠地去识别所有训练样本（平均攻击真阳性率（TPR）>99%，假阳性率（FPR）0.1%），并且被破坏的模型仍然保持与其未被破坏的对应模型相当的性能（平均准确率下降<1%）。最后，本文还展示了被投毒的模型在常见的成员隐私审计下可以有效地伪装其放大的成员泄露，这种泄露只能通过攻击者已知的一组秘密样本才能揭示。<!--more-->

## 原理

本文提出的攻击方法基于代码投毒，通过修改模型训练代码中的损失值计算函数和模型结构，使得模型在训练过程中不仅学习到训练样本的特征，还会记住一组由攻击者设计的合成样本。这些合成样本的输出标签被设计来编码训练样本的成员信息。攻击者通过这些合成样本的输出，可以推断出训练样本的成员身份。具体来说，攻击者首先在训练代码中注入恶意代码，使得模型在训练时记住这些合成样本。然后，攻击者通过黑盒查询访问训练后的模型，利用模型对合成样本的输出来推断训练样本的成员身份。这种攻击方法的关键在于，它能够在不显著影响模型性能的情况下，放大模型的成员隐私泄露。

## 流程

1. 攻击者设计并注入恶意代码到模型训练代码中，该代码包含一组合成样本及其输出标签，这些标签编码了训练样本的成员信息。
2. 数据持有者使用被投毒的训练代码在其敏感数据上训练模型，但不知道代码已被篡改。
3. 训练完成后，模型不仅学习了训练样本的特征，还记住了攻击者设计的合成样本。
4. 攻击者通过黑盒查询访问训练后的模型，利用模型对合成样本的输出来推断训练样本的成员身份。
5. 由于模型在标准成员推理攻击评估中表现正常，数据持有者无法检测到模型的成员隐私泄露。

## 应用

本文提出的攻击方法揭示了现有隐私审计方法的一个常见陷阱，即它们无法检测到通过代码投毒放大的成员隐私泄露。因此，该研究对未来重新思考机器学习模型中的成员隐私审计实践具有重要意义。此外，该攻击方法的应用前景包括但不限于：
1. 提高对机器学习模型隐私保护的认识，推动更严格的隐私审计标准和方法的发展。
2. 促进开发新的防御机制，以检测和防止代码投毒攻击。
3. 增强机器学习模型的安全性，特别是在处理敏感数据（如临床记录）时。