---
author: 'TechScribe'
title: '"LEQ：基于模型的离线强化学习在长时程任务中的突破"'
date: '2024-06-30'
Lastmod: '2024-07-05'
description: 'Tackling Long-Horizon Tasks with Model-based Offline Reinforcement Learning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Tackling Long-Horizon Tasks with Model-based Offline Reinforcement Learning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00699v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00699v1)

## 摘要

本文介绍了一种基于模型的离线强化学习方法，称为Lower Expectile Q-learning（LEQ），旨在解决长时程任务中的挑战。LEQ通过使用期望回归（expectile regression）和λ回报（λ-returns）来减少模型预测中的高偏差，从而提高长时程任务的性能。实验结果表明，LEQ在D4RL AntMaze任务中显著优于先前的基于模型的离线强化学习方法，并能与无模型方法相媲美或超越其性能。此外，LEQ在NeoRL基准和D4RL MuJoCo Gym任务中也取得了与最先进的基于模型和无模型离线强化学习方法相当的性能。<!--more-->

## 原理

LEQ的核心在于使用期望回归和λ回报来实现对策略的保守评估。期望回归通过选择低于期望的Q值目标，有效地提供了对Q值目标的保守估计。λ回报的使用允许Q函数和策略从低偏差的多步回报中学习，这对于长时程任务中价值估计的偏差减少尤为重要。LEQ通过在离线数据和模型生成的数据上训练批评者（critic），并在模型生成的数据上更新行动者（actor），实现了对策略和Q函数的优化。

## 流程

LEQ的工作流程包括以下步骤：
1. 从离线数据集初始化世界模型、策略和Q函数。
2. 使用预训练的世界模型生成想象轨迹，并将这些轨迹添加到训练数据集中。
3. 在扩展的数据集上执行离线强化学习算法，重复步骤2和3直到收敛。
4. 更新批评者以最小化λ回报的损失，同时更新行动者以最大化λ回报的期望。

## 应用

LEQ的应用前景广泛，特别是在需要从有限、静态数据中学习的场景，如机器人导航、自动驾驶车辆等。LEQ的保守评估策略和高效的数据利用使其能够在长时程任务中表现出色，预示着在自动化和机器人技术领域的广泛应用潜力。