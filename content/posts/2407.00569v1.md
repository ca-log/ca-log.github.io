---
author: 'TechScribe'
title: '"揭秘大型视觉-语言模型的幻觉问题及其解决方案"'
date: '2024-06-30'
Lastmod: '2024-07-05'
description: 'Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00569v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00569v1)

## 摘要

本文探讨了大型视觉-语言模型（LVLMs）在多模态交互中产生的幻觉问题，特别是在先前生成的幻觉影响下，模型是否会被误导并产生错误响应的问题。研究提出了一个名为MMHalSnowball的框架，用于评估LVLMs在遇到生成的幻觉时的行为，并发现开源LVLMs的性能至少下降了31%。为了缓解这一问题，研究进一步提出了一种无需额外训练的方法——残余视觉解码（Residual Visual Decoding, RVD），通过调整模型的输出分布，使其直接访问视觉信息，实验显示这种方法可以减少超过24%的幻觉累积，同时保持模型的能力。<!--more-->

## 原理

MMHalSnowball框架通过精心设计的幻觉对话来评估LVLMs的行为。模型被要求在包含幻觉的对话中回答特定的视觉问题。关键在于，模型在处理这些幻觉时，其强大的语言能力使其倾向于过度自信于幻觉上下文，从而生成通常不会支持的虚假声明，这种现象被称为多模态幻觉累积。RVD方法通过残余连接视觉信息和当前用户指令，强调视觉信息，从而修正原始输出分布，实现对视觉信息的直接访问。

## 流程

研究首先通过MMHalSnowball框架模拟幻觉对话，然后模型在这些对话中回答问题。例如，模型被要求描述一个场景并回答相关问题，如“我能过马路吗？”在幻觉对话中，模型可能会错误地建议用户过马路，尽管视觉信息显示不应如此。通过RVD方法，模型在生成响应时更加依赖视觉信息，从而减少幻觉的影响。

## 应用

该研究不仅揭示了LVLMs在多模态交互中的幻觉问题，还提供了一种有效的缓解策略。这些发现和方法对于开发更可靠的视觉-语言交互系统具有重要意义，特别是在需要高度准确性的应用场景中，如辅助视觉障碍人士、自动驾驶系统等。