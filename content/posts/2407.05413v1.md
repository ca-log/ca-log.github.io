---
author: 'TechScribe'
title: 'SBoRA：区域权重更新引领大型语言模型的高效微调新纪元'
date: '2024-07-07'
Lastmod: '2024-07-10'
description: 'SBoRA: Low-Rank Adaptation with Regional Weight Updates'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![SBoRA: Low-Rank Adaptation with Regional Weight Updates](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.05413v1.pdf_0.jpg)](https://arxiv.org/abs/2407.05413v1)

## 摘要

本文介绍了一种名为SBoRA的新型参数高效微调方法，专为大型语言模型（LLMs）设计。SBoRA在低秩适应（LoRA）和正交适应的基础上进一步发展，旨在减少计算和内存需求，同时提高学习性能。通过利用正交标准基向量初始化低秩矩阵之一，SBoRA实现了区域权重更新和内存高效的微调。这种方法在多种微调任务中显示出优越性，特别是在常识推理和算术推理任务中。此外，SBoRA在量化LLaMA模型上的有效性评估，强调了其适应新任务的潜力。<!--more-->

## 原理

SBoRA的工作原理基于选择性地更新预训练权重矩阵W0的特定行或列，从而生成一个主要由零行或列组成的更新矩阵∆W。这种方法通过使用正交标准基向量（独热向量）来构建其投影矩阵，从而实现高效的区域权重更新。SBoRA的两个变体，SBoRA-FA和SBoRA-FB，分别只更新矩阵B和矩阵A，导致稀疏的更新矩阵∆W，其中大部分权重保持不变。这种局部学习过程类似于人脑的模块化组织，其中特定的认知功能位于不同的脑区。

## 流程

SBoRA的工作流程包括以下步骤：首先，使用正交标准基向量初始化投影矩阵之一（Asb或Bsb）。然后，在微调过程中，冻结W0和Asb（或Bsb），只更新矩阵B（或A）。这种流程通过消除一个矩阵乘法操作，减少了计算复杂性。例如，在SBoRA-FA中，投影下降变换可以通过选择输入向量样本的子集来高效实现。SBoRA-FB则通过预定义的标准基投影上升矩阵Bsb和更新矩阵A来实现类似的操作。

## 应用

SBoRA的应用前景广泛，特别是在需要高效适应新任务的场景中。由于其能够进行区域权重更新，SBoRA特别适合于多任务训练，其中每个任务可以独立地进行非重叠的权重微调，从而在保持模型容量利用共享信息的同时，减少任务间的干扰。此外，SBoRA在量化模型中的有效性表明，它可以在资源受限的环境中实现高效的模型微调。