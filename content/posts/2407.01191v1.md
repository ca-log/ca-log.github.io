---
author: 'TechScribe'
title: 'MARS：引领机器人精确感知铰接对象的新框架'
date: '2024-07-01'
Lastmod: '2024-07-05'
description: 'MARS: Multimodal Active Robotic Sensing for Articulated Characterization'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![MARS: Multimodal Active Robotic Sensing for Articulated Characterization](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01191v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01191v1)

## 摘要

本文介绍了一种名为MARS的新型框架，专门用于铰接对象的精确感知。在服务机器人领域，对铰接对象的精确感知至关重要，但现有研究主要集中在单一模态的点云数据上，忽略了纹理和光照细节，并假设理想观察条件。MARS通过多模态融合模块和基于强化学习的主动感知策略，优化观察视角，从而在铰接对象的关节参数估计中实现了先进的性能。实验证明，MARS在关节参数估计准确性上超越了现有最先进的方法，并通过主动感知进一步减少了误差，提高了处理次优视角的效率。此外，MARS的方法有效地推广到现实世界的铰接对象，增强了机器人交互的能力。<!--more-->

## 原理

MARS框架的核心在于其多模态特征融合感知（MFFP）和主动感知（AS）两个主要组件。MFFP组件利用ResNet18和PointNet++作为骨干网络，从输入的RGB图像和点云数据中高效提取特征。通过多层竞争模块（MLDM），MARS能够战略性地提取和加权不同尺度的图像特征。随后，通过一个基于transformer编码器的融合模块，将多尺度RGB特征和点云特征进行整合。AS组件则基于感知评分和设定阈值，触发视角改变，通过强化学习驱动的策略，动态调整相机位置，确保获取最具信息量的视角。

## 流程

MARS的工作流程从机器人选择一个对象部分开始，然后将相应的RGB图像和点云数据输入到感知网络中，以确定该部分的移动性和识别关节参数。如果该部分被认为是可移动的，并且感知评分超过阈值，机器人将根据识别的关节参数和操作命令规划操作序列。在检测到次优视角时，机器人会调整其视角进行进一步评估，确保与铰接对象的可靠和精确交互。

## 应用

MARS框架的应用前景广泛，特别是在需要精确感知和操作铰接对象的服务机器人领域。通过提高关节参数估计的准确性和处理次优视角的能力，MARS能够增强机器人在复杂环境中的交互和操作效率。此外，MARS的强化学习驱动的主动感知策略使其能够适应不断变化的环境和对象状态，从而在未来的智能机器人和自动化系统中具有巨大的应用潜力。