---
author: 'TechScribe'
title: '突破性能极限：CPU基深度FFMs在多数据中心规模上的高效部署'
date: '2024-07-14'
Lastmod: '2024-07-16'
description: 'A Bag of Tricks for Scaling CPU-based Deep FFMs to more than 300m Predictions per Second'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![A Bag of Tricks for Scaling CPU-based Deep FFMs to more than 300m Predictions per Second](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.10115v1.pdf_0.jpg)](https://arxiv.org/abs/2407.10115v1)

## 摘要

本文由Blaž Škrlj等人撰写，详细介绍了一种基于CPU的深度场感知分解机（Deep FFMs）的实现及其在多数据中心规模上的部署。该研究主要针对点击率预测问题，通过一系列优化技术，实现了每秒超过3亿次预测的高性能。论文中提出的解决方案包括模型搜索、在线模型训练、存储、传输和服务的优化，以及一种新的权重量化方法，显著减少了数据中心间权重传输的带宽需求。此外，该研究还将相关技术和引擎开源，以促进机器学习社区的发展。<!--more-->

## 原理

论文的核心在于对深度场感知分解机（Deep FFMs）的优化和扩展，使其能够在仅依赖CPU的环境中高效运行。Deep FFMs结合了传统的FFM和深度学习元素，如多层感知器（MLP），以捕捉复杂的特征交互。通过集成BLAS（Basic Linear Algebra Subprograms）等高性能计算库，论文实现了关键的性能提升。此外，论文还引入了权重量化技术，通过减少权重的位数来降低模型的大小和传输所需的带宽，同时保持模型的预测性能。

## 流程

论文描述的工作流程包括模型训练和模型服务的优化。在训练阶段，通过异步学习周期和Hogwild-based训练方法，加速了模型的预热和在线训练过程。在服务阶段，通过上下文缓存和SIMD指令集的优化，提高了推理速度。具体来说，模型训练时，数据预取和多线程更新策略显著减少了预热时间。在模型服务时，通过缓存频繁的上下文部分和利用SIMD指令集，实现了更快的推理响应。

## 应用

该论文提出的技术和优化方法在推荐系统、广告投放和在线内容推荐等领域具有广泛的应用前景。通过提高模型的预测速度和效率，这些技术能够支持大规模的实时系统，处理数亿级别的请求，同时降低运营成本。此外，开源的引擎和相关技术也为其他研究者和开发者提供了宝贵的资源，有望推动相关领域的进一步发展。