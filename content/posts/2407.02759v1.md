---
author: 'TechScribe'
title: '"多智能体强化学习优化广告推荐系统：探索多场景合作的新前沿"'
date: '2024-07-03'
Lastmod: '2024-07-05'
description: 'Multi-Scenario Combination Based on Multi-Agent Reinforcement Learning to Optimize the Advertising Recommendation System'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Multi-Scenario Combination Based on Multi-Agent Reinforcement Learning to Optimize the Advertising Recommendation System](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02759v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02759v1)

## 摘要

本文探讨了在大型平台上使用多智能体强化学习（MARL）进行多场景优化的问题。文章将搜索、推荐和广告等场景视为一个合作性的、部分可观测的多智能体决策问题，并引入了多智能体循环确定性策略梯度（MARDPG）算法，该算法通过共享目标和策略通信来提升整体性能。研究结果显示，该方法在点击率（CTR）、转化率和总销售额等指标上取得了显著改进，证实了其在实际应用中的有效性。<!--more-->

## 原理

本文的核心工作原理基于强化学习和多智能体系统的理论基础。在多智能体设置中，智能体通过与环境的交互来最大化累积奖励。文章采用完全合作模型，所有智能体共享相同的目标函数，旨在优化平台的整体性能。具体实现中，文章结合了深度循环Q网络（DRQN）和确定性策略梯度（DPG）方法，通过循环神经网络（RNN）处理部分可观测环境，并使用演员-评论家框架来评估和生成动作。

## 流程

文章详细描述了MA-RDPG算法的工作流程。首先，从未公开的电子商务数据集中收集和预处理数据。然后，实现MA-RDPG算法，包括全局评论家、场景特定演员和基于LSTM的通信模块。训练过程中，智能体与环境持续交互，使用回放缓冲区存储经验并通过小批量梯度下降更新模型。性能评估通过标准A/B测试进行，比较关键指标如CTR、转化率和总商品价值（GMV）。

## 应用

本文提出的MA-RDPG算法在电子商务平台的多场景优化中显示出巨大潜力。未来，该框架可应用于其他领域，进一步细化智能体间的通信机制，扩展其在复杂环境中的应用范围。