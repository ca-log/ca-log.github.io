---
author: 'TechScribe'
title: '"Badllama 3: 快速移除大型语言模型安全微调的新方法"'
date: '2024-07-01'
Lastmod: '2024-07-05'
description: 'Badllama 3: removing safety finetuning from Llama 3 in minutes'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Badllama 3: removing safety finetuning from Llama 3 in minutes](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01376v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01376v1)

## 摘要

本文由Dmitrii Volkov发表于2024年7月1日，题为“Badllama 3: removing safety finetuning from Llama 3 in minutes”，探讨了大型语言模型（LLM）安全微调的脆弱性。论文指出，当攻击者能够访问模型权重时，现有的安全微调方法容易被绕过。研究评估了三种先进的微调方法——QLoRA、ReFT和Ortho，并展示了算法进步如何在不牺牲性能的情况下显著减少浮点运算（FLOPs）和优化需求。论文还展示了如何在单个GPU上快速移除Llama 3模型的安全微调，例如在5分钟内移除8B模型，在45分钟内移除70B模型。此外，研究还探讨了如何进一步减少这一时间，并展示了这种方法在Google Colab上的可行性，以及如何通过分发“jailbreak adapter”来快速移除模型的保护措施。<!--more-->

## 原理

论文的核心在于展示了如何通过先进的微调算法快速移除大型语言模型（LLM）的安全微调。具体来说，研究使用了三种微调方法：QLoRA、ReFT和Ortho。QLoRA是一种基于低秩适应（LoRA）的量化方法，通过分解权重增量矩阵为低秩矩阵的乘积，显著减少了训练参数的数量和成本。ReFT则是一种低秩表示微调方法，通过学习一组参数来修补激活，进一步减少了训练参数的数量。Ortho方法则是一种无需优化模型的方法，通过缓存激活并找到拒绝方向，然后通过添加或移除该方向来控制模型的拒绝行为。这些方法的先进性在于它们能够在不显著影响模型性能的情况下，大幅减少微调所需的时间和资源。

## 流程

论文的工作流程主要包括以下几个步骤：
1. **实验设置**：在Llama 3模型上进行实验，使用单个A100 GPU进行微调。
2. **微调方法评估**：评估三种微调方法——QLoRA、ReFT和Ortho，比较它们在移除安全微调方面的效率和性能。
3. **性能测试**：在标准LLM性能和安全基准上测试微调后的模型，确保移除安全微调不会显著影响模型的其他性能指标。
4. **结果分析**：分析实验结果，展示微调方法在减少拒绝不安全查询方面的有效性，并通过Attack Success Rate（ASR）等指标进行量化。
例如，论文中提到使用QLoRA方法在5分钟内成功移除了Llama 3 8B模型的安全微调，而使用Ortho方法在45分钟内完成了对Llama 3 70B模型的同样操作。

## 应用

论文的研究成果具有广泛的应用前景。首先，它为LLM的安全性研究提供了新的视角和工具，有助于更深入地理解模型安全微调的局限性。其次，这种方法可以被用于开发更高效、更安全的LLM微调技术，特别是在需要快速部署和调整模型的场景中。此外，研究还可能推动相关领域的技术进步，如模型安全性评估和对抗性攻击防御。