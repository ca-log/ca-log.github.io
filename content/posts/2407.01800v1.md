---
author: 'TechScribe'
title: '"保持学习率的恒定：归一化与投影在深度强化学习中的应用"'
date: '2024-07-01'
Lastmod: '2024-07-05'
description: 'Normalization and effective learning rates in reinforcement learning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Normalization and effective learning rates in reinforcement learning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01800v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01800v1)

## 摘要

本文探讨了深度强化学习和持续学习中归一化层的复兴，以及它们在改善损失景观条件和对抗高估偏差方面的多样化益处。然而，归一化带来了一个微妙但重要的副作用：网络参数范数的增长与有效学习率的衰减之间的等价性。这在持续学习环境中变得成问题，因为由此产生的有效学习率调度可能会相对于学习问题的时标过快地衰减到接近零。本文提出了一种简单的重参数化方法，称为归一化与投影（NaP），它将归一化层的插入与权重投影耦合，确保在整个训练过程中有效学习率保持恒定。这种方法揭示了深度强化学习中学习率调度的强大分析工具，并作为一种提高合成可塑性损失基准以及单任务和序列街机学习环境变体的鲁棒性的手段。本文还展示了这种方法可以轻松应用于流行的架构，如ResNet和Transformer，并在常见静态基准测试中恢复甚至略微提高了基础模型的性能。<!--more-->

## 原理

NaP方法的核心在于通过归一化层和权重投影的结合，确保网络的有效学习率在整个训练过程中保持恒定。归一化层通过确保激活的均值为零和单位方差，改善了网络的优化条件。权重投影则通过将权重参数投影到一个固定范数半径上，防止参数范数的无限增长，从而避免了有效学习率的衰减。这种结合不仅维持了网络的可塑性，还提高了对非平稳问题的鲁棒性。

## 流程

NaP的工作流程包括两个主要步骤：首先，在网络架构中的非线性之前插入归一化层；其次，在训练过程中定期将网络的权重投影到具有固定范数的半径上。具体来说，对于网络中的每个非线性层，如果尚未归一化，则应用层归一化。然后，计算参数的更新，并对网络中的每个权重参数进行权重投影。权重投影的具体操作包括检查参数是否为权重参数，如果是，则将其重新缩放到初始范数。

## 应用

NaP方法的应用前景广泛，特别是在需要长时间训练和面临非平稳问题的深度强化学习领域。它可以应用于各种架构，如ResNet和Transformer，并有望在未来的自适应学习率调度和深度强化学习中发挥重要作用。此外，NaP的原理也可能启发其他领域的研究，如持续学习和非平稳环境下的机器学习任务。