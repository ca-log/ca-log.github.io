---
author: 'TechScribe'
title: 'LLM 在维基百科编辑中的应用：检测和纠正偏见的能力评估'
date: '2024-07-04'
Lastmod: '2024-07-10'
description: 'Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality Norms'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality Norms](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.04183v1.pdf_0.jpg)](https://arxiv.org/abs/2407.04183v1)

## 摘要

这篇论文探讨了大型语言模型（LLM）在遵循特定社区规范方面的能力，特别是在检测和纠正维基百科编辑中的偏见方面。研究发现，LLM 在检测偏见方面表现不佳，但在生成符合规范的编辑方面表现较好。然而，LLM 的编辑方式与人类编辑不同，可能会带来一些潜在的问题。<!--more-->

## 原理

论文通过实验评估了 LLM 在检测和纠正维基百科编辑中的偏见方面的能力。在检测实验中，研究人员使用了 ChatGPT 3.5、Mistral-Medium 和 GPT-4 等模型，对维基百科的编辑进行分类，判断其是否违反了中立性原则。在生成实验中，研究人员使用了 GPT-4 模型，对违反中立性原则的编辑进行纠正。

## 流程

1. **数据收集**：研究人员从维基百科中立性语料库（WNC）中收集了编辑对，包括有偏见的编辑和中立的编辑。
2. **实验设置**：研究人员进行了多模型提示实验，使用了不同的定义和示例来指导 LLM 进行分类。
3. **实验结果**：研究人员发现，LLM 在检测偏见方面表现不佳，准确率仅为 64%。然而，LLM 在生成符合规范的编辑方面表现较好，能够去除 79%的被维基百科编辑删除的单词。

## 应用

论文的研究结果对于维基百科、模型构建者和政策制定者都具有一定的启示意义。对于维基百科来说，LLM 在检测违反中立性原则的编辑方面表现不佳，但在生成初始草稿方面具有潜力。对于模型构建者来说，研究结果表明，检索增强生成和多智能体系统等技术可以提高 LLM 的检测能力，而微调模型可以提高其与领域专家的一致性。对于政策制定者来说，研究结果强调了人类监督和利益相关者参与的重要性，以防止 LLM 过度偏离指令。