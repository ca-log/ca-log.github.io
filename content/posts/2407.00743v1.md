---
author: 'TechScribe'
title: 'AIMDiT：通过多模态维度变换实现对话情感识别的先进框架'
date: '2024-04-12'
Lastmod: '2024-07-05'
description: 'AIMDiT: Modality Augmentation and Interaction via Multimodal Dimension Transformation for Emotion Recognition in Conversations'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![AIMDiT: Modality Augmentation and Interaction via Multimodal Dimension Transformation for Emotion Recognition in Conversations](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00743v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00743v1)

## 摘要

本文提出了一种名为AIMDiT的新型框架，用于对话中的情感识别（ERC）任务中的多模态融合。该框架通过维度变换和参数高效的Inception块，设计了模态增强网络（MAN）和模态交互网络（MIN），以有效学习模态间的交互特征和模态内的特征。实验结果显示，AIMDiT在公共基准数据集MELD上的Acc-7和w-F1指标分别比现有最先进（SOTA）模型提高了2.34%和2.87%。<!--more-->

## 原理

AIMDiT框架的核心在于其模态增强网络（MAN）和模态交互网络（MIN）的设计。MAN通过维度变换和Inception卷积，将不同模态的1D序列转换为2D张量，从而捕获模态内的邻近时间点变化和模态间的同时间点变化。这种转换使得网络能够同时处理模态内和模态间的特征。MIN则进一步利用交叉模态Transformer和自模态Transformer，通过模态间的注意力机制来增强模态特征的交互和融合。这种设计不仅提高了特征的表达能力，还减少了信息冗余，使得情感识别更加准确。

## 流程

AIMDiT的工作流程包括四个主要步骤：首先，通过模态编码器提取文本、音频和视觉模态的特征；其次，MAN将这些特征转换为2D张量，并通过Inception块进行处理，以增强模态内和模态间的特征；然后，MIN利用交叉模态和自模态Transformer进一步融合这些特征；最后，情感分类器使用这些融合后的特征进行情感预测。整个流程通过残差连接和多层处理，确保了信息的有效传递和网络的深度学习能力。

## 应用

AIMDiT框架在对话情感识别领域具有广泛的应用前景，特别是在需要多模态数据融合的场景中，如人机交互、社交媒体分析和客户服务自动化等。其高效的多模态融合能力和先进的特征学习机制，使其能够适应复杂多变的对话环境，提供更准确的情感识别服务。