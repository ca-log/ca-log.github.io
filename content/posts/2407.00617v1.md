---
author: 'TechScribe'
title: '迭代纳什策略优化：通过无悔学习使LLM与人类偏好对齐'
date: '2024-06-30'
Lastmod: '2024-07-05'
description: 'Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00617v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00617v1)

## 摘要

本文探讨了在强化学习与人类反馈（RLHF）框架下，如何通过迭代纳什策略优化（INPO）算法，使大型语言模型（LLM）与人类普遍偏好对齐。传统的RLHF方法基于Bradley-Terry（BT）模型假设，可能无法完全捕捉人类偏好的复杂性。INPO算法通过无悔学习，让策略自我对弈，从而逼近纳什策略，避免了估计个体响应预期胜率的高计算或标注成本。实验结果显示，INPO在多个基准测试中显著优于现有迭代算法，特别是在AlpacaEval 2.0和Arena-Hard v0.1上取得了显著的改进。<!--more-->

## 原理

INPO算法的核心在于将LLM对齐问题建模为一个双人博弈，并通过无悔学习算法来学习纳什均衡策略。具体来说，算法通过在线镜像下降（OMD）方法，初始化策略为参考策略，然后在每次迭代中，当前策略生成响应对，并通过偏好数据集来优化策略。关键的创新点在于引入了一个新的损失目标，该目标直接在偏好数据集上最小化，从而避免了计算预期胜率的复杂性。理论分析和实验结果均表明，INPO算法能够有效地学习到接近纳什均衡的策略，且在多个基准测试中表现优异。

## 流程

INPO算法的工作流程如下：
1. 初始化策略π1为参考策略πref。
2. 对于每次迭代t，使用当前策略πt生成响应对{y(i) 1 , y(i) 2 }n i=1。
3. 通过偏好预言机P获取偏好数据集Dt = {y(i) w , y(i) l }n i=1。
4. 计算πt+1为argmin π∈Π Eyw,yl∼Dt [ht(π, yw, yl) − 1 2η ]2。
5. 重复步骤2-4，直到达到预设的迭代次数T。
6. 输出最终策略πT+1。

例如，在AlpacaEval 2.0基准测试中，INPO算法通过上述流程，最终在8B模型上实现了41.5%的长度控制胜率，显著优于其他方法。

## 应用

INPO算法不仅在现有的LLM对齐任务中显示出优越性能，而且其基于博弈论的方法论为未来更复杂的人类偏好建模提供了新的思路。随着算法的进一步优化和计算资源的增强，INPO有望在更多领域，如个性化内容生成、智能对话系统等，发挥重要作用。此外，该算法的研究也为理解人类决策过程和开发更高效的学习算法提供了宝贵的理论和实践基础。