---
author: 'TechScribe'
title: '"KDPL：无监督知识蒸馏引领视觉-语言模型的新纪元"'
date: '2024-07-03 12:24:40+00:00'
Lastmod: '2024-07-04 01:52:50.389998'
description: 'Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Improving Zero-shot Generalization of Learned Prompts via Unsupervised Knowledge Distillation](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.03056v1.pdf_0.jpg)](https://arxiv.org/abs/2407.03056v1)

## 摘要

本文提出了一种名为Knowledge Distillation Prompt Learning (KDPL)的新方法，旨在改进视觉-语言模型（VLMs）的零样本泛化能力。KDPL通过无监督的知识蒸馏技术，从更强大的模型中提取知识，以提高轻量级VLMs在下游任务中的表现，尤其是在数据有限的情况下。该方法不依赖于标注样本，能够无缝集成到现有的提示学习技术中，显著提高了零样本领域泛化、跨数据集泛化和基础到新颖类别的泛化问题的性能。KDPL的代码已公开发布，显示出其在实际应用中的潜力和先进性。<!--more-->

## 原理

KDPL的核心在于利用知识蒸馏技术，将一个更强大的视觉-语言模型（教师模型）的知识转移到轻量级的学生模型中。具体来说，教师模型通过其图像和文本编码器生成预测概率，而学生模型则通过学习这些预测概率来调整其提示参数。整个过程中，学生模型的其他参数保持不变，仅通过蒸馏损失函数来优化提示参数，从而实现无监督的学习过程。这种方法不仅提高了学生模型的泛化能力，还避免了传统方法中对标注数据的依赖。

## 流程

KDPL的工作流程包括以下几个步骤：
1. 教师模型对输入图像和文本进行编码，生成预测概率。
2. 学生模型使用相同的输入，通过其图像和文本编码器生成预测概率。
3. 通过计算教师和学生模型预测概率之间的KL散度，定义蒸馏损失函数。
4. 使用蒸馏损失函数优化学生模型的提示参数，而其他参数保持不变。
5. 重复上述过程，直到学生模型的性能达到满意水平。

例如，在处理一张图片时，教师模型可能会预测图片中的物体是“猫”，而学生模型在初始阶段可能预测不准确。通过KDPL的训练过程，学生模型逐渐学会更准确地预测，最终达到接近教师模型的性能。

## 应用

KDPL的应用前景广泛，特别是在需要零样本学习或小样本学习的场景中，如图像分类、目标检测和视觉问答等。由于其不依赖于标注数据，KDPL可以大幅降低数据收集和标注的成本，加速模型的部署和应用。此外，KDPL的灵活性和集成性使其能够与多种现有的提示学习技术结合，进一步提升模型的性能和适应性。