---
author: 'TechScribe'
title: '专家专业化微调：稀疏架构大型语言模型的参数高效定制'
date: '2024-07-02'
Lastmod: '2024-07-05'
description: 'Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01906v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01906v1)

## 摘要

本文探讨了在资源受限的情况下，如何通过参数高效微调（PEFT）方法定制大型语言模型（LLMs），特别是在稀疏架构的LLMs中。尽管已有多种针对密集架构LLMs的PEFT方法，但稀疏架构LLMs的PEFT研究仍不足。本文主要研究了具有混合专家（MoE）架构的LLMs的PEFT方法，并提出了专家专业化微调（ESFT），该方法在保持或超越全参数微调性能的同时，显著提高了微调效率并节省了计算资源。<!--more-->

## 原理

ESFT方法的核心在于选择与下游任务最相关的专家进行微调，同时冻结其他专家和模块的参数。通过分析任务激活专家的分散程度，发现特定任务的路由分布倾向于高度集中，而不同任务之间的激活专家分布差异显著。ESFT通过计算专家与任务的相关性得分（如平均门控分数和令牌选择比率），选择相关性最高的专家进行微调，从而在保持专家专业化的同时，提高训练效率和模型性能。

## 流程

ESFT的工作流程包括数据采样、专家相关性得分计算、专家选择和微调。首先，从训练数据中随机采样一个子集用于专家选择。然后，计算每个专家与任务的相关性得分，选择得分最高的专家进行微调。在训练和推理过程中，只有选定的专家参数会被更新，其他专家和模块保持冻结状态。通过这种方式，ESFT能够高效地选择和微调与任务最相关的专家，从而优化模型性能和计算效率。

## 应用

ESFT方法适用于需要定制化LLMs的各种场景，特别是在模型需要适应特定领域或任务，同时保持高效计算和资源利用的情况下。该方法不仅提高了模型在特定任务上的性能，还保持了模型在通用任务上的能力，具有广泛的应用前景，包括但不限于自然语言处理、代码生成、法律文本分析等领域。