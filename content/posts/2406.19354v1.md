---
author: 'TechScribe'
title: '探索大型语言模型中的信念修正：挑战与解决方案'
date: '2024-06-27'
Lastmod: '2024-07-05'
description: 'Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Fundamental Problems With Model Editing: How Should Rational Belief Revision Work in LLMs?](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.19354v1.pdf_0.jpg)](https://arxiv.org/abs/2406.19354v1)

## 摘要

本文探讨了大型语言模型（LLMs）中的模型编辑问题，即如何随着时间的推移更新模型以反映世界的新事实。文章指出，尽管模型编辑在哲学上是一个长期未解决的问题，但在实际应用中，我们需要能够控制语言模型中的知识。文章批评了模型编辑问题的标准表述，并提出了一个正式的测试平台来研究模型编辑。文章首先描述了模型编辑的12个开放问题，这些问题涉及定义问题、开发基准以及假设LLMs具有可编辑的信念。接着，文章介绍了一个基于Wikidata的半合成数据集，用于评估模型编辑，并与一个理想化的贝叶斯代理进行比较，以精确量化语言模型在信念修正方面的不足。文章鼓励进一步研究，探索在这些设置中可以与黄金标准进行比较的情况。<!--more-->

## 原理

模型编辑的工作原理涉及如何更新语言模型以反映新的事实，同时保持逻辑一致性。文章提出了一个半合成数据集，该数据集基于Wikidata，允许我们评估编辑与理想化贝叶斯代理给出的标签。通过这种方式，可以精确地量化模型编辑在信念修正方面的不足。具体来说，文章使用了一个从Wikidata中提取的生成模型，该模型定义了关于假设世界的句子，并训练了一个自回归Transformer模型。通过将语言模型的概率与贝叶斯模型的后验概率进行比较，文章展示了模型编辑在泛化到其他相关信念时的不足。

## 流程

文章的工作流程包括以下几个关键步骤：
1. 定义一个基于Wikidata的生成模型，该模型用于创建关于假设世界的句子。
2. 训练一个自回归Transformer模型，该模型能够学习并记忆这些句子。
3. 创建一个评估数据集，其中包含编辑请求和测试案例，用于评估模型编辑的效果。
4. 通过比较编辑后的语言模型概率与贝叶斯模型的后验概率，评估模型编辑的性能。
例如，文章中提到了一个具体的编辑请求案例，即编辑模型以替换关于Grace Stone Coates教育背景的事实，并展示了模型在处理这一编辑请求时的表现。

## 应用

模型编辑的应用前景广泛，特别是在需要更新模型以反映新信息或修正错误信息的场景中。例如，在安全相关的应用中，模型编辑可以用于去除敏感信息或修正个别事实错误。此外，模型编辑还可以用于指导模型的行为，使其适应不断变化的世界状态。随着技术的进步，模型编辑有望成为开发安全、可适应人工智能系统的重要工具。