---
author: 'TechScribe'
title: '"利用LLMs进行项目评估：心理测量分析的新视角"'
date: '2024-07-15'
Lastmod: '2024-07-16'
description: 'Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Leveraging LLM-Respondents for Item Evaluation: a Psychometric Analysis](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.10899v1.pdf_0.jpg)](https://arxiv.org/abs/2407.10899v1)

## 摘要

本文探讨了利用大型语言模型（LLMs）如GPT-3.5、GPT-4等进行项目评估的心理测量分析。研究旨在通过模拟人类回答的方式，评估LLMs在数学领域的应用能力，特别是在大学代数中的表现。研究发现，尽管某些LLMs在某些领域的能力超过了大学生，但单一的LLM无法完全模拟人类的回答模式。然而，通过组合多个LLMs的回答，可以更接近人类的能力分布。此外，研究还评估了数据增强策略，发现重采样方法在提高项目参数的Spearman相关性方面最为有效。<!--more-->

## 原理

研究采用了六种不同的LLMs（GPT-3.5, GPT-4, Llama 2, Llama 3, Gemini Pro, Cohere Command R Plus）来生成模拟的大学代数问题回答。这些模型通过API被提示回答从OpenStax教科书中选取的20个问题，每个模型生成150个回答。通过Item Response Theory (IRT)分析，评估这些模型生成的回答与人类回答在心理测量属性上的相似性。IRT分析通过估计潜在能力（theta）和项目难度（beta）来比较模型和人类回答的分布，从而确定LLMs是否能有效模拟人类的能力分布。

## 流程

1. 选择LLMs：从六种不同的LLMs中选择模型进行实验。
2. 项目和提示工程：从OpenStax教科书中选择20个大学代数问题，并对每个问题进行特定格式的提示设计。
3. 生成回答：每个LLM生成150个回答，由研究者手动评分以评估准确性。
4. 数据增强：使用重采样方法增强数据，通过将AI生成的回答与人类回答进行匹配和混合，以提高数据集的多样性和代表性。
5. IRT分析：使用IRT模型分析LLMs和人类回答的项目参数，比较两者的相关性和分布。

## 应用

该研究展示了LLMs在教育测量中的潜在应用，特别是在低风险形成性或总结性评估中，LLMs可以作为一种低成本的选项来筛选高质量的项目。此外，这种方法提供了一种可扩展的方式来评估大量由生成AI产生的项目，适用于计算机辅导系统和各种教育评估场景。