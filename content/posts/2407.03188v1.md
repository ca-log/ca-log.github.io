---
author: 'TechScribe'
title: 'MuDiT & MuSiT：引领口语描述到歌曲生成的人工智能创新框架'
date: '2024-07-03 15:12:36+00:00'
Lastmod: '2024-07-04 01:52:42.565617'
description: 'MuDiT & MuSiT: Alignment with Colloquial Expression in Description-to-Song Generation'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![MuDiT & MuSiT: Alignment with Colloquial Expression in Description-to-Song Generation](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.03188v1.pdf_0.jpg)](https://arxiv.org/abs/2407.03188v1)

## 摘要

本文探讨了生成式人工智能与人类艺术创作过程的交叉领域，特别是自动歌曲创作中的人机对齐问题。研究提出了一项新的任务——口语描述到歌曲生成，旨在使生成的内容与人类的口语表达相匹配。该任务旨在弥合口语语言理解和听觉表达之间的差距，最终目标是创作出符合人类听觉期待并符合音乐规范的歌曲。为了解决该领域数据稀缺的问题，研究提出了菜虫音乐数据集（CaiMD），该数据集由专业音乐家和业余爱好者手动标注，提供了多样的视角和全面的口语描述理解。此外，研究还提出了一种创新的单阶段框架MuDiT/MuSiT，用于在歌曲创作中实现有效的人机对齐。该框架不仅实现了口语语言和听觉音乐感知之间的跨模态理解，还确保生成的歌曲与用户的期望结果相符。<!--more-->

## 原理

MuDiT/MuSiT框架的核心在于其能够处理口语描述并生成与之对齐的歌曲。该框架利用一个经过微调的大型语言模型（LLM）来生成歌词，并结合一个DiT/SiT模型来生成旋律、和声、节奏、人声和乐器等其他音乐组件。这种方法确保了所有生成的音乐组件在声音上的和谐一致，从而更好地与人类的听觉期待相共鸣。MuDiT/MuSiT通过跨模态编码器MuChin将文本描述转换为向量，然后与随机噪声结合，输入到基于变分自编码器（VAE）的扩散模型中，生成高质量、结构化的歌曲。此外，该框架还利用VAE和HIFI-GAN将生成的歌曲内容解码为Mel频谱图，并转换为WAV音频文件。

## 流程

MuDiT/MuSiT的工作流程从接收用户的口语描述开始，通过MuChin编码器将描述转换为向量。这些向量与随机噪声结合，作为扩散模型DiT/SiT的输入。在扩散过程中，模型逐步从噪声中生成音乐内容，最终输出为VAE的潜在空间表示。随后，VAE解码器和HIFI-GAN将这些潜在空间表示转换为Mel频谱图，并进一步转换为最终的WAV音频文件。整个过程中，歌词和结构信息通过交叉注意力机制与音乐内容相结合，确保生成的歌曲与用户的口语描述高度对齐。

## 应用

MuDiT/MuSiT框架的应用前景广泛，特别是在自动歌曲创作、音乐教育、娱乐产业和个性化音乐推荐等领域。该框架能够根据用户的口语描述生成定制化的音乐内容，满足不同用户的个性化需求。随着技术的进一步发展和数据集的丰富，预计该框架将在音乐创作和相关领域发挥更大的作用，推动人机协同在艺术创作中的应用。