---
author: 'TechScribe'
title: '探索TADPoLe：利用文本感知扩散模型革新策略学习'
date: '2024-07-02 03:08:20+00:00'
Lastmod: '2024-07-04 01:17:43.925286'
description: 'Text-Aware Diffusion for Policy Learning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Text-Aware Diffusion for Policy Learning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01903v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01903v1)

## 摘要

本文介绍了一种名为Text-Aware Diffusion for Policy Learning (TADPoLe)的新型强化学习框架，该框架利用预训练的、冻结的文本条件扩散模型来计算密集的零样本奖励信号，以支持文本对齐的策略学习。TADPoLe的核心在于利用大规模预训练生成模型中编码的丰富先验知识，使代理不仅能够以文本对齐的方式行动，还能与从互联网规模训练数据中总结的自然性概念保持一致。实验证明，TADPoLe能够在没有真实奖励或专家演示的情况下，通过自然语言指定的新目标实现和连续运动行为学习，且在人类评估中表现出更自然的质量。此外，TADPoLe在Meta-World环境中的机器人操作任务上也展现出竞争力。<!--more-->

## 原理

TADPoLe的工作原理基于一个关键洞察：强化学习策略可以被视为在具有视觉渲染能力的环境中操作时的一个以代理为中心的隐式视频表示。策略πθ通过选择动作并将其转换为视频子序列来生成视频，这些子序列通过环境的渲染函数实现。策略因此可以被看作是根据其选择的动作迭代生成帧；另一方面，文本到图像的扩散模型也可以生成静态图像帧，但条件是自然语言。这样，策略和扩散模型之间就可以建立联系，策略“生成”的帧或视频段可以通过评估文本条件扩散模型生成相同视觉内容的可能性来批判，从而提供密集的文本对齐奖励信号以指导策略学习。

具体来说，TADPoLe通过使用生成扩散模型以判别方式实现文本条件策略学习。它将奖励信号计算为两个奖励项的加权组合，这两个奖励项旨在衡量渲染观察与文本条件之间的对齐程度以及代理行为的自然性。通过这种方式，我们可以有效地“提炼”扩散模型中捕获的自然视觉和运动先验以及视觉文本对齐理解到策略中。

## 流程

TADPoLe的工作流程包括以下步骤：
1. 在每个时间步t，通过预训练的文本到图像扩散模型计算奖励rt，该模型将渲染的后续图像ot+1与描述感兴趣行为的提供的文本标题y进行对齐评分。
2. 通过向渲染图像ot+1添加采样的高斯源噪声向量ϵ0来产生噪声观察˜ot+1，并使用扩散模型进行无条件预测ˆϵϕ(˜ot+1; tnoise)和条件预测ˆϵϕ(˜ot+1; tnoise, y)。
3. 计算两个预测之间的均方误差（MSE）作为奖励信号ralign t，以最大化：
   ralign t = ∥ˆϵϕ(˜ot+1; tnoise, y) − ˆϵϕ(˜ot+1; tnoise)∥2 2。
4. 为了鼓励自然的人类感知行为，通过扩散模型预测应用的精确源噪声向量的准确性来近似行为的自然性。直观地说，如果模型自愿预测精确的噪声向量，从而完美重建查询图像，那么扩散模型认为原始渲染帧是合理的自然（根据扩散模型捕获的先验）。
5. 最终的奖励信号rt是通过对单个项进行可调超参数常数缩放并应用symlog变换操作来组合这两个项：
   rt = symlog(w1 ∗ ralign t ) + symlog(w2 ∗ rrec t )。

## 应用

TADPoLe的应用前景广泛，特别是在需要通过自然语言灵活指定目标或行为的领域，如角色动画和机器人操作。由于其能够学习新颖的零样本策略，且不需要针对每个任务手动指定复杂的奖励函数，TADPoLe为将大规模预训练中总结的先验知识提炼到策略中提供了有希望的路径，最终导致在任意环境中学习更自然对齐的行为。此外，TADPoLe的框架可以扩展到视频级别的扩散模型，进一步增强其在连续运动行为学习中的能力。