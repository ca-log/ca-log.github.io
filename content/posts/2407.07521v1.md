---
author: 'TechScribe'
title: '探索CHILLI：一种数据上下文感知的扰动方法，提升AI的可解释性'
date: '2024-07-10'
Lastmod: '2024-07-11'
description: 'CHILLI: A data context-aware perturbation method for XAI'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![CHILLI: A data context-aware perturbation method for XAI](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.07521v1.pdf_0.jpg)](https://arxiv.org/abs/2407.07521v1)

## 摘要

本文探讨了在机器学习（ML）模型中提高可解释性AI（XAI）的重要性，特别是在高风险或伦理敏感的应用中。传统的XAI方法通过扰动数据来近似模型行为，但这些方法常常忽视特征依赖性，导致解释基于可能不现实的数据。为了解决这一问题，本文提出了一种新的框架——CHILLI，该框架通过生成与基础模型训练数据一致的上下文感知扰动，来提高解释的合理性和准确性。CHILLI框架的提出，为ML模型的可解释性提供了新的视角和方法，特别是在需要高度信任和理解的领域。<!--more-->

## 原理

CHILLI框架的核心在于其能够生成与训练数据上下文一致的扰动，这些扰动忠实于基础模型的行为。具体来说，CHILLI通过以下几个关键步骤实现这一目标：
1. **上下文感知接近度测量**：传统的接近度测量方法（如欧几里得距离）在处理非线性或循环特征时可能不准确。CHILLI提出了一种改进的接近度测量方法，该方法考虑了每个特征的尺度边界，确保计算的距离真正代表特征间的实际距离。
2. **领域代表性扰动生成**：CHILLI借鉴了SMOTE算法，通过线性插值在实例和训练数据中的其他随机实例之间生成扰动。这种方法确保了扰动不仅符合训练数据的分布，而且考虑了特征的实际边界和依赖性。
3. **局部代理模型拟合**：通过上述生成的扰动数据，CHILLI拟合一个局部代理模型，该模型能够更好地反映基础模型在特定实例附近的预测行为。

## 流程

CHILLI的工作流程可以概括为以下几个步骤：
1. **输入数据准备**：选择需要解释的实例和相关的训练数据集。
2. **扰动生成**：使用CHILLI算法生成一组上下文感知的扰动数据。
3. **代理模型拟合**：利用生成的扰动数据拟合一个局部代理模型。
4. **解释生成**：基于拟合的代理模型生成解释，这些解释展示了每个特征对预测结果的贡献。
例如，在WebTRIS数据集上，CHILLI生成的扰动更符合实际交通数据的分布，从而使得生成的解释更加准确和可靠。

## 应用

CHILLI框架的应用前景广泛，特别是在需要高度信任和理解的领域，如金融系统、医疗健康和刑事司法。通过提供更准确和可靠的解释，CHILLI有助于增强用户对ML模型的信任，促进这些技术在关键领域的更广泛应用。此外，CHILLI的方法也可以扩展到其他需要解释复杂模型的场景，如自然语言处理和图像识别。