---
author: 'TechScribe'
title: 'FPGA上的高效能神经网络加速器：AMU的突破性设计与应用'
date: '2024-07-02'
Lastmod: '2024-07-10'
description: 'Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Fast, Scalable, Energy-Efficient Non-element-wise Matrix Multiplication on FPGA](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02362v2.pdf_0.jpg)](https://arxiv.org/abs/2407.02362v2)

## 摘要

本文提出了一种基于FPGA的高吞吐量、可扩展且能效高的非元素级矩阵乘法单元（AMU），作为神经网络（NN）的基本组件。AMU通过优化MADDNESS算法中的层间和层内冗余，设计了一种快速、高效的近似矩阵乘法模块。实验结果显示，使用AMU的FPGA基数量化神经网络（QNN）加速器相比现有解决方案，吞吐量提高了9倍，能效提高了112倍。<!--more-->

## 原理

AMU通过消除元素级算术操作，引入了三种优化策略（I/O数据修剪、特征图重组和参数压缩），从而将乘法计算开销与输入特征图的分辨率解耦。这种设计使得AMU能够在FPGA上实现显著的吞吐量和能效提升。AMU的核心在于其能够通过编码、检索和求和操作替代传统的元素级算术操作，从而在保持计算效率的同时，大幅减少资源消耗。

## 流程

AMU的工作流程包括三个主要组件：分配器、编码器和聚合器。分配器负责解包接收到的数据包并分配给多个并行编码器。编码器通过读取数据块并使用分割值来识别下一个数据块，最终确定原型的地址。聚合器则负责从LUT中检索部分点积结果，并进行求和和偏置操作，最后通过连续阈值处理输出结果。整个流程通过优化内存分配和访问设计，实现了高效的数据处理和低延迟。

## 应用

AMU的设计不仅适用于IoT应用中的面部识别，还能在健康监测等需要高精度的应用中发挥作用。此外，AMU的灵活性和高效性使其在各种AI应用场景中具有广泛的应用前景，特别是在需要高性能和低能耗的边缘计算环境中。