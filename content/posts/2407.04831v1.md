---
author: 'TechScribe'
title: '探索大型语言模型中的代码幻觉：HallTrigger技术的突破与应用'
date: '2024-07-05'
Lastmod: '2024-07-10'
description: 'Code Hallucination'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Code Hallucination](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.04831v1.pdf_0.jpg)](https://arxiv.org/abs/2407.04831v1)

## 摘要

本文由Mirza Masfiqur Rahman和Ashish Kundu共同撰写，针对大型语言模型（LLM）在代码生成中的“幻觉”现象进行了深入研究。这些模型虽然广泛用于代码辅助生成和完整程序生成，但生成的代码往往在正确性、真实性和可靠性方面存在问题，这些问题统称为LLM幻觉。本文通过手动生成幻觉代码并提出一种名为HallTrigger的技术，展示了如何有效地触发这些幻觉，而无需访问模型的架构或参数。研究结果表明，HallTrigger在流行的黑盒模型中非常有效，LLM幻觉对软件开发有重大影响。<!--more-->

## 原理

HallTrigger技术利用LLM的三个动态属性来设计能够成功触发幻觉的提示。首先，模型在同一上下文中的交互式和重复性提示可以显著修改初始响应。其次，通过设计元提示，模型可以在用户和代理之间生成对话，从而激发模型的创造性，忽略事实性。最后，利用基于人类反馈的强化学习（RLHF）机制，通过在用户-代理元提示中附加奖励过程，引导模型向特定方向生成内容。这些方法共同作用，使得模型在不访问内部架构或参数的情况下，能够生成包含幻觉的代码。

## 流程

研究团队首先定义了LLM代码幻觉的概念，并通过实验设计了一种自动触发幻觉的方法。他们选择了三个黑盒LLM模型（如ChatGPT、Google Gemini和Microsoft Copilot）进行实验，展示了不同类型的幻觉及其对开发标准、功能和客观正确性的影响。例如，在触发算法具有不切实际的边界时，模型会生成超出当前技术水平的解决方案，如ChatGPT在生成最小顶点覆盖问题的代码时，错误地提供了具有1.5近似因子的贪婪算法。

## 应用

本文的研究不仅揭示了LLM在代码生成中的幻觉问题，还提供了一种实用的方法来触发和分析这些幻觉。这对于改进代码生成模型的质量和可靠性具有重要意义。未来，这种方法可以用于开发更有效的模型训练和评估工具，以减少幻觉现象，提高生成代码的实用性和安全性。