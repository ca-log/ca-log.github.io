---
author: 'TechScribe'
title: '推进公平的解释性AI：跨学科专家小组审查框架的应用与前景'
date: '2024-05-29'
Lastmod: '2024-07-05'
description: 'Interdisciplinary Expertise to Advance Equitable Explainable AI'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Interdisciplinary Expertise to Advance Equitable Explainable AI](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.18563v1.pdf_0.jpg)](https://arxiv.org/abs/2406.18563v1)

## 摘要

本文由Chloe R. Bennett等作者撰写，聚焦于通过跨学科专家小组（IDEP）审查可解释人工智能（XAI）分析，以推进公平的解释性AI。文章指出，尽管人工智能在医疗健康领域的应用迅速增长，但仍存在针对受广泛结构性压迫群体的偏见和性能不佳问题。文章提出，通过利用社会流行病学和健康公平的最佳实践，可以提高AI的解释性，从而帮助我们发展对发现关联的假设。IDEP框架允许从多个角度讨论和批判性地评估AI模型解释，识别偏见区域和未来研究方向。文章强调了跨学科专家小组在产生更准确、公平解释方面的重要性，这些解释是历史和情境化的。跨学科小组讨论有助于减少偏见，识别潜在的混杂因素，并识别研究空白，从而为AI模型改进提供机会。<!--more-->

## 原理

IDEP框架的工作原理是基于世界卫生组织的健康社会决定因素模型、Bronfenbrenner的生态系统理论和Krieger的生态社会理论。这些模型和理论解释了塑造健康的社会、政治、经济、代际、生命历程和环境因素，提出健康不仅仅是生物学决定的，尽管生物学受到这些因素的影响。通过这些模型和理论，IDEP框架旨在将社会和政治、社区、组织和个人背景置于专家讨论的前沿。提示帮助小组成员识别相关情境特定证据和理由，然后用于检查和讨论模型发现，以及它们是否由于变化、偏见或混杂而产生。

## 流程

IDEP审查过程包括四个阶段：首先，临床医生与AI研究人员一起审查XAI方法的发现，描述基于解剖学和临床知识的XAI发现中的视觉属性。其次，将初步解释提交给社会和社技术科学家审查，每位成员提供基于经验、实证研究和/或文献的支持或冲突的额外证据。第三阶段，整个专家小组包括AI研究人员，审查在第二阶段提出的问题，并生成关于信号为何对分类器有用的假设。最后阶段，主题专家和AI研究人员一起考虑第一至第三阶段提出的所有证据，支持或冲突初步解释，并就总结XAI发现的最佳方式达成共识。

## 应用

IDEP框架的应用前景广泛，特别是在医疗健康领域的AI模型中，包括生成式AI。这些模型存在提供不准确诊断或医疗信息的风险，可能加剧偏见，并可能被误解为描述因果关系。通过与其他提出的偏见检测方法（如数据表、数据卡、模型卡和健康表）结合使用，IDEP框架可以帮助评估关联是否是数据集中存在的偏见的结果。此外，通过包括社区成员或患者倡导者作为小组成员，该方法可以轻松地融入社区参与式研究。