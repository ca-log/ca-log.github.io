---
author: 'TechScribe'
title: '探索大型语言模型在游戏中的战略思维：一个可扩展的基准测试与排行榜'
date: '2024-07-10'
Lastmod: '2024-07-11'
description: 'Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.07796v1.pdf_0.jpg)](https://arxiv.org/abs/2407.07796v1)

## 摘要

本文介绍了一种新颖且可扩展的大型语言模型（LLM）基准测试，通过基于网格的游戏如井字棋、四子棋和五子棋来评估LLM的性能。该基准测试提供了一个开源的游戏模拟代码，允许LLM在游戏中竞争，并生成详细的JSON、CSV、TXT和PNG格式的数据文件，用于排行榜排名和进一步分析。研究结果显示，LLM在不同游戏和提示类型中的表现存在显著差异，分析涵盖了胜率、淘汰率和无效移动分析。该研究增强了我们对LLM在未专门训练的游戏中能力的理解，有助于评估其规则理解和战略思维能力，并为未来在复杂决策场景中探索其应用奠定了基础。<!--more-->

## 原理

该基准测试通过基于网格的游戏如井字棋、四子棋和五子棋来评估LLM的性能。游戏模拟代码允许LLM在游戏中竞争，并生成详细的数据文件，用于排行榜排名和进一步分析。LLM在游戏中的表现通过胜率、淘汰率和无效移动等指标进行评估。此外，还分析了LLM在游戏中错过的胜利机会和阻止对手胜利的机会，以评估其战略思维能力。该基准测试的先进性在于其可扩展性和开源性，鼓励社区贡献新的游戏和结果，以不断更新和完善评估框架。

## 流程

游戏模拟的工作流程如下：用户通过网页界面选择游戏类型和参与的LLM模型，以及提示类型（列表、插图、图像）。游戏模拟开始后，系统将选择的提示发送给第一个LLM模型，等待其做出移动。一旦收到响应，系统更新游戏状态，并发送给第二个LLM模型进行下一步移动。这个过程持续进行，直到游戏结束或某个玩家被淘汰。每次移动的详细信息都被记录下来，包括游戏状态、LLM的响应和移动结果。这些数据以JSON、CSV、TXT和PNG格式存储，用于后续分析和排行榜更新。

## 应用

该基准测试的应用前景广泛，不仅限于游戏领域。通过评估LLM在游戏中的表现，可以推断其在其他复杂决策场景中的潜在应用，如机器人、自动驾驶系统和交互式AI。此外，该基准测试的开源性和可扩展性使其成为研究和开发LLM能力的宝贵工具，有助于推动人工智能向通用人工智能（AGI）的进步。