---
author: 'TechScribe'
title: '“This&That”：通过语言和手势控制视频生成实现机器人规划的先进方法'
date: '2024-07-08'
Lastmod: '2024-07-10'
description: 'This&That: Language-Gesture Controlled Video Generation for Robot Planning'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![This&That: Language-Gesture Controlled Video Generation for Robot Planning](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.05530v1.pdf_0.jpg)](https://arxiv.org/abs/2407.05530v1)

## 摘要

本文介绍了一种名为“This&That”的机器人学习方法，该方法通过利用视频生成模型来实现广泛的通信、规划和任务执行。这种方法的核心是通过互联网规模的数据训练视频生成模型，以处理视频规划中的三个基本挑战：1) 通过简单的人类指令进行明确的任务通信；2) 尊重用户意图的可控视频生成；3) 将视觉规划转化为机器人动作。本文提出了一种结合语言和手势的条件生成技术，这种方法比仅使用语言的方法更简单、更清晰，特别是在复杂和不确定的环境中。此外，本文还提出了一种行为克隆设计，无缝地结合了视频规划。实验结果表明，“This&That”框架在解决上述三个挑战方面表现出色，证明了视频生成作为可泛化任务规划和执行的中间表示的有效性。<!--more-->

## 原理

“This&That”框架的工作原理基于两个主要组件：语言-手势条件视频生成和基于视频的机器人规划。首先，通过一个大规模的、开放词汇的文本到视频扩散模型（VDM），该模型被微调以适应机器人设置。VDM通过输入描述任务的语言（使用指示词如“这个”和“那里”）和手势（表示为第一帧图像中的2D位置）来生成视频。这种结合语言和手势的条件生成技术有效地整合了多模态条件，生成与人类意图高度一致的视频，即使在复杂场景中也能保持高水平的视频质量。其次，通过行为克隆（BC）架构，如动作分块变换器（Action Chunking Transformer），将生成的视频预测与实时观察结合，实现视频规划和操作的统一。这种BC架构通过交叉关注视频帧，有效地将视频规划转化为机器人动作。

## 流程

1. **视频生成阶段**：使用VDM模型，输入包括初始帧、语言描述和手势位置。模型生成一系列视频帧，每个帧都基于不同的手势和文本提示。
2. **机器人执行阶段**：生成的视频帧被输入到行为克隆模型中，该模型通过Transformer编码器-解码器架构处理视频帧和实时图像观察。模型预测下一组动作，这些动作基于当前图像观察、机器人末端执行器姿态和视频序列。
3. **动作执行**：机器人执行预测的动作，并在执行完整动作块后再次查询模型。

## 应用

“This&That”框架的应用前景广泛，特别是在需要复杂任务规划和执行的领域，如家庭服务机器人、工业自动化和辅助技术。该方法通过简单的语言和手势指令，使得非专业用户也能轻松控制机器人执行各种任务，提高了机器人的可用性和适应性。此外，该框架的泛化能力使其能够适应新任务和环境，具有很高的灵活性和扩展性。