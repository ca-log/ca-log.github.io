---
author: 'TechScribe'
title: '探索Qwen2：超越传统，引领未来的大型语言模型'
date: '2024-07-15'
Lastmod: '2024-07-16'
description: 'Qwen2 Technical Report'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Qwen2 Technical Report](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.10671v1.pdf_0.jpg)](https://arxiv.org/abs/2407.10671v1)

## 摘要

本文介绍了Qwen2系列，这是我们大型语言模型和大型多模态模型的最新成员。我们发布了一系列基础和指令微调的语言模型，参数范围从0.5亿到720亿，包括密集模型和专家混合模型。Qwen2超越了大多数先前的开放权重模型，包括其前身Qwen1.5，并在语言理解、生成、多语言能力、编码、数学和推理等多个基准上与专有模型相比表现出竞争力。<!--more-->

## 原理

Qwen2系列基于Transformer架构，采用下一个词预测进行训练。该系列包括基础语言模型（预训练但未对齐人类偏好）和指令微调模型（通过单轮和多轮指令跟随数据集进行微调，适用于聊天和代理目的）。我们发布了四个密集模型，参数分别为0.5亿、1.5亿、7亿和72亿，以及一个包含57亿参数的专家混合模型，其中每个token激活14亿参数。较小的模型设计用于智能手机、耳机和智能眼镜等便携设备的轻松部署，而较大的模型则适用于不同规模的GPU部署。

## 流程

所有模型都在一个高质量、大规模的数据集上进行了预训练，该数据集包含超过7万亿个token，涵盖广泛的领域和语言。与之前的Qwen版本相比，Qwen2包括更广泛的语料数据，增强了代码和数学内容的数量和质量。这被假设为提高大型语言模型的推理能力。在后期训练阶段，所有模型都经过了监督微调和直接偏好优化（DPO），通过从人类反馈中学习来对齐人类偏好。这一过程赋予模型有效遵循指令的能力。

## 应用

Qwen2模型的广泛参数范围和多语言能力使其适用于多种应用场景，包括自然语言处理、代码生成、数学问题解决和多语言内容生成。通过在Hugging Face和ModelScope等平台上公开模型权重和补充材料，我们促进了社区的创新和可访问性，使研究人员和开发者能够利用Qwen2的全部潜力进行各种应用和研究项目。