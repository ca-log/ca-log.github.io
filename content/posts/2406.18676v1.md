---
author: 'TechScribe'
title: 'DPA-RAG：解决 RAG 系统偏好不一致问题的通用框架'
date: '2024-06-26'
Lastmod: '2024-07-05'
description: 'Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.18676v1.pdf_0.jpg)](https://arxiv.org/abs/2406.18676v1)

## 摘要

本文提出了一种名为 DPA-RAG 的通用框架，旨在解决检索增强生成（RAG）系统中不同组件之间的偏好不一致问题，提高生成性能。通过引入多粒度偏好对齐任务和知识自对齐机制，DPA-RAG 能够更好地满足大语言模型（LLM）的知识需求，从而提高生成质量。<!--more-->

## 原理

DPA-RAG 由三个关键组件组成：
1. **偏好知识构建**：通过初步分析，提取影响 LLM 推理偏好的特定知识，并引入五种查询增强策略和质量过滤过程，以合成高质量的偏好知识。
2. **Reranker-LLM 对齐**：通过多任务优化，将点对、点对和对比偏好对齐能力联合集成到 reranker 中，实现 RAG 组件之间的外部偏好对齐。
3. **LLM 自对齐**：在标准监督微调（SFT）阶段之前引入预对齐阶段，使 LLM 能够从多个文档中捕获与推理偏好对齐的知识，完成 LLM 的内部自对齐。

## 流程

1. **任务定义**：将查询 q 转换为信息需求，通过密集检索器获取相关文档，利用 reranker 对文档进行重排，并由 LLM 基于查询和重排后的文档生成目标文本。
2. **偏好知识构建**：通过提取偏好知识、进行查询增强和质量过滤，构建高质量的偏好数据集。
3. **Reranker-LLM 对齐**：通过多粒度偏好对齐任务，包括点对偏好对齐、点对偏好对齐和对比偏好对齐，对 reranker 进行微调，以实现与 LLM 知识偏好的对齐。
4. **LLM 自对齐**：在预对齐阶段，通过随机选择文档和 k-1 个随机文档，构建 top-k 文档集，并使用特定模板进行训练，使 LLM 能够隐式学习捕获与推理偏好对齐的知识。在监督微调阶段，加载预训练参数，并使用偏好对齐的 reranker 对文档进行重排，形成最终的训练集。

## 应用

DPA-RAG 框架在四个知识密集型问答数据集上的实验结果表明，该方法优于所有基线方法，并且可以无缝集成黑盒和开源的 LLM 阅读器。进一步的定性分析和讨论也为开发可靠的 RAG 系统提供了经验指导。