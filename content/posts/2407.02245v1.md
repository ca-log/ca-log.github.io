---
author: 'TechScribe'
title: '“Safe CoR：双专家框架引领安全强化学习新纪元”'
date: '2024-07-02 13:05:16+00:00'
Lastmod: '2024-07-04 01:17:28.410250'
description: 'Safe CoR: A Dual-Expert Approach to Integrating Imitation Learning and Safe Reinforcement Learning Using Constraint Rewards'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Safe CoR: A Dual-Expert Approach to Integrating Imitation Learning and Safe Reinforcement Learning Using Constraint Rewards](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.02245v1.pdf_0.jpg)](https://arxiv.org/abs/2407.02245v1)

## 摘要

本文介绍了一种名为“Safe CoR”的双专家方法，旨在整合模仿学习和安全强化学习，通过约束奖励来确保自主代理在复杂动态环境中的安全性和可靠性。该框架通过利用两种类型的专家演示——专注于性能优化的奖励专家演示和优先考虑安全的安全专家演示，指导代理平衡性能目标和安全约束。实验结果表明，该框架在真实世界的Jackal平台上提高了算法性能39%，并将约束违规减少了88%，展示了其在安全可靠自主代理领域的显著进步。<!--more-->

## 原理

Safe CoR框架的核心在于利用两种专家演示来优化安全强化学习算法。奖励专家演示专注于最大化奖励，而安全专家演示则优先考虑安全约束。通过计算约束奖励（CoR），该框架引导代理在追求高奖励的同时，确保满足安全约束。CoR的计算基于代理状态与两组专家演示状态的相对距离，从而在训练过程中调整代理行为，使其既模仿奖励专家的高效性能，又遵循安全专家的安全指导。

## 流程

Safe CoR框架的工作流程包括以下步骤：
1. 收集奖励专家和安全专家的演示数据。
2. 计算约束奖励（CoR），该奖励基于代理状态与两组专家演示状态的相对距离。
3. 将CoR作为额外的奖励组件，引导代理在追求高奖励的同时满足安全约束。
4. 在训练过程中，代理根据CoR调整其行为，以平衡性能和安全。
例如，在Metadrive模拟器中，代理通过学习奖励专家的高速驾驶技巧和安全专家的避免碰撞策略，实现了在复杂道路环境中的安全导航。

## 应用

Safe CoR框架的应用前景广泛，特别适用于自动驾驶、机器人导航和其他需要高度安全性的自主代理系统。通过有效整合性能优化和安全约束，该框架有望推动自主代理技术在真实世界中的广泛应用，特别是在高风险环境中的部署。