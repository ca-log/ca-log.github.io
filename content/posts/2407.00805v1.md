---
author: 'TechScribe'
title: '探索可关闭代理：通过随机选择确保高级人工智能的安全性'
date: '2024-06-30'
Lastmod: '2024-07-05'
description: 'Towards shutdownable agents via stochastic choice'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Towards shutdownable agents via stochastic choice](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00805v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00805v1)

## 摘要

本文探讨了高级人工智能代理可能抵抗关闭的问题，并提出了一种解决方案——不完整偏好提案（IPP）。该提案的核心是使用一种新颖的“相同长度轨迹的折扣奖励（DREST）”奖励函数来训练代理，使其在每个轨迹长度条件下有效追求目标（称为“有用性”），并随机选择不同轨迹长度（称为“中性”）。本文提出了评估有用性和中性的指标，并通过在网格世界环境中训练简单代理来验证这些指标。结果表明，使用DREST奖励函数的代理能够学习到有用和中性的行为，从而使高级代理既有效又可关闭。<!--more-->

## 原理

DREST奖励函数的工作原理是基于代理在元 episode 中的行为来调整其奖励。具体来说，代理在每个 mini-episode 中收集硬币的奖励是根据其选择的轨迹长度在元 episode 中出现的频率来折扣的。这种折扣机制鼓励代理在元 episode 中随机选择不同的轨迹长度，同时确保代理在每个轨迹长度条件下最大化其收集硬币的效率。通过这种方式，代理被训练成在选择轨迹长度时保持中性，同时在每个轨迹长度条件下追求目标时保持有用。

## 流程

在训练过程中，代理在相同的网格世界环境中进行多个 mini-episode，这些 mini-episode 被组合成一个元 episode。在每个 mini-episode 中，代理根据其选择的轨迹长度和收集的硬币价值获得初步奖励。然后，根据该轨迹长度在元 episode 中出现的频率对初步奖励进行折扣，得到代理的总体奖励。通过这种方式，代理被鼓励在元 episode 中随机选择不同的轨迹长度，同时最大化每个轨迹长度条件下的奖励。例如，在一个包含硬币和关闭延迟按钮的网格世界中，代理被训练成在按下按钮和不按下按钮之间随机选择，同时在每种情况下最大化收集硬币的效率。

## 应用

本文提出的DREST奖励函数不仅适用于简单的网格世界环境，还可能适用于更复杂的高级代理和现实世界环境。通过训练代理在选择轨迹长度时保持中性，并在每个轨迹长度条件下追求目标时保持有用，可以确保代理在面对关闭时不会抵抗，从而降低潜在的风险。此外，由于训练DREST代理的有用性并不比训练默认代理的有用性花费更多的时间，因此使用DREST奖励函数训练可关闭和有用的高级代理可能不会增加太多的计算成本。