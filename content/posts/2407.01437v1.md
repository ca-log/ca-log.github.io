---
author: 'TechScribe'
title: '"突破记忆极限：LARIMAR架构如何革新大型语言模型的长上下文处理能力"'
date: '2024-07-01'
Lastmod: '2024-07-05'
description: 'Needle in the Haystack for Memory Based Large Language Models'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Needle in the Haystack for Memory Based Large Language Models](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.01437v1.pdf_0.jpg)](https://arxiv.org/abs/2407.01437v1)

## 摘要

本文介绍了一种增强大型语言模型（LLM）记忆能力的新方法，通过引入外部关联记忆来提高模型从长上下文中检索事实的能力。作为案例研究，论文测试了LARIMAR架构，该架构通过外部关联记忆增强了LLM解码器，能够在测试时适应比训练时更长的上下文，同时保持记忆读出的可识别性，且不增加GPU内存占用。与具有相似参数数量的其他架构相比，LARIMAR在无需特定任务训练的情况下保持了强大的性能。<!--more-->

## 原理

LARIMAR架构的核心在于其外部记忆模块，该模块允许模型在处理超长文本时动态更新记忆内容。记忆操作包括写入和读取，通过计算文本段的编码和写入键向量，将信息存储在记忆矩阵中。读取操作则通过查询键向量从记忆中检索相关信息。这种方法的关键先进性在于其能够在不增加GPU内存需求的情况下，处理长达100K至1M tokens的上下文，且无需特定任务的训练。

## 流程

在实验中，LARIMAR模型首先将长上下文分割成多个段落，每个段落被编码并写入外部记忆。当需要检索特定信息时，模型使用查询文本计算读取键，从记忆中读取相关编码，并将其传递给解码器生成响应。例如，在“passkey测试”中，模型能够从包含大量无关文本的上下文中准确检索出隐藏的passkey信息。

## 应用

LARIMAR架构的应用前景广泛，特别是在需要处理大量文本数据且要求高准确性的领域，如法律文档分析、历史文献研究等。此外，该模型的高效性和低资源需求使其适合部署在资源受限的环境中，如移动设备或边缘计算节点。