---
author: 'TechScribe'
title: '超越功能正确性：探索大型语言模型中的编码风格不一致性'
date: '2024-06-29'
Lastmod: '2024-07-05'
description: 'Beyond Functional Correctness: Investigating Coding Style Inconsistencies in Large Language Models'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Beyond Functional Correctness: Investigating Coding Style Inconsistencies in Large Language Models](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.00456v1.pdf_0.jpg)](https://arxiv.org/abs/2407.00456v1)

## 摘要

本文探讨了大型语言模型（LLMs）在代码生成中的编码风格一致性问题。尽管LLMs在代码生成方面取得了显著进展，但之前的研究主要集中在代码生成的准确性上，而对LLMs与人类开发者之间的编码风格差异关注不足。本文通过实证分析，比较了主流代码LLMs生成的代码与人类开发者编写的代码在可读性、简洁性和健壮性方面的差异，并总结了编码风格不一致的分类。研究结果揭示了LLMs和开发者之间在编码风格上的显著差异，并探讨了这些不一致的可能原因及解决方案。<!--more-->

## 原理

本文通过手动分析大量生成结果，总结了编码风格不一致的类型，并将其分为五个维度：格式不一致、语义不一致、表达/语句不一致、控制流不一致和容错不一致。研究团队选择了四个主流的代码LLMs，并在CoderEval基准上进行了实验，通过比较生成的代码与人类编写的代码，识别出24种不一致类型。这些不一致类型涵盖了从代码格式到语义表达的各个方面，展示了LLMs在编码风格上的局限性。

## 流程

研究团队首先收集了230个Python代码生成任务，并使用四个LLMs生成代码样本。然后，通过自动过滤和手动过滤确保代码样本的功能正确性。接下来，进行开放编码，通过逐行比较生成的代码与基准代码，识别并分类编码风格的不一致。最后，通过实验探索了不同的提示策略，以改善LLMs的编码风格。整个流程确保了研究的系统性和严谨性。

## 应用

本文的研究成果不仅揭示了LLMs在编码风格上的局限性，也为未来的研究和开发提供了方向。通过改进提示策略和模型训练方法，可以进一步提高LLMs在代码生成中的编码风格一致性，从而提升软件开发的效率和质量。此外，研究结果还为开发更智能的代码生成工具和辅助系统提供了理论基础和实践指导。