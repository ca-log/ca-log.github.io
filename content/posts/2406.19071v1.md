---
author: 'TechScribe'
title: '"优化大型语言模型的共情响应生成：理论驱动的方法与实践"'
date: '2024-06-27'
Lastmod: '2024-07-05'
description: 'EmPO: Theory-Driven Dataset Construction for Empathetic Response Generation through Preference Optimization'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![EmPO: Theory-Driven Dataset Construction for Empathetic Response Generation through Preference Optimization](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2406.19071v1.pdf_0.jpg)](https://arxiv.org/abs/2406.19071v1)

## 摘要

本文由Ondrej Sotolar等人撰写，探讨了在对话代理中生成共情响应的问题。共情响应生成（ERG）是使对话代理能够理解用户情境、情感和体验，从而生成适当、类人响应的关键技术。尽管大型语言模型（LLM）在此任务上显示出潜力，但确保响应的共情质量和模型的泛化性能仍面临挑战。本文提出了一种新颖的方法，通过构建理论驱动的偏好数据集并使用偏好优化算法来对齐LLM，以解决这些挑战。研究使用了EmpatheticDialogues数据集，并评估了共情响应生成的效果，同时公开了所有数据集、源代码和模型。<!--more-->

## 原理

本文的核心在于通过理论驱动的偏好数据集来优化大型语言模型（LLM）的共情响应生成。首先，研究者利用EmpatheticDialogues数据集构建了一个包含偏好和非偏好完成的偏好数据集。每个对话都与一个情感标签相关联，研究者使用Plutchik的情感轮来找到与原始情感标签相对的情感标签，从而构建出偏好和非偏好的对话完成对。接着，通过Direct Preference Optimization（DPO）算法对LLM进行微调，使其生成的响应与偏好完成对齐。这种方法不仅提高了共情响应的质量，还保持了模型在其他任务上的泛化能力。

## 流程

1. **数据集构建**：从EmpatheticDialogues数据集中提取对话，并根据情感标签使用Plutchik的情感轮构建偏好和非偏好的对话完成对。
2. **模型微调**：使用Direct Preference Optimization（DPO）算法对LLM进行微调，使其生成的响应与偏好完成对齐。
3. **评估**：使用diff-Epitome和BERTscore等指标评估共情响应的质量，并在MMLU基准上评估模型的泛化性能。

## 应用

本文提出的方法不仅适用于共情响应生成，还可以扩展到其他需要情感理解和响应的领域，如心理健康支持、客户服务等。通过优化LLM的共情能力，可以显著提升对话代理的交互质量和用户满意度。