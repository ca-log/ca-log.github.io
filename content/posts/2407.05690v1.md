---
author: 'TechScribe'
title: 'TransAct：革命性的LLM剪枝技术，实现高效终端部署'
date: '2024-07-08'
Lastmod: '2024-07-10'
description: 'Pruning Large Language Models to Intra-module Low-rank Architecture with Transitional Activations'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Pruning Large Language Models to Intra-module Low-rank Architecture with Transitional Activations](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.05690v1.pdf_0.jpg)](https://arxiv.org/abs/2407.05690v1)

## 摘要

本文介绍了一种名为TransAct的新型结构化剪枝方法，旨在减少大型语言模型（LLMs）的计算和内存开销，使其更适合在终端设备上部署。TransAct通过压缩多头部注意力（MHA）和多层感知器（MLP）模块内的过渡激活，同时保留模块间的激活，从而将LLM剪枝成一个低秩的内部模块架构。这种方法在保持模型性能的同时，显著减少了权重、KV缓存和注意力计算的需求。实验结果表明，TransAct在高效性和性能方面均优于现有技术，特别是在高压缩率下表现出色。<!--more-->

## 原理

TransAct的工作原理基于对LLM中MHA和MLP模块内过渡激活的观察和分析。它通过定义每个注意力头和MLP过渡维度的显著性，使用激活值来指导剪枝过程。具体来说，TransAct通过计算激活值的平方和以及最大激活值的加权平方，来评估每个头的显著性。对于MLP，则直接使用过渡激活值的平方来表示显著性。通过这种方式，TransAct能够精确地剪枝出最不显著的部分，同时保持模型的整体性能和效率。

## 流程

TransAct的工作流程包括以下几个步骤：
1. **模型架构定义**：定义LLM的基本架构，包括嵌入层、MHA、MLP和语言模型头部。
2. **过渡激活定义**：在MHA和MLP模块中定义过渡激活，这些激活是连接输入和输出权重矩阵的关键。
3. **显著性计算**：计算每个注意力头和MLP过渡维度的显著性，使用激活值的平方和以及最大激活值的加权平方。
4. **剪枝操作**：根据显著性计算结果，剪枝出最不显著的部分，同时保持模块间的激活不变。
5. **迭代剪枝和微调**：为了避免一次性剪枝对模型造成不可恢复的损害，TransAct采用迭代剪枝策略，并在每次剪枝后进行微调以恢复模型性能。

## 应用

TransAct的应用前景广泛，特别是在资源受限的终端设备上部署大型语言模型时。由于其高效的剪枝策略和低秩的内部模块架构，TransAct能够显著减少模型的计算和内存需求，从而使得LLM在移动设备、嵌入式系统和其他资源受限的环境中更加实用。此外，TransAct的剪枝方法也可以应用于服务器端，通过减少计算需求来提高推理速度和效率。