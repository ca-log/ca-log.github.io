---
author: 'TechScribe'
title: '"mllm-NPU：革命性的移动设备上LLM推理系统，实现千倍速度提升"'
date: '2024-07-08'
Lastmod: '2024-07-09'
description: 'Empowering 1000 tokens/second on-device LLM prefilling with mllm-NPU'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Empowering 1000 tokens/second on-device LLM prefilling with mllm-NPU](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.05858v1.pdf_0.jpg)](https://arxiv.org/abs/2407.05858v1)

## 摘要

本文介绍了一种名为mllm-NPU的新型LLM推理系统，该系统通过有效利用移动设备上的神经处理单元（NPU）来减少大型语言模型（LLM）的预填充延迟和能耗。mllm-NPU通过算法-系统协同设计，解决了LLM架构与现代NPU设计之间的几个语义鸿沟，实现了在设备上的高效LLM推理。该系统通过重新构建提示和模型，在提示级别、张量级别和块级别上进行了优化，显著提高了预填充速度和能效，为实际应用中的设备上LLM推理铺平了道路。<!--more-->

## 原理

mllm-NPU的核心创新在于其算法-系统协同设计，通过以下三个层次的优化来提高LLM在NPU上的推理效率：
1. **提示级别**：将可变长度的提示分割成多个固定大小的块，同时保持数据依赖性。
2. **张量级别**：识别并提取显著的异常值，以最小开销在CPU/GPU上并行运行。
3. **块级别**：根据硬件亲和性和对精度的敏感性，将Transformer块调度到CPU/GPU和NPU上。
这些技术共同作用，使得mllm-NPU能够在移动设备上实现超过1000 tokens/秒的预填充速度，显著优于现有基线系统。

## 流程

mllm-NPU的工作流程包括准备阶段和执行阶段：
- **准备阶段**：使用增强的每张量量化算法将LLM量化为W8A8格式，并生成固定长度的块共享图以高效处理可变长度提示。
- **执行阶段**：当接收到提示时，mllm-NPU将其分割成固定大小的块并因果处理。这些块图将被分割成子图，根据其数据格式高效地调度到CPU/GPU和NPU上执行。
此外，mllm-NPU还采用了影子异常值执行和无序子图执行技术，以进一步提高执行效率和准确性。

## 应用

mllm-NPU的关键内容具有广泛的应用范围，特别是在需要隐私保护和低延迟的移动应用场景中，如UI任务自动化和个性化邮件自动回复。随着移动设备上LLM的普及，mllm-NPU的高效推理能力将为这些应用提供强大的支持，推动移动AI技术的发展。