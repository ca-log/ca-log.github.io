---
author: 'TechScribe'
title: '“Lookback Lens：利用注意力映射检测和缓解大型语言模型中的上下文幻觉”'
date: '2024-07-09'
Lastmod: '2024-07-10'
description: 'Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps'
categories:
  - CS.AI
# tags:
#   - emoji
---

[![Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps](https://arxiv-research-1301205113.cos.ap-guangzhou.myqcloud.com/images/2407.07071v1.pdf_0.jpg)](https://arxiv.org/abs/2407.07071v1)

## 摘要

本文介绍了一种名为“Lookback Lens”的新方法，用于检测和缓解大型语言模型（LLMs）中的上下文幻觉问题。上下文幻觉是指LLMs在总结文章或回答问题时，生成与输入上下文不符的错误信息。Lookback Lens通过分析LLMs的注意力权重，特别是关注模型在生成新内容时对上下文信息的依赖程度，来识别这些幻觉。该方法的核心是一个基于注意力权重比率的线性分类器，能够有效地跨任务和模型进行迁移，无需重新训练。此外，Lookback Lens还被用于指导解码过程，以减少幻觉的产生，例如在XSum总结任务中减少了9.6%的幻觉。<!--more-->

## 原理

Lookback Lens的工作原理基于一个假设：上下文幻觉与LLMs在生成新内容时对上下文信息的依赖程度有关。具体来说，Lookback Lens计算每个注意力头在生成新令牌时对上下文令牌的注意力权重比率（lookback ratio）。这个比率被用作特征输入到一个线性分类器中，该分类器被训练来识别幻觉。通过这种方式，Lookback Lens能够有效地检测到模型何时生成了与上下文不符的内容，并且这种方法在多个任务和模型之间具有良好的迁移性。

## 流程

Lookback Lens的工作流程包括以下步骤：
1. 计算每个注意力头在生成新令牌时的lookback ratio。
2. 将这些比率作为特征输入到一个线性分类器中。
3. 训练分类器以识别幻觉。
4. 在解码过程中，使用Lookback Lens分类器来指导生成过程，选择那些被分类器判断为更符合上下文的内容。
例如，在XSum总结任务中，Lookback Lens通过评估多个候选块的lookback ratios，选择最不可能包含幻觉的块来生成最终的总结。

## 应用

Lookback Lens的应用前景广泛，特别是在需要高度依赖上下文信息的任务中，如总结、问答和对话系统。通过减少幻觉，Lookback Lens可以提高LLMs在这些任务中的准确性和可靠性，从而在新闻总结、教育辅导、客户服务等多个领域发挥重要作用。此外，由于其跨任务和模型的迁移能力，Lookback Lens可以简化在不同应用场景中的部署和优化过程。